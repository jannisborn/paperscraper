{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#paperscraper","title":"paperscraper","text":"<p><code>paperscraper</code> is a <code>python</code> package for scraping publication metadata or full text files (PDF or XML) from PubMed or preprint servers such as arXiv, medRxiv, bioRxiv and chemRxiv. It provides a streamlined interface to scrape metadata, allows to retrieve citation counts from Google Scholar, impact factors from journals and comes with simple postprocessing functions and plotting routines for meta-analysis.</p>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Getting Started</li> <li>Download X-rxiv Dumps</li> <li>Arxiv Local Dump</li> <li>Examples</li> <li>Publication Keyword Search</li> <li>Full-Text Retrieval (PDFs &amp; XMLs)</li> <li>Citation Search</li> <li>Journal Impact Factor</li> <li>Plotting</li> <li>Barplots</li> <li>Venn Diagrams</li> <li>Citation</li> <li>Contributions</li> </ol>"},{"location":"#getting-started","title":"Getting started","text":"<pre><code>pip install paperscraper\n</code></pre> <p>This is enough to query PubMed, arXiv or Google Scholar.</p>"},{"location":"#download-x-rxiv-dumps","title":"Download X-rxiv Dumps","text":"<p>However, to scrape publication data from the preprint servers biorxiv, medrxiv and chemrxiv, the setup is different. The entire history of papers is downloaded and stored in the <code>server_dumps</code> folder in a <code>.jsonl</code> format (one paper per line). This takes a while, as of November 2025:</p> <p><pre><code>from paperscraper.get_dumps import biorxiv, medrxiv, chemrxiv\nchemrxiv()  #  Takes 30min -&gt; +30K papers (~50 MB file)\nmedrxiv()  #  Takes &lt;1h -&gt; +90K papers (~200 MB file)\nbiorxiv()  # Up to 6h -&gt; +400K papers (~800 MB file)\n</code></pre> NOTE: Once the dumps are stored, please make sure to restart the python interpreter so that the changes take effect.  NOTE: If you experience API connection issues, since v0.2.12 there are automatic retries which you can even control and raise from the default of 10, as in <code>biorxiv(max_retries=20)</code>.</p> <p>Since v0.2.5 <code>paperscraper</code> also allows to scrape {med/bio/chem}rxiv for specific dates. <pre><code>medrxiv(start_date=\"2023-04-01\", end_date=\"2023-04-08\")\n</code></pre> But watch out. The resulting <code>.jsonl</code> file will be labelled according to the current date and all your subsequent searches will be based on this file only. If you use this option you might want to keep an eye on the source files (<code>paperscraper/server_dumps/*jsonl</code>) to ensure they contain the paper metadata for all papers you're interested in.</p>"},{"location":"#arxiv-local-dump","title":"Arxiv local dump","text":"<p>If you prefer local search rather than using the arxiv API:</p> <pre><code>from paperscraper.get_dumps import arxiv\narxiv(start_date='2024-01-01', end_date=None) # scrapes all metadata from 2024 until today.\n</code></pre> <p>Afterwards you can search the local arxiv dump just like the other x-rxiv dumps. The direct endpoint is <code>paperscraper.arxiv.get_arxiv_papers_local</code>. You can also specify the backend directly in the <code>get_and_dump_arxiv_papers</code> function: <pre><code>from paperscraper.arxiv import get_and_dump_arxiv_papers\nget_and_dump_arxiv_papers(..., backend='local')\n</code></pre></p>"},{"location":"#examples","title":"Examples","text":"<p><code>paperscraper</code> is build on top of the packages arxiv, pymed, and scholarly. </p>"},{"location":"#publication-keyword-search","title":"Publication keyword search","text":"<p>Consider you want to perform a publication keyword search with the query: <code>COVID-19</code> AND <code>Artificial Intelligence</code> AND <code>Medical Imaging</code>. </p> <ul> <li>Scrape papers from PubMed:</li> </ul> <pre><code>from paperscraper.pubmed import get_and_dump_pubmed_papers\ncovid19 = ['COVID-19', 'SARS-CoV-2']\nai = ['Artificial intelligence', 'Deep learning', 'Machine learning']\nmi = ['Medical imaging']\nquery = [covid19, ai, mi]\n\nget_and_dump_pubmed_papers(query, output_filepath='covid19_ai_imaging.jsonl')\n</code></pre> <ul> <li>Scrape papers from arXiv:</li> </ul> <pre><code>from paperscraper.arxiv import get_and_dump_arxiv_papers\n\nget_and_dump_arxiv_papers(query, output_filepath='covid19_ai_imaging.jsonl')\n</code></pre> <ul> <li>Scrape papers from bioRiv, medRxiv or chemRxiv:</li> </ul> <pre><code>from paperscraper.xrxiv.xrxiv_query import XRXivQuery\n\nquerier = XRXivQuery('server_dumps/chemrxiv_2020-11-10.jsonl')\nquerier.search_keywords(query, output_filepath='covid19_ai_imaging.jsonl')\n</code></pre> <p>You can also use <code>dump_queries</code> to iterate over a bunch of queries for all available databases.</p> <pre><code>from paperscraper import dump_queries\n\nqueries = [[covid19, ai, mi], [covid19, ai], [ai]]\ndump_queries(queries, '.')\n</code></pre> <p>Or use the harmonized interface of <code>QUERY_FN_DICT</code> to query multiple databases of your choice: <pre><code>from paperscraper.load_dumps import QUERY_FN_DICT\nprint(QUERY_FN_DICT.keys())\n\nQUERY_FN_DICT['biorxiv'](query, output_filepath='biorxiv_covid_ai_imaging.jsonl')\nQUERY_FN_DICT['medrxiv'](query, output_filepath='medrxiv_covid_ai_imaging.jsonl')\n</code></pre></p> <ul> <li>Scrape papers from Google Scholar:</li> </ul> <p>Thanks to scholarly, there is an endpoint for Google Scholar too. It does not understand Boolean expressions like the others, but should be used just like the Google Scholar search fields.</p> <p><pre><code>from paperscraper.scholar import get_and_dump_scholar_papers\ntopic = 'Machine Learning'\nget_and_dump_scholar_papers(topic)\n</code></pre> NOTE: The scholar endpoint does not require authentication but since it regularly prompts with captchas, it's difficult to apply large scale.</p>"},{"location":"#full-text-retrieval-pdfs-xmls","title":"Full-Text Retrieval (PDFs &amp; XMLs)","text":"<p><code>paperscraper</code> allows you to download full text of publications using DOIs. The basic functionality works reliably for preprint servers (arXiv, bioRxiv, medRxiv, chemRxiv), but retrieving papers from PubMed dumps is more challenging due to publisher restrictions and paywalls.</p>"},{"location":"#standard-usage","title":"Standard Usage","text":"<p>The main download functions work for all paper types with automatic fallbacks:</p> <pre><code>from paperscraper.pdf import save_pdf\npaper_data = {'doi': \"10.48550/arXiv.2207.03928\"}\nsave_pdf(paper_data, filepath='gt4sd_paper.pdf')\n</code></pre> <p>To batch download full texts from your metadata search results:</p> <pre><code>from paperscraper.pdf import save_pdf_from_dump\n\n# Save PDFs/XMLs in current folder and name the files by their DOI\nsave_pdf_from_dump('medrxiv_covid_ai_imaging.jsonl', pdf_path='.', key_to_save='doi')\n</code></pre>"},{"location":"#automatic-fallback-mechanisms","title":"Automatic Fallback Mechanisms","text":"<p>When the standard text retrieval fails, <code>paperscraper</code> automatically tries these fallbacks:</p> <ul> <li>BioC-PMC: For biomedical papers in PubMed Central (open-access repository), it retrieves open-access full-text XML from the BioC-PMC API.</li> <li>eLife Papers: For eLife journal papers, it fetches XML files from eLife's open GitHub repository.</li> </ul> <p>These fallbacks are tried automatically without requiring any additional configuration.</p>"},{"location":"#enhanced-retrieval-with-publisher-apis","title":"Enhanced Retrieval with Publisher APIs","text":"<p>For more comprehensive access to papers from major publishers, you can provide API keys for:</p> <ul> <li>Wiley TDM API: Enables access to Wiley publications (2,000+ journals).</li> <li>Elsevier TDM API: Enables access to Elsevier publications (The Lancet, Cell, ...).</li> <li>bioRxiv TDM API Enable access to bioRxiv publications (since May 2025 bioRxiv is protected with Cloudflare)</li> </ul> <p>To use publisher APIs:</p> <ol> <li> <p>Create a file with your API keys: <pre><code>WILEY_TDM_API_TOKEN=your_wiley_token_here\nELSEVIER_TDM_API_KEY=your_elsevier_key_here\nAWS_ACCESS_KEY_ID=your_aws_access_key_here\nAWS_SECRET_ACCESS_KEY=your_aws_secret_key_here\n</code></pre> NOTE: The AWS keys can be created in your AWS/IAM account. When creating the key, make sure you tick the <code>AmazonS3ReadOnlyAccess</code> permission policy.  NOTE: If you name the file <code>.env</code> it will be loaded automatically (if it is in the cwd or anywhere above the tree to home).</p> </li> <li> <p>Pass the file path when calling retrieval functions:</p> </li> </ol> <pre><code>from paperscraper.pdf import save_pdf_from_dump\n\nsave_pdf_from_dump(\n    'pubmed_query_results.jsonl',\n    pdf_path='./papers',\n    key_to_save='doi',\n    api_keys='path/to/your/api_keys.txt'\n)\n</code></pre> <p>For obtaining API keys: - Wiley TDM API: Visit Wiley Text and Data Mining (free for academic users with institutional subscription) - Elsevier TDM API: Visit Elsevier's Text and Data Mining (free for academic users with institutional subscription)</p> <p>NOTE: While these fallback mechanisms improve retrieval success rates, they cannot guarantee access to all papers due to various access restrictions.</p>"},{"location":"#citation-search","title":"Citation search","text":"<p>You can fetch the number of citations of a paper from its title or DOI</p> <pre><code>from paperscraper.citations import get_citations_from_title, get_citations_by_doi\ntitle = '\u00dcber formal unentscheidbare S\u00e4tze der Principia Mathematica und verwandter Systeme I.'\nprint(get_citations_from_title(title))\n\ndoi = '10.1021/acs.jcim.3c00132'\nget_citations_by_doi(doi)\n</code></pre> <p>NOTE: This uses the Semantic Scholar API which is bandwidth-limited. If you have an API Key set it via: <pre><code>export SS_API_KEY=YOUR_API_KEY\n</code></pre> This will increase your throughput for using <code>paperscraper.citations</code> based on the rate limits of your key.</p>"},{"location":"#journal-impact-factor","title":"Journal impact factor","text":"<p>You can also retrieve the impact factor for all journals: <pre><code>&gt;&gt;&gt;from paperscraper.impact import Impactor\n&gt;&gt;&gt;i = Impactor()\n&gt;&gt;&gt;i.search(\"Nat Comms\", threshold=85, sort_by='impact') \n[\n    {'journal': 'Nature Communications', 'factor': 17.694, 'score': 94}, \n    {'journal': 'Natural Computing', 'factor': 1.504, 'score': 88}\n]\n</code></pre> This performs a fuzzy search with a threshold of 85. <code>threshold</code> defaults to 100 in which case an exact search is performed. You can also search by journal abbreviation, E-ISSN or NLM ID. <pre><code>i.search(\"Nat Rev Earth Environ\") # [{'journal': 'Nature Reviews Earth &amp; Environment', 'factor': 37.214, 'score': 100}]\ni.search(\"101771060\") # [{'journal': 'Nature Reviews Earth &amp; Environment', 'factor': 37.214, 'score': 100}]\ni.search('2662-138X') # [{'journal': 'Nature Reviews Earth &amp; Environment', 'factor': 37.214, 'score': 100}]\n\n# Filter results by impact factor\ni.search(\"Neural network\", threshold=85, min_impact=1.5, max_impact=20)\n# [\n#   {'journal': 'IEEE Transactions on Neural Networks and Learning Systems', 'factor': 14.255, 'score': 93}, \n#   {'journal': 'NEURAL NETWORKS', 'factor': 9.657, 'score': 91},\n#   {'journal': 'WORK-A Journal of Prevention Assessment &amp; Rehabilitation', 'factor': 1.803, 'score': 86}, \n#   {'journal': 'NETWORK-COMPUTATION IN NEURAL SYSTEMS', 'factor': 1.5, 'score': 92}\n# ]\n\n# Show all fields\ni.search(\"quantum information\", threshold=90, return_all=True)\n# [\n#   {'factor': 10.758, 'jcr': 'Q1', 'journal_abbr': 'npj Quantum Inf', 'eissn': '2056-6387', 'journal': 'npj Quantum Information', 'nlm_id': '101722857', 'issn': '', 'score': 92},\n#   {'factor': 1.577, 'jcr': 'Q3', 'journal_abbr': 'Nation', 'eissn': '0027-8378', 'journal': 'NATION', 'nlm_id': '9877123', 'issn': '0027-8378', 'score': 91}\n# ]\n</code></pre></p>"},{"location":"#plotting","title":"Plotting","text":"<p>When multiple query searches are performed, two types of plots can be generated automatically: Venn diagrams and bar plots.</p>"},{"location":"#barplots","title":"Barplots","text":"<p>Compare the temporal evolution of different queries across different servers.</p> <pre><code>from paperscraper import QUERY_FN_DICT\nfrom paperscraper.postprocessing import aggregate_paper\nfrom paperscraper.utils import get_filename_from_query, load_jsonl\n\n# Define search terms and their synonyms\nml = ['Deep learning', 'Neural Network', 'Machine learning']\nmol = ['molecule', 'molecular', 'drug', 'ligand', 'compound']\ngnn = ['gcn', 'gnn', 'graph neural', 'graph convolutional', 'molecular graph']\nsmiles = ['SMILES', 'Simplified molecular']\nfp = ['fingerprint', 'molecular fingerprint', 'fingerprints']\n\n# Define queries\nqueries = [[ml, mol, smiles], [ml, mol, fp], [ml, mol, gnn]]\n\nroot = '../keyword_dumps'\n\ndata_dict = dict()\nfor query in queries:\n    filename = get_filename_from_query(query)\n    data_dict[filename] = dict()\n    for db,_ in QUERY_FN_DICT.items():\n        # Assuming the keyword search has been performed already\n        data = load_jsonl(os.path.join(root, db, filename))\n\n        # Unstructured matches are aggregated into 6 bins, 1 per year\n        # from 2015 to 2020. Sanity check is performed by having \n        # `filtering=True`, removing papers that don't contain all of\n        # the keywords in query.\n        data_dict[filename][db], filtered = aggregate_paper(\n            data, 2015, bins_per_year=1, filtering=True,\n            filter_keys=query, return_filtered=True\n        )\n\n# Plotting is now very simple\nfrom paperscraper.plotting import plot_comparison\n\ndata_keys = [\n    'deeplearning_molecule_fingerprint.jsonl',\n    'deeplearning_molecule_smiles.jsonl', \n    'deeplearning_molecule_gcn.jsonl'\n]\nplot_comparison(\n    data_dict,\n    data_keys,\n    title_text=\"'Deep Learning' AND 'Molecule' AND X\",\n    keyword_text=['Fingerprint', 'SMILES', 'Graph'],\n    figpath='mol_representation'\n)\n</code></pre> <p></p>"},{"location":"#venn-diagrams","title":"Venn Diagrams","text":"<pre><code>from paperscraper.plotting import (\n    plot_venn_two, plot_venn_three, plot_multiple_venn\n)\n\nsizes_2020 = (30842, 14474, 2292, 35476, 1904, 1408, 376)\nsizes_2019 = (55402, 11899, 2563)\nlabels_2020 = ('Medical\\nImaging', 'Artificial\\nIntelligence', 'COVID-19')\nlabels_2019 = ['Medical Imaging', 'Artificial\\nIntelligence']\n\nplot_venn_two(sizes_2019, labels_2019, title='2019', figname='ai_imaging')\n</code></pre> <pre><code>plot_venn_three(\n    sizes_2020, labels_2020, title='2020', figname='ai_imaging_covid'\n)\n</code></pre> <p>Or plot both together:</p> <pre><code>plot_multiple_venn(\n    [sizes_2019, sizes_2020], [labels_2019, labels_2020], \n    titles=['2019', '2020'], suptitle='Keyword search comparison', \n    gridspec_kw={'width_ratios': [1, 2]}, figsize=(10, 6),\n    figname='both'\n)\n</code></pre> <p></p>"},{"location":"#citation","title":"Citation","text":"<p>If you use <code>paperscraper</code>, please cite a paper that motivated our development of this tool.</p> <pre><code>@article{born2021trends,\n  title={Trends in Deep Learning for Property-driven Drug Design},\n  author={Born, Jannis and Manica, Matteo},\n  journal={Current Medicinal Chemistry},\n  volume={28},\n  number={38},\n  pages={7862--7886},\n  year={2021},\n  publisher={Bentham Science Publishers}\n}\n</code></pre>"},{"location":"#contributions","title":"Contributions","text":"<p>Thanks to the following contributors: - @mathinic: Since <code>v0.3.0</code> improved PubMed full text retrieval with additional fallback mechanisms (BioC-PMC, eLife and optional Wiley/Elsevier APIs).</p> <ul> <li> <p>@memray: Since <code>v0.2.12</code> there are automatic retries when downloading the {med/bio/chem}rxiv dumps.</p> </li> <li> <p>@achouhan93: Since <code>v0.2.5</code> {med/bio/chem}rxiv can be scraped for specific dates!</p> </li> <li> <p>@daenuprobst: Since  <code>v0.2.4</code> PDF files can be scraped directly (<code>paperscraper.pdf.save_pdf</code>)</p> </li> <li> <p>@oppih: Since <code>v0.2.3</code> chemRxiv API also provides DOI and URL if available</p> </li> <li> <p>@lukasschwab: Enabled support for <code>arxiv</code> &gt;<code>1.4.2</code> in paperscraper <code>v0.1.0</code>.</p> </li> <li> <p>@juliusbierk: Bugfixes</p> </li> </ul>"},{"location":"api_reference/","title":"API Reference","text":""},{"location":"api_reference/#paperscraper","title":"<code>paperscraper</code>","text":"<p>Initialize the module.</p>"},{"location":"api_reference/#paperscraper.dump_queries","title":"<code>dump_queries(keywords: List[List[Union[str, List[str]]]], dump_root: str) -&gt; None</code>","text":"<p>Performs keyword search on all available servers and dump the results.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[List[Union[str, List[str]]]]</code> <p>List of lists of keywords Each second-level list is considered a separate query. Within each query, each item (whether str or List[str]) are considered AND separated. If an item is again a list, strs are considered synonyms (OR separated).</p> required <code>dump_root</code> <code>str</code> <p>Path to root for dumping.</p> required Source code in <code>paperscraper/__init__.py</code> <pre><code>def dump_queries(keywords: List[List[Union[str, List[str]]]], dump_root: str) -&gt; None:\n    \"\"\"Performs keyword search on all available servers and dump the results.\n\n    Args:\n        keywords (List[List[Union[str, List[str]]]]): List of lists of keywords\n            Each second-level list is considered a separate query. Within each\n            query, each item (whether str or List[str]) are considered AND\n            separated. If an item is again a list, strs are considered synonyms\n            (OR separated).\n        dump_root (str): Path to root for dumping.\n    \"\"\"\n\n    for idx, keyword in enumerate(keywords):\n        for db, f in QUERY_FN_DICT.items():\n            logger.info(f\" Keyword {idx + 1}/{len(keywords)}, DB: {db}\")\n            filename = get_filename_from_query(keyword)\n            os.makedirs(os.path.join(dump_root, db), exist_ok=True)\n            f(keyword, output_filepath=os.path.join(dump_root, db, filename))\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv","title":"<code>arxiv</code>","text":""},{"location":"api_reference/#paperscraper.arxiv.XRXivQuery","title":"<code>XRXivQuery</code>","text":"<p>Query class.</p> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>class XRXivQuery:\n    \"\"\"Query class.\"\"\"\n\n    def __init__(\n        self,\n        dump_filepath: str,\n        fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n    ):\n        \"\"\"\n        Initialize the query class.\n\n        Args:\n            dump_filepath (str): filepath to the dump to be queried.\n            fields (List[str], optional): fields to contained in the dump per paper.\n                Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n        \"\"\"\n        self.dump_filepath = dump_filepath\n        self.fields = fields\n        self.errored = False\n\n        try:\n            self.df = pd.read_json(self.dump_filepath, lines=True)\n            self.df[\"date\"] = [date.strftime(\"%Y-%m-%d\") for date in self.df[\"date\"]]\n        except ValueError as e:\n            logger.warning(f\"Problem in reading file {dump_filepath}: {e} - Skipping!\")\n            self.errored = True\n        except KeyError as e:\n            logger.warning(f\"Key {e} missing in file from {dump_filepath} - Skipping!\")\n            self.errored = True\n\n    def search_keywords(\n        self,\n        keywords: List[Union[str, List[str]]],\n        fields: List[str] = None,\n        output_filepath: str = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Search for papers in the dump using keywords.\n\n        Args:\n            keywords (List[str, List[str]]): Items will be AND separated. If items\n                are lists themselves, they will be OR separated.\n            fields (List[str], optional): fields to be used in the query search.\n                Defaults to None, a.k.a. search in all fields excluding date.\n            output_filepath (str, optional): optional output filepath where to store\n                the hits in JSONL format. Defaults to None, a.k.a., no export to a file.\n\n        Returns:\n            pd.DataFrame: A dataframe with one paper per row.\n        \"\"\"\n        if fields is None:\n            fields = self.fields\n        fields = [field for field in fields if field != \"date\"]\n        hits_per_field = []\n        for field in fields:\n            field_data = self.df[field].str.lower()\n            hits_per_keyword = []\n            for keyword in keywords:\n                if isinstance(keyword, list):\n                    query = \"|\".join([_.lower() for _ in keyword])\n                else:\n                    query = keyword.lower()\n                hits_per_keyword.append(field_data.str.contains(query))\n            if len(hits_per_keyword):\n                keyword_hits = hits_per_keyword[0]\n                for single_keyword_hits in hits_per_keyword[1:]:\n                    keyword_hits &amp;= single_keyword_hits\n                hits_per_field.append(keyword_hits)\n        if len(hits_per_field):\n            hits = hits_per_field[0]\n            for single_hits in hits_per_field[1:]:\n                hits |= single_hits\n        if output_filepath is not None:\n            self.df[hits].to_json(output_filepath, orient=\"records\", lines=True)\n        return self.df[hits]\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.XRXivQuery.__init__","title":"<code>__init__(dump_filepath: str, fields: List[str] = ['title', 'doi', 'authors', 'abstract', 'date', 'journal'])</code>","text":"<p>Initialize the query class.</p> <p>Parameters:</p> Name Type Description Default <code>dump_filepath</code> <code>str</code> <p>filepath to the dump to be queried.</p> required <code>fields</code> <code>List[str]</code> <p>fields to contained in the dump per paper. Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].</p> <code>['title', 'doi', 'authors', 'abstract', 'date', 'journal']</code> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>def __init__(\n    self,\n    dump_filepath: str,\n    fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n):\n    \"\"\"\n    Initialize the query class.\n\n    Args:\n        dump_filepath (str): filepath to the dump to be queried.\n        fields (List[str], optional): fields to contained in the dump per paper.\n            Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n    \"\"\"\n    self.dump_filepath = dump_filepath\n    self.fields = fields\n    self.errored = False\n\n    try:\n        self.df = pd.read_json(self.dump_filepath, lines=True)\n        self.df[\"date\"] = [date.strftime(\"%Y-%m-%d\") for date in self.df[\"date\"]]\n    except ValueError as e:\n        logger.warning(f\"Problem in reading file {dump_filepath}: {e} - Skipping!\")\n        self.errored = True\n    except KeyError as e:\n        logger.warning(f\"Key {e} missing in file from {dump_filepath} - Skipping!\")\n        self.errored = True\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.XRXivQuery.search_keywords","title":"<code>search_keywords(keywords: List[Union[str, List[str]]], fields: List[str] = None, output_filepath: str = None) -&gt; pd.DataFrame</code>","text":"<p>Search for papers in the dump using keywords.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>fields</code> <code>List[str]</code> <p>fields to be used in the query search. Defaults to None, a.k.a. search in all fields excluding date.</p> <code>None</code> <code>output_filepath</code> <code>str</code> <p>optional output filepath where to store the hits in JSONL format. Defaults to None, a.k.a., no export to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with one paper per row.</p> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>def search_keywords(\n    self,\n    keywords: List[Union[str, List[str]]],\n    fields: List[str] = None,\n    output_filepath: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Search for papers in the dump using keywords.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        fields (List[str], optional): fields to be used in the query search.\n            Defaults to None, a.k.a. search in all fields excluding date.\n        output_filepath (str, optional): optional output filepath where to store\n            the hits in JSONL format. Defaults to None, a.k.a., no export to a file.\n\n    Returns:\n        pd.DataFrame: A dataframe with one paper per row.\n    \"\"\"\n    if fields is None:\n        fields = self.fields\n    fields = [field for field in fields if field != \"date\"]\n    hits_per_field = []\n    for field in fields:\n        field_data = self.df[field].str.lower()\n        hits_per_keyword = []\n        for keyword in keywords:\n            if isinstance(keyword, list):\n                query = \"|\".join([_.lower() for _ in keyword])\n            else:\n                query = keyword.lower()\n            hits_per_keyword.append(field_data.str.contains(query))\n        if len(hits_per_keyword):\n            keyword_hits = hits_per_keyword[0]\n            for single_keyword_hits in hits_per_keyword[1:]:\n                keyword_hits &amp;= single_keyword_hits\n            hits_per_field.append(keyword_hits)\n    if len(hits_per_field):\n        hits = hits_per_field[0]\n        for single_hits in hits_per_field[1:]:\n            hits |= single_hits\n    if output_filepath is not None:\n        self.df[hits].to_json(output_filepath, orient=\"records\", lines=True)\n    return self.df[hits]\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.dump_papers","title":"<code>dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None</code>","text":"<p>Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl file with one paper per line.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>DataFrame</code> <p>A dataframe of paper metadata, one paper per row.</p> required <code>filepath</code> <code>str</code> <p>Path to dump the papers, has to end with <code>.jsonl</code>.</p> required Source code in <code>paperscraper/utils.py</code> <pre><code>def dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None:\n    \"\"\"\n    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl\n    file with one paper per line.\n\n    Args:\n        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.\n        filepath (str): Path to dump the papers, has to end with `.jsonl`.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}\")\n    if not filepath.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a filepath with .jsonl extension\")\n\n    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):\n        papers = pd.DataFrame(papers)\n        logger.warning(\n            \"Preferably pass a pd.DataFrame, not a list of dictionaries. \"\n            \"Passing a list is a legacy functionality that might become deprecated.\"\n        )\n\n    if not isinstance(papers, pd.DataFrame):\n        raise TypeError(f\"papers must be a pd.DataFrame, not {type(papers)}\")\n\n    paper_list = list(papers.T.to_dict().values())\n\n    with open(filepath, \"w\") as f:\n        for paper in paper_list:\n            f.write(json.dumps(paper) + \"\\n\")\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.get_query_from_keywords","title":"<code>get_query_from_keywords(keywords: List[Union[str, List[str]]], start_date: str = 'None', end_date: str = 'None') -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the arxiv API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to arxiv API.</p> Source code in <code>paperscraper/arxiv/utils.py</code> <pre><code>def get_query_from_keywords(\n    keywords: List[Union[str, List[str]]],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the arxiv API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        start_date (str): Start date for the search. Needs to be in format:\n            YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date (str): End date for the search. Same notation as start_date.\n\n    Returns:\n        str: query to enter to arxiv API.\n    \"\"\"\n\n    query = \"\"\n    for i, key in enumerate(keywords):\n        if isinstance(key, str):\n            query += f\"all:{key} AND \"\n        elif isinstance(key, list):\n            inter = \"\".join([f\"all:{syn} OR \" for syn in key])\n            query += finalize_disjunction(inter)\n\n    query = finalize_conjunction(query)\n    if start_date == \"None\" and end_date == \"None\":\n        return query\n    elif start_date == \"None\":\n        start_date = EARLIEST_START\n    elif end_date == \"None\":\n        end_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    start = format_date(start_date)\n    end = format_date(end_date)\n    date_filter = f\" AND submittedDate:[{start} TO {end}]\"\n    return query + date_filter\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.get_arxiv_papers_local","title":"<code>get_arxiv_papers_local(keywords: List[Union[str, List[str]]], fields: List[str] = None, output_filepath: str = None) -&gt; pd.DataFrame</code>","text":"<p>Search for papers in the dump using keywords.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>fields</code> <code>List[str]</code> <p>fields to be used in the query search. Defaults to None, a.k.a. search in all fields excluding date.</p> <code>None</code> <code>output_filepath</code> <code>str</code> <p>optional output filepath where to store the hits in JSONL format. Defaults to None, a.k.a., no export to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with one paper per row.</p> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_arxiv_papers_local(\n    keywords: List[Union[str, List[str]]],\n    fields: List[str] = None,\n    output_filepath: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Search for papers in the dump using keywords.\n\n    Args:\n        keywords: Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        fields: fields to be used in the query search.\n            Defaults to None, a.k.a. search in all fields excluding date.\n        output_filepath: optional output filepath where to store the hits in JSONL format.\n            Defaults to None, a.k.a., no export to a file.\n\n    Returns:\n        pd.DataFrame: A dataframe with one paper per row.\n    \"\"\"\n    search_local_arxiv()\n    if ARXIV_QUERIER is None:\n        raise ValueError(\n            \"Could not find local arxiv dump. Use `backend=api` or download dump via `paperscraper.get_dumps.arxiv\"\n        )\n    return ARXIV_QUERIER(\n        keywords=keywords, fields=fields, output_filepath=output_filepath\n    )\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.get_arxiv_papers_api","title":"<code>get_arxiv_papers_api(query: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], max_results: int = 99999, client_options: Dict = {'num_retries': 10}, search_options: Dict = dict(), verbose: bool = True) -&gt; pd.DataFrame</code>","text":"<p>Performs arxiv API request of a given query and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to arxiv API. Needs to match the arxiv API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>max_results</code> <code>int</code> <p>Maximal number of results, defaults to 99999.</p> <code>99999</code> <code>client_options</code> <code>Dict</code> <p>Optional arguments for <code>arxiv.Client</code>. E.g.: page_size (int), delay_seconds (int), num_retries (int). NOTE: Decreasing 'num_retries' will speed up processing but might result in more frequent 'UnexpectedEmptyPageErrors'.</p> <code>{'num_retries': 10}</code> <code>search_options</code> <code>Dict</code> <p>Optional arguments for <code>arxiv.Search</code>. E.g.: id_list (List), sort_by, or sort_order.</p> <code>dict()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: One row per paper.</p> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_arxiv_papers_api(\n    query: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    max_results: int = 99999,\n    client_options: Dict = {\"num_retries\": 10},\n    search_options: Dict = dict(),\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs arxiv API request of a given query and returns list of papers with\n    fields as desired.\n\n    Args:\n        query: Query to arxiv API. Needs to match the arxiv API notation.\n        fields: List of strings with fields to keep in output.\n        max_results: Maximal number of results, defaults to 99999.\n        client_options: Optional arguments for `arxiv.Client`. E.g.:\n            page_size (int), delay_seconds (int), num_retries (int).\n            NOTE: Decreasing 'num_retries' will speed up processing but might\n            result in more frequent 'UnexpectedEmptyPageErrors'.\n        search_options: Optional arguments for `arxiv.Search`. E.g.:\n            id_list (List), sort_by, or sort_order.\n\n    Returns:\n        pd.DataFrame: One row per paper.\n\n    \"\"\"\n    client = arxiv.Client(**client_options)\n    search = arxiv.Search(query=query, max_results=max_results, **search_options)\n    results = client.results(search)\n\n    processed = pd.DataFrame(\n        [\n            {\n                arxiv_field_mapper.get(key, key): process_fields.get(\n                    arxiv_field_mapper.get(key, key), lambda x: x\n                )(value)\n                for key, value in vars(paper).items()\n                if arxiv_field_mapper.get(key, key) in fields and key != \"doi\"\n            }\n            for paper in tqdm(results, desc=f\"Processing {query}\", disable=not verbose)\n        ]\n    )\n    return processed\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.get_and_dump_arxiv_papers","title":"<code>get_and_dump_arxiv_papers(keywords: List[Union[str, List[str]]], output_filepath: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], start_date: str = 'None', end_date: str = 'None', backend: Literal['api', 'local', 'infer'] = 'api', *args, **kwargs)</code>","text":"<p>Combines get_arxiv_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>List of keywords for arxiv search. The outer list level will be considered as AND separated keys, the inner level as OR separated.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. Defaults to ['title', 'authors', 'date', 'abstract', 'journal', 'doi'].</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> <code>backend</code> <code>Literal['api', 'local', 'infer']</code> <p>If <code>api</code>, the arXiv API is queried. If <code>local</code> the local arXiv dump is queried (has to be downloaded before). If <code>infer</code> the local dump will be used if exists, otherwise API will be queried. Defaults to <code>api</code> since it is faster.</p> <code>'api'</code> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_and_dump_arxiv_papers(\n    keywords: List[Union[str, List[str]]],\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n    backend: Literal[\"api\", \"local\", \"infer\"] = \"api\",\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Combines get_arxiv_papers and dump_papers.\n\n    Args:\n        keywords: List of keywords for arxiv search.\n            The outer list level will be considered as AND separated keys, the\n            inner level as OR separated.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n            Defaults to ['title', 'authors', 'date', 'abstract',\n            'journal', 'doi'].\n        start_date: Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date: End date for the search. Same notation as start_date.\n        backend: If `api`, the arXiv API is queried. If `local` the local arXiv dump\n            is queried (has to be downloaded before). If `infer` the local dump will\n            be used if exists, otherwise API will be queried. Defaults to `api`\n            since it is faster.\n        *args, **kwargs are additional arguments for `get_arxiv_papers`.\n    \"\"\"\n    # Translate keywords into query.\n    query = get_query_from_keywords(keywords, start_date=start_date, end_date=end_date)\n\n    if backend not in {\"api\", \"local\", \"infer\"}:\n        raise ValueError(\n            f\"Invalid backend: {backend}. Must be one of ['api', 'local', 'infer']\"\n        )\n    elif backend == \"infer\":\n        backend = infer_backend()\n\n    if backend == \"api\":\n        papers = get_arxiv_papers_api(query, fields, *args, **kwargs)\n    elif backend == \"local\":\n        papers = get_arxiv_papers_local(keywords, fields, *args, **kwargs)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.arxiv","title":"<code>arxiv</code>","text":""},{"location":"api_reference/#paperscraper.arxiv.arxiv.get_arxiv_papers_local","title":"<code>get_arxiv_papers_local(keywords: List[Union[str, List[str]]], fields: List[str] = None, output_filepath: str = None) -&gt; pd.DataFrame</code>","text":"<p>Search for papers in the dump using keywords.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>fields</code> <code>List[str]</code> <p>fields to be used in the query search. Defaults to None, a.k.a. search in all fields excluding date.</p> <code>None</code> <code>output_filepath</code> <code>str</code> <p>optional output filepath where to store the hits in JSONL format. Defaults to None, a.k.a., no export to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with one paper per row.</p> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_arxiv_papers_local(\n    keywords: List[Union[str, List[str]]],\n    fields: List[str] = None,\n    output_filepath: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Search for papers in the dump using keywords.\n\n    Args:\n        keywords: Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        fields: fields to be used in the query search.\n            Defaults to None, a.k.a. search in all fields excluding date.\n        output_filepath: optional output filepath where to store the hits in JSONL format.\n            Defaults to None, a.k.a., no export to a file.\n\n    Returns:\n        pd.DataFrame: A dataframe with one paper per row.\n    \"\"\"\n    search_local_arxiv()\n    if ARXIV_QUERIER is None:\n        raise ValueError(\n            \"Could not find local arxiv dump. Use `backend=api` or download dump via `paperscraper.get_dumps.arxiv\"\n        )\n    return ARXIV_QUERIER(\n        keywords=keywords, fields=fields, output_filepath=output_filepath\n    )\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.arxiv.get_arxiv_papers_api","title":"<code>get_arxiv_papers_api(query: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], max_results: int = 99999, client_options: Dict = {'num_retries': 10}, search_options: Dict = dict(), verbose: bool = True) -&gt; pd.DataFrame</code>","text":"<p>Performs arxiv API request of a given query and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to arxiv API. Needs to match the arxiv API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>max_results</code> <code>int</code> <p>Maximal number of results, defaults to 99999.</p> <code>99999</code> <code>client_options</code> <code>Dict</code> <p>Optional arguments for <code>arxiv.Client</code>. E.g.: page_size (int), delay_seconds (int), num_retries (int). NOTE: Decreasing 'num_retries' will speed up processing but might result in more frequent 'UnexpectedEmptyPageErrors'.</p> <code>{'num_retries': 10}</code> <code>search_options</code> <code>Dict</code> <p>Optional arguments for <code>arxiv.Search</code>. E.g.: id_list (List), sort_by, or sort_order.</p> <code>dict()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: One row per paper.</p> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_arxiv_papers_api(\n    query: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    max_results: int = 99999,\n    client_options: Dict = {\"num_retries\": 10},\n    search_options: Dict = dict(),\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs arxiv API request of a given query and returns list of papers with\n    fields as desired.\n\n    Args:\n        query: Query to arxiv API. Needs to match the arxiv API notation.\n        fields: List of strings with fields to keep in output.\n        max_results: Maximal number of results, defaults to 99999.\n        client_options: Optional arguments for `arxiv.Client`. E.g.:\n            page_size (int), delay_seconds (int), num_retries (int).\n            NOTE: Decreasing 'num_retries' will speed up processing but might\n            result in more frequent 'UnexpectedEmptyPageErrors'.\n        search_options: Optional arguments for `arxiv.Search`. E.g.:\n            id_list (List), sort_by, or sort_order.\n\n    Returns:\n        pd.DataFrame: One row per paper.\n\n    \"\"\"\n    client = arxiv.Client(**client_options)\n    search = arxiv.Search(query=query, max_results=max_results, **search_options)\n    results = client.results(search)\n\n    processed = pd.DataFrame(\n        [\n            {\n                arxiv_field_mapper.get(key, key): process_fields.get(\n                    arxiv_field_mapper.get(key, key), lambda x: x\n                )(value)\n                for key, value in vars(paper).items()\n                if arxiv_field_mapper.get(key, key) in fields and key != \"doi\"\n            }\n            for paper in tqdm(results, desc=f\"Processing {query}\", disable=not verbose)\n        ]\n    )\n    return processed\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.arxiv.get_and_dump_arxiv_papers","title":"<code>get_and_dump_arxiv_papers(keywords: List[Union[str, List[str]]], output_filepath: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], start_date: str = 'None', end_date: str = 'None', backend: Literal['api', 'local', 'infer'] = 'api', *args, **kwargs)</code>","text":"<p>Combines get_arxiv_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>List of keywords for arxiv search. The outer list level will be considered as AND separated keys, the inner level as OR separated.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. Defaults to ['title', 'authors', 'date', 'abstract', 'journal', 'doi'].</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> <code>backend</code> <code>Literal['api', 'local', 'infer']</code> <p>If <code>api</code>, the arXiv API is queried. If <code>local</code> the local arXiv dump is queried (has to be downloaded before). If <code>infer</code> the local dump will be used if exists, otherwise API will be queried. Defaults to <code>api</code> since it is faster.</p> <code>'api'</code> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_and_dump_arxiv_papers(\n    keywords: List[Union[str, List[str]]],\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n    backend: Literal[\"api\", \"local\", \"infer\"] = \"api\",\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Combines get_arxiv_papers and dump_papers.\n\n    Args:\n        keywords: List of keywords for arxiv search.\n            The outer list level will be considered as AND separated keys, the\n            inner level as OR separated.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n            Defaults to ['title', 'authors', 'date', 'abstract',\n            'journal', 'doi'].\n        start_date: Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date: End date for the search. Same notation as start_date.\n        backend: If `api`, the arXiv API is queried. If `local` the local arXiv dump\n            is queried (has to be downloaded before). If `infer` the local dump will\n            be used if exists, otherwise API will be queried. Defaults to `api`\n            since it is faster.\n        *args, **kwargs are additional arguments for `get_arxiv_papers`.\n    \"\"\"\n    # Translate keywords into query.\n    query = get_query_from_keywords(keywords, start_date=start_date, end_date=end_date)\n\n    if backend not in {\"api\", \"local\", \"infer\"}:\n        raise ValueError(\n            f\"Invalid backend: {backend}. Must be one of ['api', 'local', 'infer']\"\n        )\n    elif backend == \"infer\":\n        backend = infer_backend()\n\n    if backend == \"api\":\n        papers = get_arxiv_papers_api(query, fields, *args, **kwargs)\n    elif backend == \"local\":\n        papers = get_arxiv_papers_local(keywords, fields, *args, **kwargs)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/#paperscraper.arxiv.utils.format_date","title":"<code>format_date(date_str: str) -&gt; str</code>","text":"<p>Converts a date in YYYY-MM-DD format to arXiv's YYYYMMDDTTTT format.</p> Source code in <code>paperscraper/arxiv/utils.py</code> <pre><code>def format_date(date_str: str) -&gt; str:\n    \"\"\"Converts a date in YYYY-MM-DD format to arXiv's YYYYMMDDTTTT format.\"\"\"\n    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n    return date_obj.strftime(\"%Y%m%d0000\")\n</code></pre>"},{"location":"api_reference/#paperscraper.arxiv.utils.get_query_from_keywords","title":"<code>get_query_from_keywords(keywords: List[Union[str, List[str]]], start_date: str = 'None', end_date: str = 'None') -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the arxiv API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to arxiv API.</p> Source code in <code>paperscraper/arxiv/utils.py</code> <pre><code>def get_query_from_keywords(\n    keywords: List[Union[str, List[str]]],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the arxiv API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        start_date (str): Start date for the search. Needs to be in format:\n            YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date (str): End date for the search. Same notation as start_date.\n\n    Returns:\n        str: query to enter to arxiv API.\n    \"\"\"\n\n    query = \"\"\n    for i, key in enumerate(keywords):\n        if isinstance(key, str):\n            query += f\"all:{key} AND \"\n        elif isinstance(key, list):\n            inter = \"\".join([f\"all:{syn} OR \" for syn in key])\n            query += finalize_disjunction(inter)\n\n    query = finalize_conjunction(query)\n    if start_date == \"None\" and end_date == \"None\":\n        return query\n    elif start_date == \"None\":\n        start_date = EARLIEST_START\n    elif end_date == \"None\":\n        end_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    start = format_date(start_date)\n    end = format_date(end_date)\n    date_filter = f\" AND submittedDate:[{start} TO {end}]\"\n    return query + date_filter\n</code></pre>"},{"location":"api_reference/#paperscraper.async_utils","title":"<code>async_utils</code>","text":""},{"location":"api_reference/#paperscraper.async_utils.optional_async","title":"<code>optional_async(func: Callable[..., Awaitable[T]]) -&gt; Callable[..., Union[T, Awaitable[T]]]</code>","text":"<p>Allows an async function to be called from sync code (blocks until done) or from within an async context (returns a coroutine to await).</p> Source code in <code>paperscraper/async_utils.py</code> <pre><code>def optional_async(\n    func: Callable[..., Awaitable[T]],\n) -&gt; Callable[..., Union[T, Awaitable[T]]]:\n    \"\"\"\n    Allows an async function to be called from sync code (blocks until done)\n    or from within an async context (returns a coroutine to await).\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs) -&gt; Union[T, Awaitable[T]]:\n        coro = func(*args, **kwargs)\n        try:\n            # If we're already in an asyncio loop, hand back the coroutine:\n            asyncio.get_running_loop()\n            return coro  # caller must await it\n        except RuntimeError:\n            # Otherwise, schedule on the background loop and block\n            future = asyncio.run_coroutine_threadsafe(coro, _background_loop)\n            return future.result()\n\n    return wrapper\n</code></pre>"},{"location":"api_reference/#paperscraper.async_utils.retry_with_exponential_backoff","title":"<code>retry_with_exponential_backoff(*, max_retries: int = 5, base_delay: float = 1.0) -&gt; Callable[[F], F]</code>","text":"<p>Decorator factory that retries an <code>async def</code> on HTTP 429, with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>how many times to retry before giving up.</p> <code>5</code> <code>base_delay</code> <code>float</code> <p>initial delay in seconds; next delays will be duplication of previous.</p> <code>1.0</code> <pre><code>@retry_with_exponential_backoff(max_retries=3, base_delay=0.5)\nasync def fetch_data(...):\n    ...\n</code></pre> Source code in <code>paperscraper/async_utils.py</code> <pre><code>def retry_with_exponential_backoff(\n    *, max_retries: int = 5, base_delay: float = 1.0\n) -&gt; Callable[[F], F]:\n    \"\"\"\n    Decorator factory that retries an `async def` on HTTP 429, with exponential backoff.\n\n    Args:\n        max_retries: how many times to retry before giving up.\n        base_delay: initial delay in seconds; next delays will be duplication of previous.\n\n    Usage:\n\n        @retry_with_exponential_backoff(max_retries=3, base_delay=0.5)\n        async def fetch_data(...):\n            ...\n\n    \"\"\"\n\n    def decorator(func: F) -&gt; F:\n        @wraps(func)\n        async def wrapper(*args, **kwargs) -&gt; Any:\n            delay = base_delay\n            for attempt in range(max_retries):\n                try:\n                    return await func(*args, **kwargs)\n                except httpx.HTTPStatusError as e:\n                    # only retry on 429\n                    status = e.response.status_code if e.response is not None else None\n                    if status != 429 or attempt == max_retries - 1:\n                        raise\n                # backoff\n                await asyncio.sleep(delay)\n                delay *= 2\n            # in theory we never reach here\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api_reference/#paperscraper.citations","title":"<code>citations</code>","text":""},{"location":"api_reference/#paperscraper.citations.citations","title":"<code>citations</code>","text":""},{"location":"api_reference/#paperscraper.citations.citations.get_citations_by_doi","title":"<code>get_citations_by_doi(doi: str) -&gt; int</code>","text":"<p>Get the number of citations of a paper according to semantic scholar.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>the DOI of the paper.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of citations</p> Source code in <code>paperscraper/citations/citations.py</code> <pre><code>def get_citations_by_doi(doi: str) -&gt; int:\n    \"\"\"\n    Get the number of citations of a paper according to semantic scholar.\n\n    Args:\n        doi: the DOI of the paper.\n\n    Returns:\n        The number of citations\n    \"\"\"\n\n    try:\n        paper = sch.get_paper(doi)\n        citations = len(paper[\"citations\"])\n    except SemanticScholarException.ObjectNotFoundException:\n        logger.warning(f\"Could not find paper {doi}, assuming 0 citation.\")\n        citations = 0\n    except ConnectionRefusedError as e:\n        logger.warning(f\"Waiting for 10 sec since {doi} gave: {e}\")\n        sleep(10)\n        citations = len(sch.get_paper(doi)[\"citations\"])\n    finally:\n        return citations\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.citations.get_citations_from_title","title":"<code>get_citations_from_title(title: str) -&gt; int</code>","text":"<p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of paper to be searched on Scholar.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If sth else than str is passed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of citations of paper.</p> Source code in <code>paperscraper/citations/citations.py</code> <pre><code>def get_citations_from_title(title: str) -&gt; int:\n    \"\"\"\n    Args:\n        title (str): Title of paper to be searched on Scholar.\n\n    Raises:\n        TypeError: If sth else than str is passed.\n\n    Returns:\n        int: Number of citations of paper.\n    \"\"\"\n\n    if not isinstance(title, str):\n        raise TypeError(f\"Pass str not {type(title)}\")\n\n    # Search for exact match\n    title = '\"' + title.strip() + '\"'\n\n    matches = scholarly.search_pubs(title)\n    counts = list(map(lambda p: int(p[\"num_citations\"]), matches))\n    if len(counts) == 0:\n        logger.warning(f\"Found no match for {title}.\")\n        return 0\n    if len(counts) &gt; 1:\n        logger.warning(f\"Found {len(counts)} matches for {title}, returning first one.\")\n    return counts[0]\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.entity","title":"<code>entity</code>","text":""},{"location":"api_reference/#paperscraper.citations.entity.core","title":"<code>core</code>","text":""},{"location":"api_reference/#paperscraper.citations.entity.core.Entity","title":"<code>Entity</code>","text":"<p>An abstract entity class with a set of utilities shared by the objects that perform self-linking analyses, such as Paper and Researcher.</p> Source code in <code>paperscraper/citations/entity/core.py</code> <pre><code>class Entity:\n    \"\"\"\n    An abstract entity class with a set of utilities shared by the objects that perform\n    self-linking analyses, such as Paper and Researcher.\n    \"\"\"\n\n    @abstractmethod\n    def self_references(self):\n        \"\"\"\n        Has to be implemented by the child class. Performs a self-referencing analyses\n        for the object.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def self_citations(self):\n        \"\"\"\n        Has to be implemented by the child class. Performs a self-citation analyses\n        for the object.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_result(self):\n        \"\"\"\n        Has to be implemented by the child class. Provides the result of the analysis.\n        \"\"\"\n        ...\n</code></pre> <code></code> <code>self_references()</code> <code>abstractmethod</code> \u00b6 <p>Has to be implemented by the child class. Performs a self-referencing analyses for the object.</p> Source code in <code>paperscraper/citations/entity/core.py</code> <pre><code>@abstractmethod\ndef self_references(self):\n    \"\"\"\n    Has to be implemented by the child class. Performs a self-referencing analyses\n    for the object.\n    \"\"\"\n    ...\n</code></pre> <code></code> <code>self_citations()</code> <code>abstractmethod</code> \u00b6 <p>Has to be implemented by the child class. Performs a self-citation analyses for the object.</p> Source code in <code>paperscraper/citations/entity/core.py</code> <pre><code>@abstractmethod\ndef self_citations(self):\n    \"\"\"\n    Has to be implemented by the child class. Performs a self-citation analyses\n    for the object.\n    \"\"\"\n    ...\n</code></pre> <code></code> <code>get_result()</code> <code>abstractmethod</code> \u00b6 <p>Has to be implemented by the child class. Provides the result of the analysis.</p> Source code in <code>paperscraper/citations/entity/core.py</code> <pre><code>@abstractmethod\ndef get_result(self):\n    \"\"\"\n    Has to be implemented by the child class. Provides the result of the analysis.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.entity.paper","title":"<code>paper</code>","text":""},{"location":"api_reference/#paperscraper.citations.entity.paper.Paper","title":"<code>Paper</code>","text":"<p>               Bases: <code>Entity</code></p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>class Paper(Entity):\n    title: str = \"\"\n    doi: str = \"\"\n    authors: List[str] = []\n\n    def __init__(self, input: str, mode: ModeType = \"infer\"):\n        \"\"\"\n        Set up a Paper object for analysis.\n\n        Args:\n            input: Paper identifier. This can be the title, DOI or semantic scholar ID\n                of the paper.\n            mode: The format in which the ID was provided. Defaults to \"infer\".\n\n        Raises:\n            ValueError: If unknown mode is given.\n        \"\"\"\n        if mode not in MODES:\n            raise ValueError(f\"Unknown mode {mode} chose from {MODES}.\")\n\n        input = input.strip()\n        self.input = input\n        if mode == \"infer\":\n            mode = determine_paper_input_type(input)\n\n        if mode == \"doi\":\n            self.doi = input\n        elif mode == \"title\":\n            self.doi = get_doi_from_title(input)\n        elif mode == \"ssid\":\n            self.doi = get_doi_from_ssid(input)\n\n        if self.doi is not None:\n            out = get_title_and_id_from_doi(self.doi)\n            if out is not None:\n                self.title = out[\"title\"]\n                self.ssid = out[\"ssid\"]\n\n    def self_references(self):\n        \"\"\"\n        Extracts the self references of a paper, for each author.\n        \"\"\"\n        if isinstance(self.doi, str):\n            self.ref_result: ReferenceResult = self_references_paper(self.doi)\n\n    def self_citations(self):\n        \"\"\"\n        Extracts the self citations of a paper, for each author.\n        \"\"\"\n        if isinstance(self.doi, str):\n            self.citation_result: CitationResult = self_citations_paper(self.doi)\n\n    def get_result(self) -&gt; Optional[PaperResult]:\n        \"\"\"\n        Provides the result of the analysis.\n\n        Returns: PaperResult if available.\n        \"\"\"\n        if not hasattr(self, \"ref_result\"):\n            logger.warning(\n                f\"Can't get result since no referencing result for {self.input} exists. Run `.self_references` first.\"\n            )\n            return\n        elif not hasattr(self, \"citation_result\"):\n            logger.warning(\n                f\"Can't get result since no citation result for {self.input} exists. Run `.self_citations` first.\"\n            )\n            return\n        ref_result = self.ref_result.model_dump()\n        ref_result.pop(\"ssid\", None)\n        return PaperResult(\n            title=self.title, **ref_result, **self.citation_result.model_dump()\n        )\n</code></pre> <code></code> <code>__init__(input: str, mode: ModeType = 'infer')</code> \u00b6 <p>Set up a Paper object for analysis.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>Paper identifier. This can be the title, DOI or semantic scholar ID of the paper.</p> required <code>mode</code> <code>ModeType</code> <p>The format in which the ID was provided. Defaults to \"infer\".</p> <code>'infer'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown mode is given.</p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>def __init__(self, input: str, mode: ModeType = \"infer\"):\n    \"\"\"\n    Set up a Paper object for analysis.\n\n    Args:\n        input: Paper identifier. This can be the title, DOI or semantic scholar ID\n            of the paper.\n        mode: The format in which the ID was provided. Defaults to \"infer\".\n\n    Raises:\n        ValueError: If unknown mode is given.\n    \"\"\"\n    if mode not in MODES:\n        raise ValueError(f\"Unknown mode {mode} chose from {MODES}.\")\n\n    input = input.strip()\n    self.input = input\n    if mode == \"infer\":\n        mode = determine_paper_input_type(input)\n\n    if mode == \"doi\":\n        self.doi = input\n    elif mode == \"title\":\n        self.doi = get_doi_from_title(input)\n    elif mode == \"ssid\":\n        self.doi = get_doi_from_ssid(input)\n\n    if self.doi is not None:\n        out = get_title_and_id_from_doi(self.doi)\n        if out is not None:\n            self.title = out[\"title\"]\n            self.ssid = out[\"ssid\"]\n</code></pre> <code></code> <code>self_references()</code> \u00b6 <p>Extracts the self references of a paper, for each author.</p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>def self_references(self):\n    \"\"\"\n    Extracts the self references of a paper, for each author.\n    \"\"\"\n    if isinstance(self.doi, str):\n        self.ref_result: ReferenceResult = self_references_paper(self.doi)\n</code></pre> <code></code> <code>self_citations()</code> \u00b6 <p>Extracts the self citations of a paper, for each author.</p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>def self_citations(self):\n    \"\"\"\n    Extracts the self citations of a paper, for each author.\n    \"\"\"\n    if isinstance(self.doi, str):\n        self.citation_result: CitationResult = self_citations_paper(self.doi)\n</code></pre> <code></code> <code>get_result() -&gt; Optional[PaperResult]</code> \u00b6 <p>Provides the result of the analysis.</p> <p>Returns: PaperResult if available.</p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>def get_result(self) -&gt; Optional[PaperResult]:\n    \"\"\"\n    Provides the result of the analysis.\n\n    Returns: PaperResult if available.\n    \"\"\"\n    if not hasattr(self, \"ref_result\"):\n        logger.warning(\n            f\"Can't get result since no referencing result for {self.input} exists. Run `.self_references` first.\"\n        )\n        return\n    elif not hasattr(self, \"citation_result\"):\n        logger.warning(\n            f\"Can't get result since no citation result for {self.input} exists. Run `.self_citations` first.\"\n        )\n        return\n    ref_result = self.ref_result.model_dump()\n    ref_result.pop(\"ssid\", None)\n    return PaperResult(\n        title=self.title, **ref_result, **self.citation_result.model_dump()\n    )\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.entity.researcher","title":"<code>researcher</code>","text":""},{"location":"api_reference/#paperscraper.citations.entity.researcher.Researcher","title":"<code>Researcher</code>","text":"<p>               Bases: <code>Entity</code></p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>class Researcher(Entity):\n    name: str\n    ssid: int\n    orcid: Optional[str] = None\n\n    def __init__(self, input: str, mode: ModeType = \"infer\"):\n        \"\"\"\n        Construct researcher object for self citation/reference analysis.\n\n        Args:\n            input: A researcher to search for.\n            mode: This can be a `name` `orcid` (ORCID iD) or `ssaid` (Semantic Scholar Author ID).\n                Defaults to \"infer\".\n\n        Raises:\n            ValueError: Unknown mode\n        \"\"\"\n        if mode not in MODES:\n            raise ValueError(f\"Unknown mode {mode} chose from {MODES}.\")\n\n        input = input.strip()\n        if mode == \"infer\":\n            if input.isdigit():\n                mode = \"ssaid\"\n            elif (\n                input.count(\"-\") == 3\n                and len(input) == 19\n                and all([x.isdigit() for x in input.split(\"-\")])\n            ):\n                mode = \"orcid\"\n            else:\n                mode = \"author\"\n\n        if mode == \"ssaid\":\n            self.author = sch.get_author(input)\n            self.ssid = input\n        elif mode == \"orcid\":\n            self.author = orcid_to_author_name(input)\n            self.orcid = input\n            self.ssid = author_name_to_ssaid(input)\n        elif mode == \"author\":\n            self.author = input\n            self.ssid = author_name_to_ssaid(input)\n\n        # TODO: Skip over erratum / corrigendum\n        self.ssids = get_papers_for_author(self.ssid)\n\n    def self_references(self):\n        \"\"\"\n        Sifts through all papers of a researcher and extracts the self references.\n        \"\"\"\n        # TODO: Asynchronous call to self_references\n        print(\"Going through SSIDs\", self.ssids)\n\n        # TODO: Aggregate results\n\n    def self_citations(self):\n        \"\"\"\n        Sifts through all papers of a researcher and finds how often they are self-cited.\n        \"\"\"\n        ...\n\n    def get_result(self) -&gt; ResearcherResult:\n        \"\"\"\n        Provides the result of the analysis.\n        \"\"\"\n        ...\n</code></pre> <code></code> <code>__init__(input: str, mode: ModeType = 'infer')</code> \u00b6 <p>Construct researcher object for self citation/reference analysis.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>A researcher to search for.</p> required <code>mode</code> <code>ModeType</code> <p>This can be a <code>name</code> <code>orcid</code> (ORCID iD) or <code>ssaid</code> (Semantic Scholar Author ID). Defaults to \"infer\".</p> <code>'infer'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Unknown mode</p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>def __init__(self, input: str, mode: ModeType = \"infer\"):\n    \"\"\"\n    Construct researcher object for self citation/reference analysis.\n\n    Args:\n        input: A researcher to search for.\n        mode: This can be a `name` `orcid` (ORCID iD) or `ssaid` (Semantic Scholar Author ID).\n            Defaults to \"infer\".\n\n    Raises:\n        ValueError: Unknown mode\n    \"\"\"\n    if mode not in MODES:\n        raise ValueError(f\"Unknown mode {mode} chose from {MODES}.\")\n\n    input = input.strip()\n    if mode == \"infer\":\n        if input.isdigit():\n            mode = \"ssaid\"\n        elif (\n            input.count(\"-\") == 3\n            and len(input) == 19\n            and all([x.isdigit() for x in input.split(\"-\")])\n        ):\n            mode = \"orcid\"\n        else:\n            mode = \"author\"\n\n    if mode == \"ssaid\":\n        self.author = sch.get_author(input)\n        self.ssid = input\n    elif mode == \"orcid\":\n        self.author = orcid_to_author_name(input)\n        self.orcid = input\n        self.ssid = author_name_to_ssaid(input)\n    elif mode == \"author\":\n        self.author = input\n        self.ssid = author_name_to_ssaid(input)\n\n    # TODO: Skip over erratum / corrigendum\n    self.ssids = get_papers_for_author(self.ssid)\n</code></pre> <code></code> <code>self_references()</code> \u00b6 <p>Sifts through all papers of a researcher and extracts the self references.</p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>def self_references(self):\n    \"\"\"\n    Sifts through all papers of a researcher and extracts the self references.\n    \"\"\"\n    # TODO: Asynchronous call to self_references\n    print(\"Going through SSIDs\", self.ssids)\n</code></pre> <code></code> <code>self_citations()</code> \u00b6 <p>Sifts through all papers of a researcher and finds how often they are self-cited.</p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>def self_citations(self):\n    \"\"\"\n    Sifts through all papers of a researcher and finds how often they are self-cited.\n    \"\"\"\n    ...\n</code></pre> <code></code> <code>get_result() -&gt; ResearcherResult</code> \u00b6 <p>Provides the result of the analysis.</p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>def get_result(self) -&gt; ResearcherResult:\n    \"\"\"\n    Provides the result of the analysis.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.orcid","title":"<code>orcid</code>","text":""},{"location":"api_reference/#paperscraper.citations.orcid.orcid_to_author_name","title":"<code>orcid_to_author_name(orcid_id: str) -&gt; Optional[str]</code>","text":"<p>Given an ORCID ID (as a string, e.g. '0000-0002-1825-0097'), returns the full name of the author from the ORCID public API.</p> Source code in <code>paperscraper/citations/orcid.py</code> <pre><code>def orcid_to_author_name(orcid_id: str) -&gt; Optional[str]:\n    \"\"\"\n    Given an ORCID ID (as a string, e.g. '0000-0002-1825-0097'),\n    returns the full name of the author from the ORCID public API.\n    \"\"\"\n\n    headers = {\"Accept\": \"application/json\"}\n    response = requests.get(f\"{BASE_URL}{orcid_id}/person\", headers=headers)\n    if response.status_code == 200:\n        data = response.json()\n        given = data.get(\"name\", {}).get(\"given-names\", {}).get(\"value\", \"\")\n        family = data.get(\"name\", {}).get(\"family-name\", {}).get(\"value\", \"\")\n        full_name = f\"{given} {family}\".strip()\n        return full_name\n    logger.error(\n        f\"Error fetching ORCID data ({orcid_id}): {response.status_code} {response.text}\"\n    )\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.self_citations","title":"<code>self_citations</code>","text":""},{"location":"api_reference/#paperscraper.citations.self_citations.self_citations_paper","title":"<code>self_citations_paper(inputs: Union[str, List[str]], verbose: bool = False) -&gt; Union[CitationResult, List[CitationResult]]</code>  <code>async</code>","text":"<p>Analyze self-citations for one or more papers by DOI or Semantic Scholar ID.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>A single DOI/SSID string or a list of them.</p> required <code>verbose</code> <code>bool</code> <p>If True, logs detailed information for each paper.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[CitationResult, List[CitationResult]]</code> <p>A single CitationResult if a string was passed, else a list of CitationResults.</p> Source code in <code>paperscraper/citations/self_citations.py</code> <pre><code>@optional_async\n@retry_with_exponential_backoff(max_retries=4, base_delay=1.0)\nasync def self_citations_paper(\n    inputs: Union[str, List[str]], verbose: bool = False\n) -&gt; Union[CitationResult, List[CitationResult]]:\n    \"\"\"\n    Analyze self-citations for one or more papers by DOI or Semantic Scholar ID.\n\n    Args:\n        inputs: A single DOI/SSID string or a list of them.\n        verbose: If True, logs detailed information for each paper.\n\n    Returns:\n        A single CitationResult if a string was passed, else a list of CitationResults.\n    \"\"\"\n    single_input = isinstance(inputs, str)\n    identifiers = [inputs] if single_input else list(inputs)\n\n    async with httpx.AsyncClient(timeout=httpx.Timeout(20)) as client:\n        tasks = [_process_single(client, ident) for ident in identifiers]\n        results = await asyncio.gather(*tasks)\n\n    if verbose:\n        for res in results:\n            logger.info(\n                f'Self-citations in \"{res.ssid}\": N={res.num_citations}, Score={res.citation_score}%'\n            )\n            for author, pct in res.self_citations.items():\n                logger.info(f\"  {author}: {pct}%\")\n\n    return results[0] if single_input else results\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.self_references","title":"<code>self_references</code>","text":""},{"location":"api_reference/#paperscraper.citations.self_references.self_references_paper","title":"<code>self_references_paper(inputs: Union[str, List[str]], verbose: bool = False) -&gt; Union[ReferenceResult, List[ReferenceResult]]</code>  <code>async</code>","text":"<p>Analyze self-references for one or more papers by DOI or Semantic Scholar ID.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>A single DOI/SSID string or a list of them.</p> required <code>verbose</code> <code>bool</code> <p>If True, logs detailed information for each paper.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ReferenceResult, List[ReferenceResult]]</code> <p>A single ReferenceResult if a string was passed, else a list of ReferenceResults.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no references are found for a given identifier.</p> Source code in <code>paperscraper/citations/self_references.py</code> <pre><code>@optional_async\n@retry_with_exponential_backoff(max_retries=4, base_delay=1.0)\nasync def self_references_paper(\n    inputs: Union[str, List[str]], verbose: bool = False\n) -&gt; Union[ReferenceResult, List[ReferenceResult]]:\n    \"\"\"\n    Analyze self-references for one or more papers by DOI or Semantic Scholar ID.\n\n    Args:\n        inputs: A single DOI/SSID string or a list of them.\n        verbose: If True, logs detailed information for each paper.\n\n    Returns:\n        A single ReferenceResult if a string was passed, else a list of ReferenceResults.\n\n    Raises:\n        ValueError: If no references are found for a given identifier.\n    \"\"\"\n    single_input = isinstance(inputs, str)\n    identifiers = [inputs] if single_input else list(inputs)\n\n    async with httpx.AsyncClient(timeout=httpx.Timeout(20)) as client:\n        tasks = [_process_single_reference(client, ident) for ident in identifiers]\n        results = await asyncio.gather(*tasks)\n\n    if verbose:\n        for res in results:\n            logger.info(\n                f'Self-references in \"{res.ssid}\": N={res.num_references}, '\n                f\"Score={res.reference_score}%\"\n            )\n            for author, pct in res.self_references.items():\n                logger.info(f\"  {author}: {pct}% self-reference\")\n\n    return results[0] if single_input else results\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.tests","title":"<code>tests</code>","text":""},{"location":"api_reference/#paperscraper.citations.tests.test_self_references","title":"<code>test_self_references</code>","text":""},{"location":"api_reference/#paperscraper.citations.tests.test_self_references.TestSelfReferences","title":"<code>TestSelfReferences</code>","text":"Source code in <code>paperscraper/citations/tests/test_self_references.py</code> <pre><code>class TestSelfReferences:\n    @pytest.fixture\n    def dois(self):\n        return [\n            \"10.1038/s41586-023-06600-9\",\n            \"10.1016/j.neunet.2014.09.003\",\n        ]\n\n    def test_single_doi(self, dois):\n        result = self_references_paper(dois[0])\n        assert isinstance(result, ReferenceResult)\n        assert isinstance(result.num_references, int)\n        assert result.num_references &gt; 0\n        assert isinstance(result.ssid, str)\n        assert isinstance(result.reference_score, float)\n        assert result.reference_score &gt; 0\n        assert isinstance(result.self_references, Dict)\n        for author, self_cites in result.self_references.items():\n            assert isinstance(author, str)\n            assert isinstance(self_cites, float)\n            assert self_cites &gt;= 0 and self_cites &lt;= 100\n\n    def test_multiple_dois(self, dois):\n        results = self_references_paper(dois[1:])\n        assert isinstance(results, list)\n        assert len(results) == len(dois[1:])\n        for ref_result in results:\n            assert isinstance(ref_result, ReferenceResult)\n            assert isinstance(ref_result.ssid, str)\n            assert isinstance(ref_result.num_references, int)\n            assert ref_result.num_references &gt; 0\n            assert ref_result.reference_score &gt; 0\n            assert isinstance(ref_result.reference_score, float)\n            for author, self_cites in ref_result.self_references.items():\n                assert isinstance(author, str)\n                assert isinstance(self_cites, float)\n                assert self_cites &gt;= 0 and self_cites &lt;= 100\n\n    def test_compare_async_and_sync_performance(self, dois):\n        \"\"\"\n        Compares the execution time of asynchronous and synchronous `self_references`\n        for a list of DOIs.\n        \"\"\"\n\n        start_time = time.perf_counter()\n        async_results = self_references_paper(dois)\n        async_duration = time.perf_counter() - start_time\n\n        # Measure synchronous execution time (three independent calls)\n        start_time = time.perf_counter()\n        sync_results = [self_references_paper(doi) for doi in dois]\n\n        sync_duration = time.perf_counter() - start_time\n\n        print(f\"Asynchronous execution time (batch): {async_duration:.2f} seconds\")\n        print(\n            f\"Synchronous execution time (independent calls): {sync_duration:.2f} seconds\"\n        )\n        for a, s in zip(async_results, sync_results):\n            assert a == s, f\"{a} vs {s}\"\n\n        assert 0.5 * async_duration &lt;= sync_duration, (\n            f\"Async execution ({async_duration:.2f}s) is slower than sync execution \"\n            f\"({sync_duration:.2f}s)\"\n        )\n</code></pre> <code></code> <code>test_compare_async_and_sync_performance(dois)</code> \u00b6 <p>Compares the execution time of asynchronous and synchronous <code>self_references</code> for a list of DOIs.</p> Source code in <code>paperscraper/citations/tests/test_self_references.py</code> <pre><code>def test_compare_async_and_sync_performance(self, dois):\n    \"\"\"\n    Compares the execution time of asynchronous and synchronous `self_references`\n    for a list of DOIs.\n    \"\"\"\n\n    start_time = time.perf_counter()\n    async_results = self_references_paper(dois)\n    async_duration = time.perf_counter() - start_time\n\n    # Measure synchronous execution time (three independent calls)\n    start_time = time.perf_counter()\n    sync_results = [self_references_paper(doi) for doi in dois]\n\n    sync_duration = time.perf_counter() - start_time\n\n    print(f\"Asynchronous execution time (batch): {async_duration:.2f} seconds\")\n    print(\n        f\"Synchronous execution time (independent calls): {sync_duration:.2f} seconds\"\n    )\n    for a, s in zip(async_results, sync_results):\n        assert a == s, f\"{a} vs {s}\"\n\n    assert 0.5 * async_duration &lt;= sync_duration, (\n        f\"Async execution ({async_duration:.2f}s) is slower than sync execution \"\n        f\"({sync_duration:.2f}s)\"\n    )\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/#paperscraper.citations.utils.get_doi_from_title","title":"<code>get_doi_from_title(title: str) -&gt; Optional[str]</code>","text":"<p>Searches the DOI of a paper based on the paper title</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Paper title</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>DOI according to semantic scholar API</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def get_doi_from_title(title: str) -&gt; Optional[str]:\n    \"\"\"\n    Searches the DOI of a paper based on the paper title\n\n    Args:\n        title: Paper title\n\n    Returns:\n        DOI according to semantic scholar API\n    \"\"\"\n    response = requests.get(\n        PAPER_URL + \"search\",\n        params={\"query\": title, \"fields\": \"externalIds\", \"limit\": 1},\n    )\n    data = response.json()\n\n    if data.get(\"data\"):\n        paper = data[\"data\"][0]\n        doi = paper.get(\"externalIds\", {}).get(\"DOI\")\n        if doi:\n            return doi\n    logger.warning(f\"Did not find DOI for title={title}\")\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils.get_doi_from_ssid","title":"<code>get_doi_from_ssid(ssid: str, max_retries: int = 10) -&gt; Optional[str]</code>","text":"<p>Given a Semantic Scholar paper ID, returns the corresponding DOI if available.</p> <p>Parameters:</p> Name Type Description Default <code>ssid</code> <code>str</code> <p>The paper ID on Semantic Scholar.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>str or None: The DOI of the paper, or None if not found or in case of an error.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def get_doi_from_ssid(ssid: str, max_retries: int = 10) -&gt; Optional[str]:\n    \"\"\"\n    Given a Semantic Scholar paper ID, returns the corresponding DOI if available.\n\n    Parameters:\n      ssid (str): The paper ID on Semantic Scholar.\n\n    Returns:\n      str or None: The DOI of the paper, or None if not found or in case of an error.\n    \"\"\"\n    logger.warning(\n        \"Semantic Scholar API is easily overloaded when passing SS IDs, provide DOIs to improve throughput.\"\n    )\n    attempts = 0\n    for attempt in tqdm(\n        range(1, max_retries + 1), desc=f\"Fetching DOI for {ssid}\", unit=\"attempt\"\n    ):\n        # Make the GET request to Semantic Scholar.\n        response = requests.get(\n            f\"{PAPER_URL}{ssid}\", params={\"fields\": \"externalIds\", \"limit\": 1}\n        )\n\n        # If successful, try to extract and return the DOI.\n        if response.status_code == 200:\n            data = response.json()\n            doi = data.get(\"externalIds\", {}).get(\"DOI\")\n            return doi\n        attempts += 1\n        sleep(10)\n    logger.warning(\n        f\"Did not find DOI for paper ID {ssid}. Code={response.status_code}, text={response.text}\"\n    )\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils.get_title_and_id_from_doi","title":"<code>get_title_and_id_from_doi(doi: str) -&gt; Dict[str, Any]</code>","text":"<p>Given a DOI, retrieves the paper's title and semantic scholar paper ID.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI of the paper (e.g., \"10.18653/v1/N18-3011\").</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>dict or None: A dictionary with keys 'title' and 'ssid'.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def get_title_and_id_from_doi(doi: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Given a DOI, retrieves the paper's title and semantic scholar paper ID.\n\n    Parameters:\n        doi (str): The DOI of the paper (e.g., \"10.18653/v1/N18-3011\").\n\n    Returns:\n        dict or None: A dictionary with keys 'title' and 'ssid'.\n    \"\"\"\n\n    # Send the GET request to Semantic Scholar\n    response = requests.get(f\"{PAPER_URL}DOI:{doi}\")\n    if response.status_code == 200:\n        data = response.json()\n        return {\"title\": data.get(\"title\"), \"ssid\": data.get(\"paperId\")}\n    logger.warning(\n        f\"Could not get authors &amp; semantic scholar ID for DOI={doi}, {response.status_code}: {response.text}\"\n    )\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils.author_name_to_ssaid","title":"<code>author_name_to_ssaid(author_name: str) -&gt; str</code>","text":"<p>Given an author name, returns the Semantic Scholar author ID.</p> <p>Parameters:</p> Name Type Description Default <code>author_name</code> <code>str</code> <p>The full name of the author.</p> required <p>Returns:</p> Type Description <code>str</code> <p>str or None: The Semantic Scholar author ID or None if no author is found.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def author_name_to_ssaid(author_name: str) -&gt; str:\n    \"\"\"\n    Given an author name, returns the Semantic Scholar author ID.\n\n    Parameters:\n        author_name (str): The full name of the author.\n\n    Returns:\n        str or None: The Semantic Scholar author ID or None if no author is found.\n    \"\"\"\n\n    response = requests.get(\n        AUTHOR_URL, params={\"query\": author_name, \"fields\": \"name\", \"limit\": 1}\n    )\n    if response.status_code == 200:\n        data = response.json()\n        authors = data.get(\"data\", [])\n        if authors:\n            # Return the Semantic Scholar author ID from the first result.\n            return authors[0].get(\"authorId\")\n\n    logger.error(\n        f\"Error in retrieving name from SS Author ID: {response.status_code} - {response.text}\"\n    )\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils.determine_paper_input_type","title":"<code>determine_paper_input_type(input: str) -&gt; Literal['ssid', 'doi', 'title']</code>","text":"<p>Determines the intended input type by the user if not explicitly given (<code>infer</code>).</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>Either a DOI or a semantic scholar paper ID or an author name.</p> required <p>Returns:</p> Type Description <code>Literal['ssid', 'doi', 'title']</code> <p>The input type</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def determine_paper_input_type(input: str) -&gt; Literal[\"ssid\", \"doi\", \"title\"]:\n    \"\"\"\n    Determines the intended input type by the user if not explicitly given (`infer`).\n\n    Args:\n        input: Either a DOI or a semantic scholar paper ID or an author name.\n\n    Returns:\n        The input type\n    \"\"\"\n    if len(input) &gt; 15 and \" \" not in input and (input.isalnum() and input.islower()):\n        mode = \"ssid\"\n    elif len(re.findall(DOI_PATTERN, input, re.IGNORECASE)) == 1:\n        mode = \"doi\"\n    else:\n        logger.info(\n            f\"Assuming `{input}` is a paper title, since it seems neither a DOI nor a paper ID\"\n        )\n        mode = \"title\"\n    return mode\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils.get_papers_for_author","title":"<code>get_papers_for_author(ss_author_id: str) -&gt; List[str]</code>  <code>async</code>","text":"<p>Given a Semantic Scholar author ID, returns a list of all Semantic Scholar paper IDs for that author.</p> <p>Parameters:</p> Name Type Description Default <code>ss_author_id</code> <code>str</code> <p>The Semantic Scholar author ID (e.g., \"1741101\").</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of paper IDs (as strings) authored by the given author.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>async def get_papers_for_author(ss_author_id: str) -&gt; List[str]:\n    \"\"\"\n    Given a Semantic Scholar author ID, returns a list of all Semantic Scholar paper IDs for that author.\n\n    Args:\n        ss_author_id (str): The Semantic Scholar author ID (e.g., \"1741101\").\n\n    Returns:\n        A list of paper IDs (as strings) authored by the given author.\n    \"\"\"\n    papers = []\n    offset = 0\n    limit = 100\n\n    async with httpx.AsyncClient() as client:\n        while True:\n            response = await client.get(\n                f\"https://api.semanticscholar.org/graph/v1/author/{ss_author_id}/papers\",\n                params={\"fields\": \"paperId\", \"offset\": offset, \"limit\": limit},\n            )\n            response.raise_for_status()\n            data = response.json()\n            page = data.get(\"data\", [])\n\n            # Extract paper IDs from the current page.\n            for paper in page:\n                if \"paperId\" in paper:\n                    papers.append(paper[\"paperId\"])\n\n            # If fewer papers were returned than the limit, we've reached the end.\n            if len(page) &lt; limit:\n                break\n\n            offset += limit\n\n    return papers\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils.find_matching","title":"<code>find_matching(first: List[Dict[str, str]], second: List[Dict[str, str]]) -&gt; List[str]</code>","text":"<p>Ingests two sets of authors and returns a list of those that match (either based on name     or on author ID).</p> <p>Parameters:</p> Name Type Description Default <code>first</code> <code>List[Dict[str, str]]</code> <p>First set of authors given as list of dict with two keys (<code>authorID</code> and <code>name</code>).</p> required <code>second</code> <code>List[Dict[str, str]]</code> <p>Second set of authors given as list of dict with two same keys.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of names of authors in first list where a match was found.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def find_matching(\n    first: List[Dict[str, str]], second: List[Dict[str, str]]\n) -&gt; List[str]:\n    \"\"\"\n    Ingests two sets of authors and returns a list of those that match (either based on name\n        or on author ID).\n\n    Args:\n        first: First set of authors given as list of dict with two keys (`authorID` and `name`).\n        second: Second set of authors given as list of dict with two same keys.\n\n    Returns:\n        List of names of authors in first list where a match was found.\n    \"\"\"\n    # Check which author IDs overlap\n    second_names = set(map(lambda x: x[\"authorId\"], second))\n    overlap_ids = {f[\"name\"] for f in first if f[\"authorId\"] in second_names}\n\n    overlap_names = {\n        f[\"name\"]\n        for f in first\n        if f[\"authorId\"] not in overlap_ids\n        and any([check_overlap(f[\"name\"], s[\"name\"]) for s in second])\n    }\n    return list(overlap_ids | overlap_names)\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils.check_overlap","title":"<code>check_overlap(n1: str, n2: str) -&gt; bool</code>","text":"<p>Check whether two author names are identical. TODO: This can be made more robust</p> <p>Parameters:</p> Name Type Description Default <code>n1</code> <code>str</code> <p>first name</p> required <code>n2</code> <code>str</code> <p>second name</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether names are identical.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def check_overlap(n1: str, n2: str) -&gt; bool:\n    \"\"\"\n    Check whether two author names are identical.\n    TODO: This can be made more robust\n\n    Args:\n        n1: first name\n        n2: second name\n\n    Returns:\n        bool: Whether names are identical.\n    \"\"\"\n    # remove initials and check for name intersection\n    s1 = {w for w in clean_name(n1).split()}\n    s2 = {w for w in clean_name(n2).split()}\n    return len(s2) &gt; 0 and len(s1 | s2) == len(s1)\n</code></pre>"},{"location":"api_reference/#paperscraper.citations.utils.clean_name","title":"<code>clean_name(s: str) -&gt; str</code>","text":"<p>Clean up a str by removing special characters.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input possibly containing special symbols</p> required <p>Returns:</p> Type Description <code>str</code> <p>Homogenized string.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def clean_name(s: str) -&gt; str:\n    \"\"\"\n    Clean up a str by removing special characters.\n\n    Args:\n        s: Input possibly containing special symbols\n\n    Returns:\n        Homogenized string.\n    \"\"\"\n    return \"\".join(ch for ch in unidecode(s) if ch.isalpha() or ch.isspace()).lower()\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps","title":"<code>get_dumps</code>","text":""},{"location":"api_reference/#paperscraper.get_dumps.arxiv","title":"<code>arxiv</code>","text":"<p>Dump arxiv data in JSONL format.</p>"},{"location":"api_reference/#paperscraper.get_dumps.arxiv.arxiv","title":"<code>arxiv(start_date: Optional[str] = None, end_date: Optional[str] = None, save_path: str = save_path)</code>","text":"<p>Fetches papers from arXiv based on time range, i.e., start_date and end_date. If the start_date and end_date are not provided, fetches papers from the earliest possible date to the current date. The fetched papers are stored in JSONL format.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>Start date in format YYYY-MM-DD. Defaults to None.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>End date in format YYYY-MM-DD. Defaults to None.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path to save the JSONL dump. Defaults to save_path.</p> <code>save_path</code> Source code in <code>paperscraper/get_dumps/arxiv.py</code> <pre><code>def arxiv(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    save_path: str = save_path,\n):\n    \"\"\"\n    Fetches papers from arXiv based on time range, i.e., start_date and end_date.\n    If the start_date and end_date are not provided, fetches papers from the earliest\n    possible date to the current date. The fetched papers are stored in JSONL format.\n\n    Args:\n        start_date (str, optional): Start date in format YYYY-MM-DD. Defaults to None.\n        end_date (str, optional): End date in format YYYY-MM-DD. Defaults to None.\n        save_path (str, optional): Path to save the JSONL dump. Defaults to save_path.\n    \"\"\"\n    # Set default dates\n    EARLIEST_START = \"1991-01-01\"\n    if start_date is None:\n        start_date = EARLIEST_START\n    if end_date is None:\n        end_date = datetime.today().strftime(\"%Y-%m-%d\")\n\n    # Convert dates to datetime objects\n    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n    if start_date &gt; end_date:\n        raise ValueError(\n            f\"start_date {start_date} cannot be later than end_date {end_date}\"\n        )\n\n    # Open file for writing results\n    with open(save_path, \"w\") as fp:\n        progress_bar = tqdm(total=(end_date - start_date).days + 1)\n\n        current_date = start_date\n        while current_date &lt;= end_date:\n            next_date = current_date + timedelta(days=1)\n            progress_bar.set_description(\n                f\"Fetching {current_date.strftime('%Y-%m-%d')}\"\n            )\n\n            # Format dates for query\n            query = f\"submittedDate:[{current_date.strftime('%Y%m%d0000')} TO {next_date.strftime('%Y%m%d0000')}]\"\n            try:\n                papers = get_arxiv_papers_api(\n                    query=query,\n                    fields=[\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n                    verbose=False,\n                )\n                if not papers.empty:\n                    for paper in papers.to_dict(orient=\"records\"):\n                        fp.write(json.dumps(paper) + \"\\n\")\n            except Exception as e:\n                print(f\"Arxiv scraping error: {current_date.strftime('%Y-%m-%d')}: {e}\")\n            current_date = next_date\n            progress_bar.update(1)\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.biorxiv","title":"<code>biorxiv</code>","text":"<p>Dump bioRxiv data in JSONL format.</p>"},{"location":"api_reference/#paperscraper.get_dumps.biorxiv.biorxiv","title":"<code>biorxiv(start_date: Optional[str] = None, end_date: Optional[str] = None, save_path: str = save_path, max_retries: int = 10)</code>","text":"<p>Fetches papers from biorxiv based on time range, i.e., start_date and end_date. If the start_date and end_date are not provided, papers will be fetched from biorxiv from the launch date of biorxiv until the current date. The fetched papers will be stored in jsonl format in save_path.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>begin date expressed as YYYY-MM-DD. Defaults to None, i.e., earliest possible.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>end date expressed as YYYY-MM-DD. Defaults to None, i.e., today.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path where the dump is stored. Defaults to save_path.</p> <code>save_path</code> <code>max_retries</code> <code>int</code> <p>Number of retries when API shows connection issues. Defaults to 10.</p> <code>10</code> Source code in <code>paperscraper/get_dumps/biorxiv.py</code> <pre><code>def biorxiv(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    save_path: str = save_path,\n    max_retries: int = 10,\n):\n    \"\"\"Fetches papers from biorxiv based on time range, i.e., start_date and end_date.\n    If the start_date and end_date are not provided, papers will be fetched from biorxiv\n    from the launch date of biorxiv until the current date. The fetched papers will be\n    stored in jsonl format in save_path.\n\n    Args:\n        start_date (str, optional): begin date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., earliest possible.\n        end_date (str, optional): end date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., today.\n        save_path (str, optional): Path where the dump is stored.\n            Defaults to save_path.\n        max_retries (int, optional): Number of retries when API shows connection issues.\n            Defaults to 10.\n    \"\"\"\n    # create API client\n    api = BioRxivApi(max_retries=max_retries)\n\n    # dump all papers\n    with open(save_path, \"w\") as fp:\n        for index, paper in enumerate(\n            tqdm(api.get_papers(start_date=start_date, end_date=end_date))\n        ):\n            if index &gt; 0:\n                fp.write(os.linesep)\n            fp.write(json.dumps(paper))\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.chemrxiv","title":"<code>chemrxiv</code>","text":"<p>Dump chemRxiv data in JSONL format.</p>"},{"location":"api_reference/#paperscraper.get_dumps.chemrxiv.chemrxiv","title":"<code>chemrxiv(start_date: Optional[str] = None, end_date: Optional[str] = None, save_path: str = save_path) -&gt; None</code>","text":"<p>Fetches papers from bichemrxiv based on time range, i.e., start_date and end_date. If the start_date and end_date are not provided, papers will be fetched from chemrxiv from the launch date of chemrxiv until the current date. The fetched papers will be stored in jsonl format in save_path.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>begin date expressed as YYYY-MM-DD. Defaults to None, i.e., earliest possible.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>end date expressed as YYYY-MM-DD. Defaults to None, i.e., today.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path where the dump is stored. Defaults to save_path.</p> <code>save_path</code> Source code in <code>paperscraper/get_dumps/chemrxiv.py</code> <pre><code>def chemrxiv(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    save_path: str = save_path,\n) -&gt; None:\n    \"\"\"Fetches papers from bichemrxiv based on time range, i.e., start_date and end_date.\n    If the start_date and end_date are not provided, papers will be fetched from chemrxiv\n    from the launch date of chemrxiv until the current date. The fetched papers will be\n    stored in jsonl format in save_path.\n\n    Args:\n        start_date (str, optional): begin date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., earliest possible.\n        end_date (str, optional): end date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., today.\n        save_path (str, optional): Path where the dump is stored.\n            Defaults to save_path.\n    \"\"\"\n\n    # create API client\n    api = ChemrxivAPI(start_date, end_date)\n    # Download the data\n    download_full(save_folder, api)\n    # Convert to JSONL format.\n    parse_dump(save_folder, save_path)\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.medrxiv","title":"<code>medrxiv</code>","text":"<p>Dump medrxiv data in JSONL format.</p>"},{"location":"api_reference/#paperscraper.get_dumps.medrxiv.medrxiv","title":"<code>medrxiv(start_date: Optional[str] = None, end_date: Optional[str] = None, save_path: str = save_path, max_retries: int = 10)</code>","text":"<p>Fetches papers from medrxiv based on time range, i.e., start_date and end_date. If the start_date and end_date are not provided, then papers will be fetched from medrxiv starting from the launch date of medrxiv until current date. The fetched papers will be stored in jsonl format in save_path.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>begin date expressed as YYYY-MM-DD. Defaults to None, i.e., earliest possible.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>end date expressed as YYYY-MM-DD. Defaults to None, i.e., today.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path where the dump is stored. Defaults to save_path.</p> <code>save_path</code> <code>max_retries</code> <code>int</code> <p>Number of retries when API shows connection issues. Defaults to 10.</p> <code>10</code> Source code in <code>paperscraper/get_dumps/medrxiv.py</code> <pre><code>def medrxiv(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    save_path: str = save_path,\n    max_retries: int = 10,\n):\n    \"\"\"Fetches papers from medrxiv based on time range, i.e., start_date and end_date.\n    If the start_date and end_date are not provided, then papers will be fetched from\n    medrxiv starting from the launch date of medrxiv until current date. The fetched\n    papers will be stored in jsonl format in save_path.\n\n    Args:\n        start_date (str, optional): begin date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., earliest possible.\n        end_date (str, optional): end date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., today.\n        save_path (str, optional): Path where the dump is stored.\n            Defaults to save_path.\n        max_retries (int, optional): Number of retries when API shows connection issues.\n            Defaults to 10.\n    \"\"\"\n    # create API client\n    api = MedRxivApi(max_retries=max_retries)\n    # dump all papers\n    with open(save_path, \"w\") as fp:\n        for index, paper in enumerate(\n            tqdm(api.get_papers(start_date=start_date, end_date=end_date))\n        ):\n            if index &gt; 0:\n                fp.write(os.linesep)\n            fp.write(json.dumps(paper))\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/#paperscraper.get_dumps.utils.chemrxiv","title":"<code>chemrxiv</code>","text":""},{"location":"api_reference/#paperscraper.get_dumps.utils.chemrxiv.get_author","title":"<code>get_author(author_list: List[Dict]) -&gt; str</code>","text":"<p>Parse ChemRxiv dump entry to extract author list</p> <p>Parameters:</p> Name Type Description Default <code>author_list</code> <code>list</code> <p>List of dicts, one per author.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>;-concatenated author list.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_author(author_list: List[Dict]) -&gt; str:\n    \"\"\"Parse ChemRxiv dump entry to extract author list\n\n    Args:\n        author_list (list): List of dicts, one per author.\n\n    Returns:\n        str: ;-concatenated author list.\n    \"\"\"\n\n    return \"; \".join([\" \".join([a[\"firstName\"], a[\"lastName\"]]) for a in author_list])\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.utils.chemrxiv.get_categories","title":"<code>get_categories(category_list: List[Dict]) -&gt; str</code>","text":"<p>Parse ChemRxiv dump entry to extract the categories of the paper</p> <p>Parameters:</p> Name Type Description Default <code>category_list</code> <code>list</code> <p>List of dicts, one per category.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>;-concatenated category list.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_categories(category_list: List[Dict]) -&gt; str:\n    \"\"\"Parse ChemRxiv dump entry to extract the categories of the paper\n\n    Args:\n        category_list (list): List of dicts, one per category.\n\n    Returns:\n        str: ;-concatenated category list.\n    \"\"\"\n\n    return \"; \".join([a[\"name\"] for a in category_list])\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.utils.chemrxiv.get_date","title":"<code>get_date(datestring: str) -&gt; str</code>","text":"<p>Get the date of a chemrxiv dump enry.</p> <p>Parameters:</p> Name Type Description Default <code>datestring</code> <code>str</code> <p>String in the format: 2021-10-15T05:12:32.356Z</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Date in the format: YYYY-MM-DD.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_date(datestring: str) -&gt; str:\n    \"\"\"Get the date of a chemrxiv dump enry.\n\n    Args:\n        datestring: String in the format: 2021-10-15T05:12:32.356Z\n\n    Returns:\n        str: Date in the format: YYYY-MM-DD.\n    \"\"\"\n    return datestring.split(\"T\")[0]\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.utils.chemrxiv.get_metrics","title":"<code>get_metrics(metrics_list: List[Dict]) -&gt; Dict</code>","text":"<p>Parse ChemRxiv dump entry to extract the access metrics of the paper.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_list</code> <code>List[Dict]</code> <p>A list of single-keyed, dictionaries each containing key and value for exactly one metric.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A flattened dictionary with all metrics and a timestamp</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_metrics(metrics_list: List[Dict]) -&gt; Dict:\n    \"\"\"\n    Parse ChemRxiv dump entry to extract the access metrics of the paper.\n\n    Args:\n        metrics_list (List[Dict]): A list of single-keyed, dictionaries each\n            containing key and value for exactly one metric.\n\n    Returns:\n        Dict: A flattened dictionary with all metrics and a timestamp\n    \"\"\"\n    metric_dict = {m[\"description\"]: m[\"value\"] for m in metrics_list}\n\n    # This assumes that the .jsonl is constructed at roughly the same date\n    # where this entry was obtained from the API\n    metric_dict.update({\"timestamp\": today})\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.utils.chemrxiv.parse_dump","title":"<code>parse_dump(source_path: str, target_path: str) -&gt; None</code>","text":"<p>Parses the dump as generated by the chemrXiv API and this repo: https://github.com/cthoyt/chemrxiv-summarize into a format that is equal to that of biorXiv and medRxiv.</p> <p>NOTE: This is a lazy parser trying to store all data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>Path to the source dump</p> required Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def parse_dump(source_path: str, target_path: str) -&gt; None:\n    \"\"\"\n    Parses the dump as generated by the chemrXiv API and this repo:\n    https://github.com/cthoyt/chemrxiv-summarize\n    into a format that is equal to that of biorXiv and medRxiv.\n\n    NOTE: This is a lazy parser trying to store all data in memory.\n\n    Args:\n        source_path: Path to the source dump\n    \"\"\"\n\n    dump = []\n    # Read source dump\n    for file_name in tqdm(os.listdir(source_path)):\n        if not file_name.endswith(\".json\"):\n            continue\n        filepath = os.path.join(source_path, file_name)\n        with open(filepath, \"r\") as f:\n            source_paper = json.load(f)\n\n        target_paper = {\n            \"title\": source_paper[\"title\"],\n            \"doi\": source_paper[\"doi\"],\n            \"published_doi\": (\n                source_paper[\"vor\"][\"vorDoi\"] if source_paper[\"vor\"] else \"N.A.\"\n            ),\n            \"published_url\": (\n                source_paper[\"vor\"][\"url\"] if source_paper[\"vor\"] else \"N.A.\"\n            ),\n            \"authors\": get_author(source_paper[\"authors\"]),\n            \"abstract\": source_paper[\"abstract\"],\n            \"date\": get_date(source_paper[\"statusDate\"]),\n            \"journal\": \"chemRxiv\",\n            \"categories\": get_categories(source_paper[\"categories\"]),\n            \"metrics\": get_metrics(source_paper[\"metrics\"]),\n            \"license\": source_paper[\"license\"][\"name\"],\n        }\n        dump.append(target_paper)\n        os.remove(filepath)\n    # Write dump\n    with open(target_path, \"w\") as f:\n        for idx, target_paper in enumerate(dump):\n            if idx &gt; 0:\n                f.write(os.linesep)\n            f.write(json.dumps(target_paper))\n    logger.info(\"Done, shutting down\")\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.utils.chemrxiv.chemrxiv_api","title":"<code>chemrxiv_api</code>","text":"<code>ChemrxivAPI</code> \u00b6 <p>Handle OpenEngage API requests, using access. Adapted from https://github.com/fxcoudert/tools/blob/master/chemRxiv/chemRxiv.py.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>class ChemrxivAPI:\n    \"\"\"Handle OpenEngage API requests, using access.\n    Adapted from https://github.com/fxcoudert/tools/blob/master/chemRxiv/chemRxiv.py.\n    \"\"\"\n\n    base = \"https://chemrxiv.org/engage/chemrxiv/public-api/v1/\"\n\n    def __init__(\n        self,\n        start_date: Optional[str] = None,\n        end_date: Optional[str] = None,\n        page_size: Optional[int] = None,\n        max_retries: int = 10,\n    ):\n        \"\"\"\n        Initialize API class.\n\n        Args:\n            start_date (Optional[str], optional): begin date expressed as YYYY-MM-DD.\n                Defaults to None.\n            end_date (Optional[str], optional): end date expressed as YYYY-MM-DD.\n                Defaults to None.\n            page_size (int, optional): The batch size used to fetch the records from chemrxiv.\n            max_retries (int): Number of retries in case of error\n        \"\"\"\n\n        self.page_size = page_size or 50\n        self.max_retries = max_retries\n\n        # Begin Date and End Date of the search\n        launch_date = launch_dates[\"chemrxiv\"]\n        launch_datetime = datetime.fromisoformat(launch_date)\n\n        if start_date:\n            start_datetime = datetime.fromisoformat(start_date)\n            if start_datetime &lt; launch_datetime:\n                self.start_date = launch_date\n                logger.warning(\n                    f\"Begin date {start_date} is before chemrxiv launch date. Will use {launch_date} instead.\"\n                )\n            else:\n                self.start_date = start_date\n        else:\n            self.start_date = launch_date\n        if end_date:\n            end_datetime = datetime.fromisoformat(end_date)\n            if end_datetime &gt; now_datetime:\n                logger.warning(\n                    f\"End date {end_date} is in the future. Will use {now_datetime} instead.\"\n                )\n                self.end_date = now_datetime.strftime(\"%Y-%m-%d\")\n            else:\n                self.end_date = end_date\n        else:\n            self.end_date = now_datetime.strftime(\"%Y-%m-%d\")\n\n    def request(self, url, method, params=None, parse_json: bool = False):\n        \"\"\"Send an API request to open Engage.\"\"\"\n\n        headers = {\"Accept-Encoding\": \"identity\", \"Accept\": \"application/json\"}\n        retryable = (\n            ChunkedEncodingError,\n            ContentDecodingError,\n            DecodeError,\n            ReadTimeout,\n            ConnectionError,\n        )\n        transient_status = {429, 500, 502, 503, 504}\n        backoff = 0.1\n\n        for attempt in range(self.max_retries):\n            try:\n                if method.casefold() == \"get\":\n                    response = requests.get(\n                        url, params=params, headers=headers, timeout=(5, 30)\n                    )\n                elif method.casefold() == \"post\":\n                    response = requests.post(\n                        url, json=params, headers=headers, timeout=(5, 30)\n                    )\n                else:\n                    raise ConnectionError(f\"Unknown method for query: {method}\")\n                if response.status_code in transient_status:\n                    logger.warning(\n                        f\"{response.status_code} for {url} (attempt {attempt + 1}/{self.max_retries}); retrying in {backoff:.1f}s\"\n                    )\n                    if attempt + 1 == self.max_retries:\n                        response.raise_for_status()\n                    sleep(backoff)\n                    backoff = min(60.0, backoff * 2)\n                    continue\n                elif 400 &lt;= response.status_code &lt; 500:\n                    response.raise_for_status()\n                if not parse_json:\n                    return response\n\n                try:\n                    return response.json()\n                except JSONDecodeError:\n                    logger.warning(\n                        f\"JSONDecodeError for {response.url} \"\n                        f\"(attempt {attempt + 1}/{self.max_retries}); retrying in {backoff:.1f}s\"\n                    )\n                    if attempt + 1 == self.max_retries:\n                        raise\n                    sleep(backoff)\n                    backoff = min(60.0, backoff * 2)\n                    continue\n\n            except retryable as e:\n                logger.warning(\n                    f\"{e.__class__.__name__} for {url} (attempt {attempt + 1}/{self.max_retries}); \"\n                    f\"retrying in {backoff:.1f}s\"\n                )\n                if attempt + 1 == self.max_retries:\n                    raise\n                sleep(backoff)\n                backoff = min(60.0, backoff * 2)\n\n    def query(self, query, method=\"get\", params=None):\n        \"\"\"Perform a direct query.\"\"\"\n\n        return self.request(\n            urljoin(self.base, query), method, params=params, parse_json=True\n        )\n\n    def query_generator(\n        self, query, method: str = \"get\", params: Optional[Dict] = None\n    ):\n        \"\"\"Query for a list of items, with paging. Returns a generator.\"\"\"\n\n        start_datetime = datetime.fromisoformat(self.start_date)\n        end_datetime = datetime.fromisoformat(self.end_date)\n\n        def year_windows():\n            year = start_datetime.year\n            while year &lt;= end_datetime.year:\n                year_start = datetime(year, 1, 1)\n                year_end = datetime(year, 12, 31)\n                win_start = max(start_datetime, year_start)\n                win_end = min(end_datetime, year_end)\n                yield win_start.strftime(\"%Y-%m-%d\"), win_end.strftime(\"%Y-%m-%d\")\n                year += 1\n\n        params = (params or {}).copy()\n\n        for year_from, year_to in year_windows():\n            logger.info(f\"Starting to scrape data from {year_from} to {year_to}\")\n            page = 0\n            while True:\n                params.update(\n                    {\n                        \"limit\": self.page_size,\n                        \"skip\": page * self.page_size,\n                        \"searchDateFrom\": year_from,\n                        \"searchDateTo\": year_to,\n                    }\n                )\n                try:\n                    data = self.request(\n                        urljoin(self.base, query),\n                        method,\n                        params=params,\n                        parse_json=True,\n                    )\n                except requests.HTTPError as e:\n                    status = getattr(e.response, \"status_code\", None)\n                    logger.warning(\n                        f\"Stopping year window {year_from}..{year_to} at skip={page * self.page_size} \"\n                        f\"due to HTTPError {status}\"\n                    )\n                    break\n                items = data.get(\"itemHits\", [])\n                if not items:\n                    break\n                for item in items:\n                    yield item\n                page += 1\n\n    def all_preprints(self):\n        \"\"\"Return a generator to all the chemRxiv articles.\"\"\"\n        return self.query_generator(\"items\")\n\n    def preprint(self, article_id):\n        \"\"\"Information on a given preprint.\n        .. seealso:: https://docs.figshare.com/#public_article\n        \"\"\"\n        return self.query(os.path.join(\"items\", article_id))\n\n    def number_of_preprints(self):\n        return self.query(\"items\")[\"totalCount\"]\n</code></pre> <code></code> <code>__init__(start_date: Optional[str] = None, end_date: Optional[str] = None, page_size: Optional[int] = None, max_retries: int = 10)</code> \u00b6 <p>Initialize API class.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Optional[str]</code> <p>begin date expressed as YYYY-MM-DD. Defaults to None.</p> <code>None</code> <code>end_date</code> <code>Optional[str]</code> <p>end date expressed as YYYY-MM-DD. Defaults to None.</p> <code>None</code> <code>page_size</code> <code>int</code> <p>The batch size used to fetch the records from chemrxiv.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Number of retries in case of error</p> <code>10</code> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def __init__(\n    self,\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    page_size: Optional[int] = None,\n    max_retries: int = 10,\n):\n    \"\"\"\n    Initialize API class.\n\n    Args:\n        start_date (Optional[str], optional): begin date expressed as YYYY-MM-DD.\n            Defaults to None.\n        end_date (Optional[str], optional): end date expressed as YYYY-MM-DD.\n            Defaults to None.\n        page_size (int, optional): The batch size used to fetch the records from chemrxiv.\n        max_retries (int): Number of retries in case of error\n    \"\"\"\n\n    self.page_size = page_size or 50\n    self.max_retries = max_retries\n\n    # Begin Date and End Date of the search\n    launch_date = launch_dates[\"chemrxiv\"]\n    launch_datetime = datetime.fromisoformat(launch_date)\n\n    if start_date:\n        start_datetime = datetime.fromisoformat(start_date)\n        if start_datetime &lt; launch_datetime:\n            self.start_date = launch_date\n            logger.warning(\n                f\"Begin date {start_date} is before chemrxiv launch date. Will use {launch_date} instead.\"\n            )\n        else:\n            self.start_date = start_date\n    else:\n        self.start_date = launch_date\n    if end_date:\n        end_datetime = datetime.fromisoformat(end_date)\n        if end_datetime &gt; now_datetime:\n            logger.warning(\n                f\"End date {end_date} is in the future. Will use {now_datetime} instead.\"\n            )\n            self.end_date = now_datetime.strftime(\"%Y-%m-%d\")\n        else:\n            self.end_date = end_date\n    else:\n        self.end_date = now_datetime.strftime(\"%Y-%m-%d\")\n</code></pre> <code></code> <code>request(url, method, params=None, parse_json: bool = False)</code> \u00b6 <p>Send an API request to open Engage.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def request(self, url, method, params=None, parse_json: bool = False):\n    \"\"\"Send an API request to open Engage.\"\"\"\n\n    headers = {\"Accept-Encoding\": \"identity\", \"Accept\": \"application/json\"}\n    retryable = (\n        ChunkedEncodingError,\n        ContentDecodingError,\n        DecodeError,\n        ReadTimeout,\n        ConnectionError,\n    )\n    transient_status = {429, 500, 502, 503, 504}\n    backoff = 0.1\n\n    for attempt in range(self.max_retries):\n        try:\n            if method.casefold() == \"get\":\n                response = requests.get(\n                    url, params=params, headers=headers, timeout=(5, 30)\n                )\n            elif method.casefold() == \"post\":\n                response = requests.post(\n                    url, json=params, headers=headers, timeout=(5, 30)\n                )\n            else:\n                raise ConnectionError(f\"Unknown method for query: {method}\")\n            if response.status_code in transient_status:\n                logger.warning(\n                    f\"{response.status_code} for {url} (attempt {attempt + 1}/{self.max_retries}); retrying in {backoff:.1f}s\"\n                )\n                if attempt + 1 == self.max_retries:\n                    response.raise_for_status()\n                sleep(backoff)\n                backoff = min(60.0, backoff * 2)\n                continue\n            elif 400 &lt;= response.status_code &lt; 500:\n                response.raise_for_status()\n            if not parse_json:\n                return response\n\n            try:\n                return response.json()\n            except JSONDecodeError:\n                logger.warning(\n                    f\"JSONDecodeError for {response.url} \"\n                    f\"(attempt {attempt + 1}/{self.max_retries}); retrying in {backoff:.1f}s\"\n                )\n                if attempt + 1 == self.max_retries:\n                    raise\n                sleep(backoff)\n                backoff = min(60.0, backoff * 2)\n                continue\n\n        except retryable as e:\n            logger.warning(\n                f\"{e.__class__.__name__} for {url} (attempt {attempt + 1}/{self.max_retries}); \"\n                f\"retrying in {backoff:.1f}s\"\n            )\n            if attempt + 1 == self.max_retries:\n                raise\n            sleep(backoff)\n            backoff = min(60.0, backoff * 2)\n</code></pre> <code></code> <code>query(query, method='get', params=None)</code> \u00b6 <p>Perform a direct query.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def query(self, query, method=\"get\", params=None):\n    \"\"\"Perform a direct query.\"\"\"\n\n    return self.request(\n        urljoin(self.base, query), method, params=params, parse_json=True\n    )\n</code></pre> <code></code> <code>query_generator(query, method: str = 'get', params: Optional[Dict] = None)</code> \u00b6 <p>Query for a list of items, with paging. Returns a generator.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def query_generator(\n    self, query, method: str = \"get\", params: Optional[Dict] = None\n):\n    \"\"\"Query for a list of items, with paging. Returns a generator.\"\"\"\n\n    start_datetime = datetime.fromisoformat(self.start_date)\n    end_datetime = datetime.fromisoformat(self.end_date)\n\n    def year_windows():\n        year = start_datetime.year\n        while year &lt;= end_datetime.year:\n            year_start = datetime(year, 1, 1)\n            year_end = datetime(year, 12, 31)\n            win_start = max(start_datetime, year_start)\n            win_end = min(end_datetime, year_end)\n            yield win_start.strftime(\"%Y-%m-%d\"), win_end.strftime(\"%Y-%m-%d\")\n            year += 1\n\n    params = (params or {}).copy()\n\n    for year_from, year_to in year_windows():\n        logger.info(f\"Starting to scrape data from {year_from} to {year_to}\")\n        page = 0\n        while True:\n            params.update(\n                {\n                    \"limit\": self.page_size,\n                    \"skip\": page * self.page_size,\n                    \"searchDateFrom\": year_from,\n                    \"searchDateTo\": year_to,\n                }\n            )\n            try:\n                data = self.request(\n                    urljoin(self.base, query),\n                    method,\n                    params=params,\n                    parse_json=True,\n                )\n            except requests.HTTPError as e:\n                status = getattr(e.response, \"status_code\", None)\n                logger.warning(\n                    f\"Stopping year window {year_from}..{year_to} at skip={page * self.page_size} \"\n                    f\"due to HTTPError {status}\"\n                )\n                break\n            items = data.get(\"itemHits\", [])\n            if not items:\n                break\n            for item in items:\n                yield item\n            page += 1\n</code></pre> <code></code> <code>all_preprints()</code> \u00b6 <p>Return a generator to all the chemRxiv articles.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def all_preprints(self):\n    \"\"\"Return a generator to all the chemRxiv articles.\"\"\"\n    return self.query_generator(\"items\")\n</code></pre> <code></code> <code>preprint(article_id)</code> \u00b6 <p>Information on a given preprint. .. seealso:: https://docs.figshare.com/#public_article</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def preprint(self, article_id):\n    \"\"\"Information on a given preprint.\n    .. seealso:: https://docs.figshare.com/#public_article\n    \"\"\"\n    return self.query(os.path.join(\"items\", article_id))\n</code></pre>"},{"location":"api_reference/#paperscraper.get_dumps.utils.chemrxiv.utils","title":"<code>utils</code>","text":"<p>Misc utils to download chemRxiv dump</p> <code>get_author(author_list: List[Dict]) -&gt; str</code> \u00b6 <p>Parse ChemRxiv dump entry to extract author list</p> <p>Parameters:</p> Name Type Description Default <code>author_list</code> <code>list</code> <p>List of dicts, one per author.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>;-concatenated author list.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_author(author_list: List[Dict]) -&gt; str:\n    \"\"\"Parse ChemRxiv dump entry to extract author list\n\n    Args:\n        author_list (list): List of dicts, one per author.\n\n    Returns:\n        str: ;-concatenated author list.\n    \"\"\"\n\n    return \"; \".join([\" \".join([a[\"firstName\"], a[\"lastName\"]]) for a in author_list])\n</code></pre> <code></code> <code>get_categories(category_list: List[Dict]) -&gt; str</code> \u00b6 <p>Parse ChemRxiv dump entry to extract the categories of the paper</p> <p>Parameters:</p> Name Type Description Default <code>category_list</code> <code>list</code> <p>List of dicts, one per category.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>;-concatenated category list.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_categories(category_list: List[Dict]) -&gt; str:\n    \"\"\"Parse ChemRxiv dump entry to extract the categories of the paper\n\n    Args:\n        category_list (list): List of dicts, one per category.\n\n    Returns:\n        str: ;-concatenated category list.\n    \"\"\"\n\n    return \"; \".join([a[\"name\"] for a in category_list])\n</code></pre> <code></code> <code>get_date(datestring: str) -&gt; str</code> \u00b6 <p>Get the date of a chemrxiv dump enry.</p> <p>Parameters:</p> Name Type Description Default <code>datestring</code> <code>str</code> <p>String in the format: 2021-10-15T05:12:32.356Z</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Date in the format: YYYY-MM-DD.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_date(datestring: str) -&gt; str:\n    \"\"\"Get the date of a chemrxiv dump enry.\n\n    Args:\n        datestring: String in the format: 2021-10-15T05:12:32.356Z\n\n    Returns:\n        str: Date in the format: YYYY-MM-DD.\n    \"\"\"\n    return datestring.split(\"T\")[0]\n</code></pre> <code></code> <code>get_metrics(metrics_list: List[Dict]) -&gt; Dict</code> \u00b6 <p>Parse ChemRxiv dump entry to extract the access metrics of the paper.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_list</code> <code>List[Dict]</code> <p>A list of single-keyed, dictionaries each containing key and value for exactly one metric.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A flattened dictionary with all metrics and a timestamp</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_metrics(metrics_list: List[Dict]) -&gt; Dict:\n    \"\"\"\n    Parse ChemRxiv dump entry to extract the access metrics of the paper.\n\n    Args:\n        metrics_list (List[Dict]): A list of single-keyed, dictionaries each\n            containing key and value for exactly one metric.\n\n    Returns:\n        Dict: A flattened dictionary with all metrics and a timestamp\n    \"\"\"\n    metric_dict = {m[\"description\"]: m[\"value\"] for m in metrics_list}\n\n    # This assumes that the .jsonl is constructed at roughly the same date\n    # where this entry was obtained from the API\n    metric_dict.update({\"timestamp\": today})\n</code></pre> <code></code> <code>parse_dump(source_path: str, target_path: str) -&gt; None</code> \u00b6 <p>Parses the dump as generated by the chemrXiv API and this repo: https://github.com/cthoyt/chemrxiv-summarize into a format that is equal to that of biorXiv and medRxiv.</p> <p>NOTE: This is a lazy parser trying to store all data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>Path to the source dump</p> required Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def parse_dump(source_path: str, target_path: str) -&gt; None:\n    \"\"\"\n    Parses the dump as generated by the chemrXiv API and this repo:\n    https://github.com/cthoyt/chemrxiv-summarize\n    into a format that is equal to that of biorXiv and medRxiv.\n\n    NOTE: This is a lazy parser trying to store all data in memory.\n\n    Args:\n        source_path: Path to the source dump\n    \"\"\"\n\n    dump = []\n    # Read source dump\n    for file_name in tqdm(os.listdir(source_path)):\n        if not file_name.endswith(\".json\"):\n            continue\n        filepath = os.path.join(source_path, file_name)\n        with open(filepath, \"r\") as f:\n            source_paper = json.load(f)\n\n        target_paper = {\n            \"title\": source_paper[\"title\"],\n            \"doi\": source_paper[\"doi\"],\n            \"published_doi\": (\n                source_paper[\"vor\"][\"vorDoi\"] if source_paper[\"vor\"] else \"N.A.\"\n            ),\n            \"published_url\": (\n                source_paper[\"vor\"][\"url\"] if source_paper[\"vor\"] else \"N.A.\"\n            ),\n            \"authors\": get_author(source_paper[\"authors\"]),\n            \"abstract\": source_paper[\"abstract\"],\n            \"date\": get_date(source_paper[\"statusDate\"]),\n            \"journal\": \"chemRxiv\",\n            \"categories\": get_categories(source_paper[\"categories\"]),\n            \"metrics\": get_metrics(source_paper[\"metrics\"]),\n            \"license\": source_paper[\"license\"][\"name\"],\n        }\n        dump.append(target_paper)\n        os.remove(filepath)\n    # Write dump\n    with open(target_path, \"w\") as f:\n        for idx, target_paper in enumerate(dump):\n            if idx &gt; 0:\n                f.write(os.linesep)\n            f.write(json.dumps(target_paper))\n    logger.info(\"Done, shutting down\")\n</code></pre>"},{"location":"api_reference/#paperscraper.impact","title":"<code>impact</code>","text":""},{"location":"api_reference/#paperscraper.impact.Impactor","title":"<code>Impactor</code>","text":"Source code in <code>paperscraper/impact.py</code> <pre><code>class Impactor:\n    def __init__(self):\n        \"\"\"\n        Initialize the Impactor class with an instance of the Factor class.\n        This allows access to the database of journal impact factors.\n        \"\"\"\n        self.fa = Factor()\n        self.all_journals = self.fa.search(\"%\")\n        self.metadata = pd.DataFrame(self.all_journals, dtype=str)\n        logger.info(f\"Loaded metadata for {len(self.metadata)} journals\")\n\n    def search(\n        self,\n        query: str,\n        threshold: int = 100,\n        sort_by: Optional[str] = None,\n        min_impact: float = 0.0,\n        max_impact: float = float(\"inf\"),\n        return_all: bool = False,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Search for journals matching the given query with an optional fuzziness\n            level and sorting.\n\n        Args:\n            query: The journal name or abbreviation to search for.\n            threshold: The threshold for fuzzy matching. If set to 100, exact matching\n                is performed. If set below 100, fuzzy matching is used. Defaults to 100.\n            sort_by: Criterion for sorting results, one of 'impact', 'journal' and 'score'.\n            min_impact: Minimum impact factor for journals to be considered, defaults to 0.\n            max_impact: Maximum impact factor for journals to be considered, defaults to infinity.\n            return_all: If True, returns all columns of the DataFrame for each match.\n\n        Returns:\n            List[dict]: A list of dictionaries containing the journal information.\n\n        \"\"\"\n        # Validation of parameters\n        if not isinstance(query, str) or not isinstance(threshold, int):\n            raise TypeError(\n                f\"Query must be a str and threshold must be an int, not {type(query)} and {type(threshold)}\"\n            )\n        if threshold &lt; 0 or threshold &gt; 100:\n            raise ValueError(\n                f\"Fuzziness threshold must be between 0 and 100, not {threshold}\"\n            )\n\n        if str.isdigit(query) and threshold &gt;= 100:\n            # When querying with NLM ID, exact matching does not work since impact_factor\n            # strips off leading zeros, so we use fuzzy matching instead\n            threshold = 99\n\n        # Define a function to calculate fuzziness score\n        def calculate_fuzziness_score(row):\n            return max(fuzz.partial_ratio(query, str(value)) for value in row.values)\n\n        # Search with or without fuzzy matching\n        if threshold &gt;= 100:\n            matched_df = self.metadata[\n                self.metadata.apply(\n                    lambda x: query.lower() in x.astype(str).str.lower().values, axis=1\n                )\n            ].copy()\n            # Exact matches get a default score of 100\n            matched_df[\"score\"] = 100\n        else:\n            matched_df = self.metadata[\n                self.metadata.apply(\n                    lambda x: calculate_fuzziness_score(x) &gt;= threshold, axis=1\n                )\n            ].copy()\n            matched_df[\"score\"] = matched_df.apply(calculate_fuzziness_score, axis=1)\n\n        # Sorting based on the specified criterion\n        if sort_by == \"score\":\n            matched_df = matched_df.sort_values(by=\"score\", ascending=False)\n        elif sort_by == \"journal\":\n            matched_df = matched_df.sort_values(by=\"journal\")\n        elif sort_by == \"impact\":\n            matched_df = matched_df.sort_values(by=\"factor\", ascending=False)\n\n        matched_df[\"factor\"] = pd.to_numeric(matched_df[\"factor\"])\n        matched_df = matched_df[\n            (matched_df[\"factor\"] &gt;= min_impact) &amp; (matched_df[\"factor\"] &lt;= max_impact)\n        ]\n\n        # Prepare the final result\n        results = [\n            (\n                row.to_dict()\n                if return_all\n                else {\n                    \"journal\": row[\"journal\"],\n                    \"factor\": row[\"factor\"],\n                    \"score\": row[\"score\"],\n                }\n            )\n            for _, row in matched_df.iterrows()\n        ]\n\n        return results\n</code></pre>"},{"location":"api_reference/#paperscraper.impact.Impactor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Impactor class with an instance of the Factor class. This allows access to the database of journal impact factors.</p> Source code in <code>paperscraper/impact.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the Impactor class with an instance of the Factor class.\n    This allows access to the database of journal impact factors.\n    \"\"\"\n    self.fa = Factor()\n    self.all_journals = self.fa.search(\"%\")\n    self.metadata = pd.DataFrame(self.all_journals, dtype=str)\n    logger.info(f\"Loaded metadata for {len(self.metadata)} journals\")\n</code></pre>"},{"location":"api_reference/#paperscraper.impact.Impactor.search","title":"<code>search(query: str, threshold: int = 100, sort_by: Optional[str] = None, min_impact: float = 0.0, max_impact: float = float('inf'), return_all: bool = False) -&gt; List[Dict[str, Any]]</code>","text":"<p>Search for journals matching the given query with an optional fuzziness     level and sorting.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The journal name or abbreviation to search for.</p> required <code>threshold</code> <code>int</code> <p>The threshold for fuzzy matching. If set to 100, exact matching is performed. If set below 100, fuzzy matching is used. Defaults to 100.</p> <code>100</code> <code>sort_by</code> <code>Optional[str]</code> <p>Criterion for sorting results, one of 'impact', 'journal' and 'score'.</p> <code>None</code> <code>min_impact</code> <code>float</code> <p>Minimum impact factor for journals to be considered, defaults to 0.</p> <code>0.0</code> <code>max_impact</code> <code>float</code> <p>Maximum impact factor for journals to be considered, defaults to infinity.</p> <code>float('inf')</code> <code>return_all</code> <code>bool</code> <p>If True, returns all columns of the DataFrame for each match.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[dict]: A list of dictionaries containing the journal information.</p> Source code in <code>paperscraper/impact.py</code> <pre><code>def search(\n    self,\n    query: str,\n    threshold: int = 100,\n    sort_by: Optional[str] = None,\n    min_impact: float = 0.0,\n    max_impact: float = float(\"inf\"),\n    return_all: bool = False,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Search for journals matching the given query with an optional fuzziness\n        level and sorting.\n\n    Args:\n        query: The journal name or abbreviation to search for.\n        threshold: The threshold for fuzzy matching. If set to 100, exact matching\n            is performed. If set below 100, fuzzy matching is used. Defaults to 100.\n        sort_by: Criterion for sorting results, one of 'impact', 'journal' and 'score'.\n        min_impact: Minimum impact factor for journals to be considered, defaults to 0.\n        max_impact: Maximum impact factor for journals to be considered, defaults to infinity.\n        return_all: If True, returns all columns of the DataFrame for each match.\n\n    Returns:\n        List[dict]: A list of dictionaries containing the journal information.\n\n    \"\"\"\n    # Validation of parameters\n    if not isinstance(query, str) or not isinstance(threshold, int):\n        raise TypeError(\n            f\"Query must be a str and threshold must be an int, not {type(query)} and {type(threshold)}\"\n        )\n    if threshold &lt; 0 or threshold &gt; 100:\n        raise ValueError(\n            f\"Fuzziness threshold must be between 0 and 100, not {threshold}\"\n        )\n\n    if str.isdigit(query) and threshold &gt;= 100:\n        # When querying with NLM ID, exact matching does not work since impact_factor\n        # strips off leading zeros, so we use fuzzy matching instead\n        threshold = 99\n\n    # Define a function to calculate fuzziness score\n    def calculate_fuzziness_score(row):\n        return max(fuzz.partial_ratio(query, str(value)) for value in row.values)\n\n    # Search with or without fuzzy matching\n    if threshold &gt;= 100:\n        matched_df = self.metadata[\n            self.metadata.apply(\n                lambda x: query.lower() in x.astype(str).str.lower().values, axis=1\n            )\n        ].copy()\n        # Exact matches get a default score of 100\n        matched_df[\"score\"] = 100\n    else:\n        matched_df = self.metadata[\n            self.metadata.apply(\n                lambda x: calculate_fuzziness_score(x) &gt;= threshold, axis=1\n            )\n        ].copy()\n        matched_df[\"score\"] = matched_df.apply(calculate_fuzziness_score, axis=1)\n\n    # Sorting based on the specified criterion\n    if sort_by == \"score\":\n        matched_df = matched_df.sort_values(by=\"score\", ascending=False)\n    elif sort_by == \"journal\":\n        matched_df = matched_df.sort_values(by=\"journal\")\n    elif sort_by == \"impact\":\n        matched_df = matched_df.sort_values(by=\"factor\", ascending=False)\n\n    matched_df[\"factor\"] = pd.to_numeric(matched_df[\"factor\"])\n    matched_df = matched_df[\n        (matched_df[\"factor\"] &gt;= min_impact) &amp; (matched_df[\"factor\"] &lt;= max_impact)\n    ]\n\n    # Prepare the final result\n    results = [\n        (\n            row.to_dict()\n            if return_all\n            else {\n                \"journal\": row[\"journal\"],\n                \"factor\": row[\"factor\"],\n                \"score\": row[\"score\"],\n            }\n        )\n        for _, row in matched_df.iterrows()\n    ]\n\n    return results\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf","title":"<code>pdf</code>","text":""},{"location":"api_reference/#paperscraper.pdf.fallbacks","title":"<code>fallbacks</code>","text":"<p>Functionalities to scrape PDF files of publications.</p>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.fallback_wiley_api","title":"<code>fallback_wiley_api(paper_metadata: Dict[str, Any], output_path: Path, api_keys: Dict[str, str], max_attempts: int = 2) -&gt; bool</code>","text":"<p>Attempt to download the PDF via the Wiley TDM API (popular publisher which blocks standard scraping attempts; API access free for academic users).</p> <p>This function uses the WILEY_TDM_API_TOKEN environment variable to authenticate with the Wiley TDM API and attempts to download the PDF for the given paper. See https://onlinelibrary.wiley.com/library-info/resources/text-and-datamining for a description on how to get your WILEY_TDM_API_TOKEN.</p> <p>Parameters:</p> Name Type Description Default <code>paper_metadata</code> <code>dict</code> <p>Dictionary containing paper metadata. Must include the 'doi' key.</p> required <code>output_path</code> <code>Path</code> <p>A pathlib.Path object representing the path where the PDF will be saved.</p> required <code>api_keys</code> <code>dict</code> <p>Preloaded API keys.</p> required <code>max_attempts</code> <code>int</code> <p>The maximum number of attempts to retry API call.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the PDF file was successfully downloaded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_wiley_api(\n    paper_metadata: Dict[str, Any],\n    output_path: Path,\n    api_keys: Dict[str, str],\n    max_attempts: int = 2,\n) -&gt; bool:\n    \"\"\"\n    Attempt to download the PDF via the Wiley TDM API (popular publisher which blocks standard scraping attempts; API access free for academic users).\n\n    This function uses the WILEY_TDM_API_TOKEN environment variable to authenticate\n    with the Wiley TDM API and attempts to download the PDF for the given paper.\n    See https://onlinelibrary.wiley.com/library-info/resources/text-and-datamining for a description on how to get your WILEY_TDM_API_TOKEN.\n\n    Args:\n        paper_metadata (dict): Dictionary containing paper metadata. Must include the 'doi' key.\n        output_path (Path): A pathlib.Path object representing the path where the PDF will be saved.\n        api_keys (dict): Preloaded API keys.\n        max_attempts (int): The maximum number of attempts to retry API call.\n\n    Returns:\n        bool: True if the PDF file was successfully downloaded, False otherwise.\n    \"\"\"\n\n    WILEY_TDM_API_TOKEN = api_keys.get(\"WILEY_TDM_API_TOKEN\")\n    encoded_doi = paper_metadata[\"doi\"].replace(\"/\", \"%2F\")\n    api_url = f\"https://api.wiley.com/onlinelibrary/tdm/v1/articles/{encoded_doi}\"\n    headers = {\"Wiley-TDM-Client-Token\": WILEY_TDM_API_TOKEN}\n\n    attempt = 0\n    success = False\n\n    while attempt &lt; max_attempts:\n        try:\n            api_response = requests.get(\n                api_url, headers=headers, allow_redirects=True, timeout=60\n            )\n            api_response.raise_for_status()\n            if api_response.content[:4] != b\"%PDF\":\n                logger.warning(\n                    f\"API returned content that is not a valid PDF for {paper_metadata['doi']}.\"\n                )\n            else:\n                with open(output_path.with_suffix(\".pdf\"), \"wb+\") as f:\n                    f.write(api_response.content)\n                logger.info(\n                    f\"Successfully downloaded PDF via Wiley API for {paper_metadata['doi']}.\"\n                )\n                success = True\n                break\n        except Exception as e2:\n            if attempt &lt; max_attempts - 1:\n                logger.info(\"Waiting 20 seconds before retrying...\")\n                time.sleep(20)\n            logger.error(\n                f\"Could not download via Wiley API (attempt {attempt + 1}/{max_attempts}): {e2}\"\n            )\n\n        attempt += 1\n\n    # **Mandatory delay of 10 seconds to comply with Wiley API rate limits**\n    logger.info(\n        \"Waiting 10 seconds before next request to comply with Wiley API rate limits...\"\n    )\n    time.sleep(10)\n    return success\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.fallback_bioc_pmc","title":"<code>fallback_bioc_pmc(doi: str, output_path: Path) -&gt; bool</code>","text":"<p>Attempt to download the XML via the BioC-PMC fallback.</p> <p>This function first converts a given DOI to a PMCID using the NCBI ID Converter API. If a PMCID is found, it constructs the corresponding PMC XML URL and attempts to download the full-text XML.</p> <p>PubMed Central\u00ae (PMC) is a free full-text archive of biomedical and life sciences journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM).</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI of the paper to retrieve.</p> required <code>output_path</code> <code>Path</code> <p>A pathlib.Path object representing the path where the XML file will be saved.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the XML file was successfully downloaded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_bioc_pmc(doi: str, output_path: Path) -&gt; bool:\n    \"\"\"\n    Attempt to download the XML via the BioC-PMC fallback.\n\n    This function first converts a given DOI to a PMCID using the NCBI ID Converter API.\n    If a PMCID is found, it constructs the corresponding PMC XML URL and attempts to\n    download the full-text XML.\n\n    PubMed Central\u00ae (PMC) is a free full-text archive of biomedical and life sciences\n    journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM).\n\n    Args:\n        doi (str): The DOI of the paper to retrieve.\n        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.\n\n    Returns:\n        bool: True if the XML file was successfully downloaded, False otherwise.\n    \"\"\"\n    ncbi_tool = \"paperscraper\"\n    ncbi_email = \"your_email@example.com\"\n\n    converter_url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\"\n    params = {\n        \"tool\": ncbi_tool,\n        \"email\": ncbi_email,\n        \"ids\": doi,\n        \"idtype\": \"doi\",\n        \"format\": \"json\",\n    }\n    try:\n        conv_response = requests.get(converter_url, params=params, timeout=60)\n        conv_response.raise_for_status()\n        data = conv_response.json()\n        records = data.get(\"records\", [])\n        if not records or \"pmcid\" not in records[0]:\n            logger.warning(\n                f\"No PMCID available for DOI {doi}. Fallback via PMC therefore not possible.\"\n            )\n            return False\n        pmcid = records[0][\"pmcid\"]\n        logger.info(f\"Converted DOI {doi} to PMCID {pmcid}.\")\n    except Exception as conv_err:\n        logger.error(f\"Error during DOI to PMCID conversion: {conv_err}\")\n        return False\n\n    # Construct PMC XML URL\n    xml_url = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_xml/{pmcid}/unicode\"\n    logger.info(f\"Attempting to download XML from BioC-PMC URL: {xml_url}\")\n    try:\n        xml_response = requests.get(xml_url, timeout=60)\n        xml_response.raise_for_status()\n        xml_path = output_path.with_suffix(\".xml\")\n        # check for xml error:\n        if xml_response.content.startswith(\n            b\"[Error] : No result can be found. &lt;BR&gt;&lt;HR&gt;&lt;B&gt; - https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/\"\n        ):\n            logger.warning(f\"No XML found for DOI {doi} at BioC-PMC URL {xml_url}.\")\n            return False\n        with open(xml_path, \"wb+\") as f:\n            f.write(xml_response.content)\n        logger.info(f\"Successfully downloaded XML for DOI {doi} to {xml_path}.\")\n        return True\n    except Exception as xml_err:\n        logger.error(f\"Failed to download XML from BioC-PMC URL {xml_url}: {xml_err}\")\n        return False\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.fallback_elsevier_api","title":"<code>fallback_elsevier_api(paper_metadata: Dict[str, Any], output_path: Path, api_keys: Dict[str, str]) -&gt; bool</code>","text":"<p>Attempt to download the full text via the Elsevier TDM API. For more information, see: https://www.elsevier.com/about/policies-and-standards/text-and-data-mining (Requires an institutional subscription and an API key provided in the api_keys dictionary under the key \"ELSEVIER_TDM_API_KEY\".)</p> <p>Parameters:</p> Name Type Description Default <code>paper_metadata</code> <code>Dict[str, Any]</code> <p>Dictionary containing paper metadata. Must include the 'doi' key.</p> required <code>output_path</code> <code>Path</code> <p>A pathlib.Path object representing the path where the XML file will be saved.</p> required <code>api_keys</code> <code>Dict[str, str]</code> <p>A dictionary containing API keys. Must include the key \"ELSEVIER_TDM_API_KEY\".</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the XML file was successfully downloaded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_elsevier_api(\n    paper_metadata: Dict[str, Any], output_path: Path, api_keys: Dict[str, str]\n) -&gt; bool:\n    \"\"\"\n    Attempt to download the full text via the Elsevier TDM API.\n    For more information, see:\n    https://www.elsevier.com/about/policies-and-standards/text-and-data-mining\n    (Requires an institutional subscription and an API key provided in the api_keys dictionary under the key \"ELSEVIER_TDM_API_KEY\".)\n\n    Args:\n        paper_metadata (Dict[str, Any]): Dictionary containing paper metadata. Must include the 'doi' key.\n        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.\n        api_keys (Dict[str, str]): A dictionary containing API keys. Must include the key \"ELSEVIER_TDM_API_KEY\".\n\n    Returns:\n        bool: True if the XML file was successfully downloaded, False otherwise.\n    \"\"\"\n    elsevier_api_key = api_keys.get(\"ELSEVIER_TDM_API_KEY\")\n    doi = paper_metadata[\"doi\"]\n    api_url = f\"https://api.elsevier.com/content/article/doi/{doi}?apiKey={elsevier_api_key}&amp;httpAccept=text%2Fxml\"\n    logger.info(f\"Attempting download via Elsevier API (XML) for {doi}: {api_url}\")\n    headers = {\"Accept\": \"application/xml\"}\n    try:\n        response = requests.get(api_url, headers=headers, timeout=60)\n\n        # Check for 401 error and look for APIKEY_INVALID in the response\n        if response.status_code == 401:\n            error_text = response.text\n            if \"APIKEY_INVALID\" in error_text:\n                logger.error(\"Invalid API key. Couldn't download via Elsevier XML API\")\n            else:\n                logger.error(\"401 Unauthorized. Couldn't download via Elsevier XML API\")\n            return False\n\n        response.raise_for_status()\n\n        # Attempt to parse it with lxml to confirm it's valid XML\n        try:\n            etree.fromstring(response.content)\n        except etree.XMLSyntaxError as e:\n            logger.warning(f\"Elsevier API returned invalid XML for {doi}: {e}\")\n            return False\n\n        xml_path = output_path.with_suffix(\".xml\")\n        with open(xml_path, \"wb\") as f:\n            f.write(response.content)\n        logger.info(\n            f\"Successfully used Elsevier API to downloaded XML for {doi} to {xml_path}\"\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"Could not download via Elsevier XML API: {e}\")\n        return False\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.fallback_elife_xml","title":"<code>fallback_elife_xml(doi: str, output_path: Path) -&gt; bool</code>","text":"<p>Attempt to download the XML via the eLife XML repository on GitHub.</p> <p>eLife provides open access to their XML files on GitHub, which can be used as a fallback. When multiple versions exist (revised papers), it takes the latest version (e.g., v3 instead of v1).</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI of the eLife paper to download.</p> required <code>output_path</code> <code>Path</code> <p>A pathlib.Path object representing the path where the XML file will be saved.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the XML file was successfully downloaded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_elife_xml(doi: str, output_path: Path) -&gt; bool:\n    \"\"\"\n    Attempt to download the XML via the eLife XML repository on GitHub.\n\n    eLife provides open access to their XML files on GitHub, which can be used as a fallback.\n    When multiple versions exist (revised papers), it takes the latest version (e.g., v3 instead of v1).\n\n    Args:\n        doi (str): The DOI of the eLife paper to download.\n        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.\n\n    Returns:\n        bool: True if the XML file was successfully downloaded, False otherwise.\n    \"\"\"\n    parts = doi.split(\"eLife.\")\n    if len(parts) &lt; 2:\n        logger.error(f\"Unable to parse eLife DOI: {doi}\")\n        return False\n    article_num = parts[1].strip()\n\n    index = get_elife_xml_index()\n    if article_num not in index:\n        logger.warning(f\"No eLife XML found for DOI {doi}.\")\n        return False\n    candidate_files = index[article_num]\n    latest_version, latest_download_url = max(candidate_files, key=lambda x: x[0])\n    try:\n        r = requests.get(latest_download_url, timeout=60)\n        r.raise_for_status()\n        latest_xml = r.content\n    except Exception as e:\n        logger.error(f\"Error downloading file from {latest_download_url}: {e}\")\n        return False\n\n    xml_path = output_path.with_suffix(\".xml\")\n    with open(xml_path, \"wb\") as f:\n        f.write(latest_xml)\n    logger.info(\n        f\"Successfully downloaded XML via eLife API ({latest_version}) for DOI {doi} to {xml_path}.\"\n    )\n    return True\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.get_elife_xml_index","title":"<code>get_elife_xml_index() -&gt; dict</code>","text":"<p>Fetch the eLife XML index from GitHub and return it as a dictionary.</p> <p>This function retrieves and caches the list of available eLife articles in XML format from the eLife GitHub repository. It ensures that the latest version of each article is accessible for downloading. The index is cached in memory to avoid repeated network requests when processing multiple eLife papers.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are article numbers (as strings) and values are   lists of tuples (version, download_url). Each list is sorted by version number.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def get_elife_xml_index() -&gt; dict:\n    \"\"\"\n    Fetch the eLife XML index from GitHub and return it as a dictionary.\n\n    This function retrieves and caches the list of available eLife articles in XML format\n    from the eLife GitHub repository. It ensures that the latest version of each article\n    is accessible for downloading. The index is cached in memory to avoid repeated\n    network requests when processing multiple eLife papers.\n\n    Returns:\n        dict: A dictionary where keys are article numbers (as strings) and values are\n              lists of tuples (version, download_url). Each list is sorted by version number.\n    \"\"\"\n    global ELIFE_XML_INDEX\n    if ELIFE_XML_INDEX is None:\n        logger.info(\"Fetching eLife XML index from GitHub using git tree API\")\n        ELIFE_XML_INDEX = {}\n        # Use the git tree API to get the full repository tree.\n        base_tree_url = \"https://api.github.com/repos/elifesciences/elife-article-xml/git/trees/master?recursive=1\"\n        r = requests.get(base_tree_url, timeout=60)\n        r.raise_for_status()\n        tree_data = r.json()\n        items = tree_data.get(\"tree\", [])\n        # Look for files in the 'articles' directory matching the pattern.\n        pattern = r\"articles/elife-(\\d+)-v(\\d+)\\.xml\"\n        for item in items:\n            path = item.get(\"path\", \"\")\n            match = re.match(pattern, path)\n            if match:\n                article_num_padded = match.group(1)\n                version = int(match.group(2))\n                # Construct the raw download URL.\n                download_url = f\"https://raw.githubusercontent.com/elifesciences/elife-article-xml/master/{path}\"\n                ELIFE_XML_INDEX.setdefault(article_num_padded, []).append(\n                    (version, download_url)\n                )\n        # Sort each article's file list by version.\n        for key in ELIFE_XML_INDEX:\n            ELIFE_XML_INDEX[key].sort(key=lambda x: x[0])\n    return ELIFE_XML_INDEX\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.month_folder","title":"<code>month_folder(doi: str) -&gt; str</code>","text":"<p>Query bioRxiv API to get the posting date of a given DOI. Convert a date to the BioRxiv S3 folder name, rolling over if it's the month's last day. E.g., if date is the last day of April, treat as May_YYYY.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI for which to retrieve the date.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Month and year in format <code>October_2019</code></p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def month_folder(doi: str) -&gt; str:\n    \"\"\"\n    Query bioRxiv API to get the posting date of a given DOI.\n    Convert a date to the BioRxiv S3 folder name, rolling over if it's the month's last day.\n    E.g., if date is the last day of April, treat as May_YYYY.\n\n    Args:\n        doi: The DOI for which to retrieve the date.\n\n    Returns:\n        Month and year in format `October_2019`\n    \"\"\"\n    url = f\"https://api.biorxiv.org/details/biorxiv/{doi}/na/json\"\n    resp = requests.get(url, timeout=30)\n    resp.raise_for_status()\n    date_str = resp.json()[\"collection\"][0][\"date\"]\n    date = datetime.date.fromisoformat(date_str)\n\n    # NOTE: bioRxiv papers posted on the last day of the month are archived the next day\n    last_day = calendar.monthrange(date.year, date.month)[1]\n    if date.day == last_day:\n        date = date + datetime.timedelta(days=1)\n    return date.strftime(\"%B_%Y\")\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.list_meca_keys","title":"<code>list_meca_keys(s3_client: BaseClient, bucket: str, prefix: str) -&gt; list</code>","text":"<p>List all .meca object keys under a given prefix in a requester-pays bucket.</p> <p>Parameters:</p> Name Type Description Default <code>s3_client</code> <code>BaseClient</code> <p>S3 client to get the data from.</p> required <code>bucket</code> <code>str</code> <p>bucket to get data from.</p> required <code>prefix</code> <code>str</code> <p>prefix to get data from.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of keys, one per existing .meca in the bucket.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def list_meca_keys(s3_client: BaseClient, bucket: str, prefix: str) -&gt; list:\n    \"\"\"\n    List all .meca object keys under a given prefix in a requester-pays bucket.\n\n    Args:\n        s3_client: S3 client to get the data from.\n        bucket: bucket to get data from.\n        prefix: prefix to get data from.\n\n    Returns:\n        List of keys, one per existing .meca in the bucket.\n    \"\"\"\n    keys = []\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    for page in paginator.paginate(\n        Bucket=bucket, Prefix=prefix, RequestPayer=\"requester\"\n    ):\n        for obj in page.get(\"Contents\", []):\n            if obj[\"Key\"].endswith(\".meca\"):\n                keys.append(obj[\"Key\"])\n    return keys\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.find_meca_for_doi","title":"<code>find_meca_for_doi(s3_client: BaseClient, bucket: str, key: str, doi_token: str, stop_event: threading.Event, tail_bytes: int = 131072) -&gt; bool</code>","text":"<p>Efficiently inspect manifest.xml within a .meca zip by fetching only necessary bytes. Parse via ZipFile to read manifest.xml and match DOI token.</p> <p>Parameters:</p> Name Type Description Default <code>s3_client</code> <code>BaseClient</code> <p>S3 client to get the data from.</p> required <code>bucket</code> <code>str</code> <p>bucket to get data from.</p> required <code>key</code> <code>str</code> <p>prefix to get data from.</p> required <code>doi_token</code> <code>str</code> <p>the DOI that should be matched</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether or not the DOI could be matched</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def find_meca_for_doi(\n    s3_client: BaseClient,\n    bucket: str,\n    key: str,\n    doi_token: str,\n    stop_event: threading.Event,\n    tail_bytes: int = 131072,\n) -&gt; bool:\n    \"\"\"\n    Efficiently inspect manifest.xml within a .meca zip by fetching only necessary bytes.\n    Parse via ZipFile to read manifest.xml and match DOI token.\n\n    Args:\n        s3_client: S3 client to get the data from.\n        bucket: bucket to get data from.\n        key: prefix to get data from.\n        doi_token: the DOI that should be matched\n\n    Returns:\n        Whether or not the DOI could be matched\n    \"\"\"\n\n    if stop_event.is_set():\n        return False\n\n    try:\n        # Try tail-only first (central directory is at end)\n        tail = s3_client.get_object(\n            Bucket=bucket,\n            Key=key,\n            Range=f\"bytes=-{tail_bytes}\",\n            RequestPayer=\"requester\",\n        )[\"Body\"].read()\n    except Exception:\n        return False\n\n    if stop_event.is_set():\n        return False\n\n    data = tail\n    try:\n        with zipfile.ZipFile(io.BytesIO(data)) as z:\n            # avoid reading file contents; inspect namelist/central directory\n            for name in z.namelist():\n                if name.endswith(\"manifest.xml\"):\n                    manifest = z.read(name)  # small file in practice\n                    token = doi_token.split(\".\")[-1].encode(\"utf-8\")\n                    return token in manifest.lower()\n    except zipfile.BadZipFile:\n        # Fallback: fetch small head slice &amp; retry zip\n        try:\n            head = s3_client.get_object(\n                Bucket=bucket, Key=key, Range=\"bytes=0-65535\", RequestPayer=\"requester\"\n            )[\"Body\"].read()\n            with zipfile.ZipFile(io.BytesIO(head + tail)) as z:\n                manifest = z.read(\"manifest.xml\")\n                token = doi_token.split(\".\")[-1].encode(\"utf-8\")\n                return token in manifest.lower()\n        except Exception:\n            return False\n    return False\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.fallbacks.fallback_s3","title":"<code>fallback_s3(doi: str, output_path: Union[str, Path], api_keys: dict, workers: int = 32) -&gt; bool</code>","text":"<p>Download a BioRxiv PDF via the requester-pays S3 bucket using range requests.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI for which to retrieve the PDF (e.g. '10.1101/798496').</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path where the PDF will be saved (with .pdf suffix added).</p> required <code>api_keys</code> <code>dict</code> <p>Dict containing 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY'.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if download succeeded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_s3(\n    doi: str, output_path: Union[str, Path], api_keys: dict, workers: int = 32\n) -&gt; bool:\n    \"\"\"\n    Download a BioRxiv PDF via the requester-pays S3 bucket using range requests.\n\n    Args:\n        doi: The DOI for which to retrieve the PDF (e.g. '10.1101/798496').\n        output_path: Path where the PDF will be saved (with .pdf suffix added).\n        api_keys: Dict containing 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY'.\n\n    Returns:\n        True if download succeeded, False otherwise.\n    \"\"\"\n\n    s3 = boto3.client(\n        \"s3\",\n        aws_access_key_id=api_keys.get(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key=api_keys.get(\"AWS_SECRET_ACCESS_KEY\"),\n        region_name=\"us-east-1\",\n        config=Config(connect_timeout=5, read_timeout=10, retries={\"max_attempts\": 3}),\n    )\n    bucket = \"biorxiv-src-monthly\"\n\n    # Derive prefix from DOI date\n    prefix = f\"Current_Content/{month_folder(doi)}/\"\n\n    # List MECA archives in that month\n    meca_keys = list_meca_keys(s3, bucket, prefix)\n    if not meca_keys:\n        return False\n\n    token = doi.split(\"/\")[-1].lower()\n\n    # Prefer keys that already contain the token\n    candidate_keys = [k for k in meca_keys if token in k.lower()]\n    # If none contain the token (older DOIs, etc.), fall back to a small prefix scan\n    if not candidate_keys:\n        candidate_keys = meca_keys[: min(500, len(meca_keys))]\n    out_pdf = Path(output_path).with_suffix(\".pdf\")\n\n    # Try candidates concurrently but keep at most `workers` in flight.\n    stop = threading.Event()\n\n    def job(k):\n        ok = _try_download_pdf_from_meca(s3, bucket, k, out_pdf, stop)\n        if ok:\n            stop.set()\n        return ok\n\n    executor = ThreadPoolExecutor(max_workers=workers)\n    found = False\n    try:\n        it = iter(candidate_keys)\n        # prime the queue with at most `workers` tasks\n        futures = set()\n        for _ in range(min(workers, len(candidate_keys))):\n            k = next(it, None)\n            if k is not None:\n                futures.add(executor.submit(job, k))\n\n        while futures and not found:\n            done, futures = wait(futures, return_when=FIRST_COMPLETED)\n            # check completed ones\n            for fut in done:\n                try:\n                    if fut.result():\n                        found = True\n                        stop.set()\n                        # cancel not-yet-started tasks\n                        for f in list(futures):\n                            f.cancel()\n                        break\n                except Exception:\n                    pass\n            # top up queue if still searching\n            while not found and len(futures) &lt; workers:\n                k = next(it, None)\n                if k is None:\n                    break\n                futures.add(executor.submit(job, k))\n    finally:\n        # don't wait for running tasks; best-effort cancel\n        executor.shutdown(wait=False, cancel_futures=True)\n\n    if not found:\n        logger.error(f\"Could not find {doi} on biorxiv\")\n        return False\n    return True\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.pdf","title":"<code>pdf</code>","text":"<p>Functionalities to scrape PDF files of publications.</p>"},{"location":"api_reference/#paperscraper.pdf.pdf.save_pdf","title":"<code>save_pdf(paper_metadata: Dict[str, Any], filepath: Union[str, Path], save_metadata: bool = False, api_keys: Optional[Union[str, Dict[str, str]]] = None) -&gt; None</code>","text":"<p>Save a PDF file of a paper.</p> <p>Parameters:</p> Name Type Description Default <code>paper_metadata</code> <code>Dict[str, Any]</code> <p>A dictionary with the paper metadata. Must contain the <code>doi</code> key.</p> required <code>filepath</code> <code>Union[str, Path]</code> <p>Path to the PDF file to be saved (with or without suffix).</p> required <code>save_metadata</code> <code>bool</code> <p>A boolean indicating whether to save paper metadata as a separate json.</p> <code>False</code> <code>api_keys</code> <code>Optional[Union[str, Dict[str, str]]]</code> <p>Either a dictionary containing API keys (if already loaded) or a string (path to API keys file).       If None, will try to load from <code>.env</code> file and if unsuccessful, skip API-based fallbacks.</p> <code>None</code> Source code in <code>paperscraper/pdf/pdf.py</code> <pre><code>def save_pdf(\n    paper_metadata: Dict[str, Any],\n    filepath: Union[str, Path],\n    save_metadata: bool = False,\n    api_keys: Optional[Union[str, Dict[str, str]]] = None,\n) -&gt; None:\n    \"\"\"\n    Save a PDF file of a paper.\n\n    Args:\n        paper_metadata: A dictionary with the paper metadata. Must contain the `doi` key.\n        filepath: Path to the PDF file to be saved (with or without suffix).\n        save_metadata: A boolean indicating whether to save paper metadata as a separate json.\n        api_keys: Either a dictionary containing API keys (if already loaded) or a string (path to API keys file).\n                  If None, will try to load from `.env` file and if unsuccessful, skip API-based fallbacks.\n    \"\"\"\n    if not isinstance(paper_metadata, Dict):\n        raise TypeError(f\"paper_metadata must be a dict, not {type(paper_metadata)}.\")\n    if \"doi\" not in paper_metadata.keys():\n        raise KeyError(\"paper_metadata must contain the key 'doi'.\")\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}.\")\n\n    output_path = Path(filepath)\n\n    if not Path(output_path).parent.exists():\n        raise ValueError(f\"The folder: {output_path} seems to not exist.\")\n\n    # load API keys from file if not already loaded via in save_pdf_from_dump (dict)\n    if not isinstance(api_keys, dict):\n        api_keys = load_api_keys(api_keys)\n\n    doi = paper_metadata[\"doi\"]\n    url = f\"https://doi.org/{doi}\"\n    user_agent = {\"User-Agent\": \"paperscraper/1.0 (+https)\"}\n    if \"arxiv\" in doi:\n        soup = None\n        try:\n            match = re.search(\n                r\"arxiv\\.([0-9]{4}\\.[0-9]{4,5}(?:v\\d+)?)\", doi, re.IGNORECASE\n            )\n            arxiv_id = match.group(1)\n            pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n            r = requests.get(pdf_url, timeout=60, headers=user_agent)\n            r.raise_for_status()\n            if r.content[:4] == b\"%PDF\":\n                with open(output_path.with_suffix(\".pdf\"), \"wb+\") as f:\n                    f.write(r.content)\n                # If metadata requested, fetch the landing page now to extract it\n                if save_metadata:\n                    try:\n                        resp_landing = requests.get(url, timeout=60, headers=user_agent)\n                        resp_landing.raise_for_status()\n                        soup = BeautifulSoup(resp_landing.text, features=\"lxml\")\n                    except Exception as _:\n                        soup = None\n                else:\n                    return\n            else:\n                logger.warning(\n                    f\"Direct arXiv fetch returned non-PDF for {doi}. Falling back.\"\n                )\n        except Exception as e:\n            logger.warning(\n                f\"Direct arXiv PDF fetch failed for {doi}: {e}. Falling back.\"\n            )\n\n    success = False\n    try:\n        response = requests.get(url, timeout=60)\n        response.raise_for_status()\n        success = True\n    except Exception as e:\n        error = str(e)\n        logger.warning(f\"Could not download from: {url} - {e}. \")\n\n    if not success and \"biorxiv\" in error:\n        if (\n            api_keys.get(\"AWS_ACCESS_KEY_ID\") is None\n            or api_keys.get(\"AWS_SECRET_ACCESS_KEY\") is None\n        ):\n            logger.info(\n                \"BiorXiv PDFs can be downloaded from a S3 bucket with a requester-pay option. \"\n                \"Consider setting `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to use this option. \"\n                \"Pricing is a few cent per GB, thus each request costs &lt; 0.1 cents. \"\n                \"For details see: https://www.biorxiv.org/tdm\"\n            )\n        else:\n            success = FALLBACKS[\"s3\"](doi, output_path, api_keys)\n            if success:\n                return\n\n    if not success:\n        # always first try fallback to BioC-PMC (open access papers on PubMed Central)\n        success = FALLBACKS[\"bioc_pmc\"](doi, output_path)\n\n        # if BioC-PMC fails, try other fallbacks\n        if not success:\n            # check for specific publishers\n            if \"elife\" in error.lower():  # elife has an open XML repository on GitHub\n                FALLBACKS[\"elife\"](doi, output_path)\n            elif (\n                (\"wiley\" in error.lower())\n                and api_keys\n                and (\"WILEY_TDM_API_TOKEN\" in api_keys)\n            ):\n                FALLBACKS[\"wiley\"](paper_metadata, output_path, api_keys)\n        return\n\n    soup = BeautifulSoup(response.text, features=\"lxml\")\n    meta_pdf = soup.find(\"meta\", {\"name\": \"citation_pdf_url\"})\n    if meta_pdf and meta_pdf.get(\"content\"):\n        pdf_url = meta_pdf.get(\"content\")\n        try:\n            response = requests.get(pdf_url, timeout=60)\n            response.raise_for_status()\n\n            if response.content[:4] != b\"%PDF\":\n                logger.warning(\n                    f\"The file from {url} does not appear to be a valid PDF.\"\n                )\n                success = FALLBACKS[\"bioc_pmc\"](doi, output_path)\n                if not success:\n                    # Check for specific publishers\n                    if \"elife\" in doi.lower():\n                        logger.info(\"Attempting fallback to eLife XML repository\")\n                        FALLBACKS[\"elife\"](doi, output_path)\n                    elif api_keys and \"WILEY_TDM_API_TOKEN\" in api_keys:\n                        FALLBACKS[\"wiley\"](paper_metadata, output_path, api_keys)\n                    elif api_keys and \"ELSEVIER_TDM_API_KEY\" in api_keys:\n                        FALLBACKS[\"elsevier\"](paper_metadata, output_path, api_keys)\n            else:\n                with open(output_path.with_suffix(\".pdf\"), \"wb+\") as f:\n                    f.write(response.content)\n        except Exception as e:\n            logger.warning(f\"Could not download {pdf_url}: {e}\")\n    else:  # if no citation_pdf_url meta tag found, try other fallbacks\n        if \"elife\" in doi.lower():\n            logger.info(\n                \"DOI contains eLife, attempting fallback to eLife XML repository on GitHub.\"\n            )\n            if not FALLBACKS[\"elife\"](doi, output_path):\n                logger.warning(\n                    f\"eLife XML fallback failed for {paper_metadata['doi']}.\"\n                )\n        elif (\n            api_keys and \"ELSEVIER_TDM_API_KEY\" in api_keys\n        ):  # elsevier journals can be accessed via the Elsevier TDM API (requires API key)\n            FALLBACKS[\"elsevier\"](paper_metadata, output_path, api_keys)\n        else:\n            logger.warning(\n                f\"Retrieval failed. No citation_pdf_url meta tag found for {url} and no applicable fallback mechanism available.\"\n            )\n\n    if not save_metadata:\n        return\n\n    metadata = {}\n    # Extract title\n    title_tag = soup.find(\"meta\", {\"name\": \"citation_title\"})\n    metadata[\"title\"] = title_tag.get(\"content\") if title_tag else \"Title not found\"\n\n    # Extract authors\n    authors = []\n    for author_tag in soup.find_all(\"meta\", {\"name\": \"citation_author\"}):\n        if author_tag.get(\"content\"):\n            authors.append(author_tag[\"content\"])\n    metadata[\"authors\"] = authors if authors else [\"Author information not found\"]\n\n    # Extract abstract\n    domain = tldextract.extract(url).domain\n    abstract_keys = ABSTRACT_ATTRIBUTE.get(domain, DEFAULT_ATTRIBUTES)\n\n    for key in abstract_keys:\n        abstract_tag = soup.find(\"meta\", {\"name\": key})\n        if abstract_tag:\n            raw_abstract = BeautifulSoup(\n                abstract_tag.get(\"content\", \"None\"), \"html.parser\"\n            ).get_text(separator=\"\\n\")\n            if raw_abstract.strip().startswith(\"Abstract\"):\n                raw_abstract = raw_abstract.strip()[8:]\n            metadata[\"abstract\"] = raw_abstract.strip()\n            break\n\n    if \"abstract\" not in metadata.keys():\n        metadata[\"abstract\"] = \"Abstract not found\"\n        logger.warning(f\"Could not find abstract for {url}\")\n    elif metadata[\"abstract\"].endswith(\"...\"):\n        logger.warning(f\"Abstract truncated from {url}\")\n\n    # Save metadata to JSON\n    try:\n        with open(output_path.with_suffix(\".json\"), \"w\", encoding=\"utf-8\") as f:\n            json.dump(metadata, f, ensure_ascii=False, indent=4)\n    except Exception as e:\n        logger.error(f\"Failed to save metadata to {str(output_path)}: {e}\")\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.pdf.save_pdf_from_dump","title":"<code>save_pdf_from_dump(dump_path: str, pdf_path: str, key_to_save: str = 'doi', save_metadata: bool = False, api_keys: Optional[str] = None) -&gt; None</code>","text":"<p>Receives a path to a <code>.jsonl</code> dump with paper metadata and saves the PDF files of each paper.</p> <p>Parameters:</p> Name Type Description Default <code>dump_path</code> <code>str</code> <p>Path to a <code>.jsonl</code> file with paper metadata, one paper per line.</p> required <code>pdf_path</code> <code>str</code> <p>Path to a folder where the files will be stored.</p> required <code>key_to_save</code> <code>str</code> <p>Key in the paper metadata to use as filename. Has to be <code>doi</code> or <code>title</code>. Defaults to <code>doi</code>.</p> <code>'doi'</code> <code>save_metadata</code> <code>bool</code> <p>A boolean indicating whether to save paper metadata as a separate json.</p> <code>False</code> <code>api_keys</code> <code>Optional[str]</code> <p>Path to a file with API keys. If None, API-based fallbacks will be skipped.</p> <code>None</code> Source code in <code>paperscraper/pdf/pdf.py</code> <pre><code>def save_pdf_from_dump(\n    dump_path: str,\n    pdf_path: str,\n    key_to_save: str = \"doi\",\n    save_metadata: bool = False,\n    api_keys: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Receives a path to a `.jsonl` dump with paper metadata and saves the PDF files of\n    each paper.\n\n    Args:\n        dump_path: Path to a `.jsonl` file with paper metadata, one paper per line.\n        pdf_path: Path to a folder where the files will be stored.\n        key_to_save: Key in the paper metadata to use as filename.\n            Has to be `doi` or `title`. Defaults to `doi`.\n        save_metadata: A boolean indicating whether to save paper metadata as a separate json.\n        api_keys: Path to a file with API keys. If None, API-based fallbacks will be skipped.\n    \"\"\"\n\n    if not isinstance(dump_path, str):\n        raise TypeError(f\"dump_path must be a string, not {type(dump_path)}.\")\n    if not dump_path.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a dump_path with .jsonl extension.\")\n\n    if not isinstance(pdf_path, str):\n        raise TypeError(f\"pdf_path must be a string, not {type(pdf_path)}.\")\n\n    if not isinstance(key_to_save, str):\n        raise TypeError(f\"key_to_save must be a string, not {type(key_to_save)}.\")\n    if key_to_save not in [\"doi\", \"title\", \"date\"]:\n        raise ValueError(\"key_to_save must be one of 'doi' or 'title'.\")\n\n    papers = load_jsonl(dump_path)\n\n    if not isinstance(api_keys, dict):\n        api_keys = load_api_keys(api_keys)\n\n    pbar = tqdm(papers, total=len(papers), desc=\"Processing\")\n    for i, paper in enumerate(pbar):\n        pbar.set_description(f\"Processing paper {i + 1}/{len(papers)}\")\n\n        if \"doi\" not in paper.keys() or paper[\"doi\"] is None:\n            logger.warning(f\"Skipping {paper['title']} since no DOI available.\")\n            continue\n        filename = paper[key_to_save].replace(\"/\", \"_\")\n        pdf_file = Path(os.path.join(pdf_path, f\"{filename}.pdf\"))\n        xml_file = pdf_file.with_suffix(\".xml\")\n        if pdf_file.exists():\n            logger.info(f\"File {pdf_file} already exists. Skipping download.\")\n            continue\n        if xml_file.exists():\n            logger.info(f\"File {xml_file} already exists. Skipping download.\")\n            continue\n        output_path = str(pdf_file)\n        save_pdf(paper, output_path, save_metadata=save_metadata, api_keys=api_keys)\n</code></pre>"},{"location":"api_reference/#paperscraper.pdf.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/#paperscraper.pdf.utils.load_api_keys","title":"<code>load_api_keys(filepath: Optional[str] = None) -&gt; Dict[str, str]</code>","text":"<p>Reads API keys from a file and returns them as a dictionary. The file should have each API key on a separate line in the format:     KEY_NAME=API_KEY_VALUE</p> Example <p>WILEY_TDM_API_TOKEN=your_wiley_token_here ELSEVIER_TDM_API_KEY=your_elsevier_key_here</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Optional[str]</code> <p>Optional path to the file containing API keys.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary where keys are API key names and values are their respective API keys.</p> Source code in <code>paperscraper/pdf/utils.py</code> <pre><code>def load_api_keys(filepath: Optional[str] = None) -&gt; Dict[str, str]:\n    \"\"\"\n    Reads API keys from a file and returns them as a dictionary.\n    The file should have each API key on a separate line in the format:\n        KEY_NAME=API_KEY_VALUE\n\n    Example:\n        WILEY_TDM_API_TOKEN=your_wiley_token_here\n        ELSEVIER_TDM_API_KEY=your_elsevier_key_here\n\n    Args:\n        filepath: Optional path to the file containing API keys.\n\n    Returns:\n        Dict[str, str]: A dictionary where keys are API key names and values are their respective API keys.\n    \"\"\"\n    if filepath:\n        load_dotenv(dotenv_path=filepath)\n    else:\n        load_dotenv(find_dotenv())\n\n    return {\n        \"WILEY_TDM_API_TOKEN\": os.getenv(\"WILEY_TDM_API_TOKEN\"),\n        \"ELSEVIER_TDM_API_KEY\": os.getenv(\"ELSEVIER_TDM_API_KEY\"),\n        \"AWS_ACCESS_KEY_ID\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n        \"AWS_SECRET_ACCESS_KEY\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n    }\n</code></pre>"},{"location":"api_reference/#paperscraper.plotting","title":"<code>plotting</code>","text":""},{"location":"api_reference/#paperscraper.plotting.plot_comparison","title":"<code>plot_comparison(data_dict: dict, keys: List[str], x_ticks: List[str] = ['2015', '2016', '2017', '2018', '2019', '2020'], show_preprint: bool = False, title_text: str = '', keyword_text: Optional[List[str]] = None, figpath: str = 'comparison_plot.pdf') -&gt; None</code>","text":"<p>Plot temporal evolution of number of papers per keyword</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>A dictionary with keywords as keys. Each value should be a dictionary itself, with keys for the different APIs. For example data_dict = {     'covid_19.jsonl': {         'pubmed': [0, 0, 0, 12345],         'arxiv': [0, 0, 0, 1234],         ...     }     'coronavirus.jsonl':         'pubmed': [234, 345, 456, 12345],         'arxiv': [123, 234, 345, 1234],         ...     } }</p> required <code>keys</code> <code>List[str]</code> <p>List of keys which should be plotted. This has to be a subset of data_dict.keys().</p> required <code>x_ticks</code> <code>List[str]</code> <p>List of strings to be used for the x-ticks. Should have same length as data_dict[key][database]. Defaults to ['2015', '2016', '2017', '2018', '2019', '2020'], meaning that papers are aggregated per year.</p> <code>['2015', '2016', '2017', '2018', '2019', '2020']</code> <code>show_preprint</code> <code>bool</code> <p>Whether preprint servers are aggregated or not. Defaults to False.</p> <code>False</code> <code>title_text</code> <code>str</code> <p>Title for the produced figure. Defaults to ''.</p> <code>''</code> <code>keyword_text</code> <code>Optional[List[str]]</code> <p>Figure caption per keyword. Defaults to None, i.e. empty strings will be used.</p> <code>None</code> <code>figpath</code> <code>str</code> <p>Name under which figure is saved. Relative or absolute paths can be given. Defaults to 'comparison_plot.pdf'.</p> <code>'comparison_plot.pdf'</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a database is missing in data_dict.</p> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_comparison(\n    data_dict: dict,\n    keys: List[str],\n    x_ticks: List[str] = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"],\n    show_preprint: bool = False,\n    title_text: str = \"\",\n    keyword_text: Optional[List[str]] = None,\n    figpath: str = \"comparison_plot.pdf\",\n) -&gt; None:\n    \"\"\"Plot temporal evolution of number of papers per keyword\n\n    Args:\n        data_dict: A dictionary with keywords as keys. Each value should be a\n            dictionary itself, with keys for the different APIs. For example\n            data_dict = {\n                'covid_19.jsonl': {\n                    'pubmed': [0, 0, 0, 12345],\n                    'arxiv': [0, 0, 0, 1234],\n                    ...\n                }\n                'coronavirus.jsonl':\n                    'pubmed': [234, 345, 456, 12345],\n                    'arxiv': [123, 234, 345, 1234],\n                    ...\n                }\n            }\n        keys: List of keys which should be plotted. This has to be a subset of data_dict.keys().\n        x_ticks: List of strings to be used for the x-ticks. Should have same length as\n            data_dict[key][database]. Defaults to ['2015', '2016', '2017', '2018', '2019', '2020'],\n            meaning that papers are aggregated per year.\n        show_preprint: Whether preprint servers are aggregated or not.\n            Defaults to False.\n        title_text: Title for the produced figure. Defaults to ''.\n        keyword_text: Figure caption per keyword. Defaults to None, i.e. empty strings will be used.\n        figpath: Name under which figure is saved. Relative or absolute\n            paths can be given. Defaults to 'comparison_plot.pdf'.\n\n    Raises:\n        KeyError: If a database is missing in data_dict.\n    \"\"\"\n\n    sns.set_palette(sns.color_palette(\"colorblind\", 10))\n    plt.rcParams.update({\"hatch.color\": \"w\"})\n    plt.rcParams[\"figure.facecolor\"] = \"white\"\n    plt.figure(figsize=(8, 5))\n\n    arxiv, biorxiv, pubmed, medrxiv, chemrxiv, preprint = [], [], [], [], [], []\n\n    for key in keys:\n        try:\n            arxiv.append(data_dict[key][\"arxiv\"])\n            biorxiv.append(data_dict[key][\"biorxiv\"])\n            medrxiv.append(data_dict[key][\"medrxiv\"])\n            chemrxiv.append(data_dict[key][\"chemrxiv\"])\n            pubmed.append(data_dict[key][\"pubmed\"])\n        except KeyError:\n            raise KeyError(\n                f\"Did not find all DBs for {key}, only found {data_dict[key].keys()}\"\n            )\n        preprint.append(arxiv[-1] + biorxiv[-1] + medrxiv[-1] + chemrxiv[-1])\n\n    ind = np.arange(len(arxiv[0]))  # the x locations for the groups\n    width = [0.2] * len(ind)  # the width of the bars: can also be len(x) sequence\n    if len(keys) == 2:\n        pos = [-0.2, 0.2]\n    elif len(keys) == 3:\n        pos = [-0.3, 0.0, 0.3]\n\n    plts = []\n    legend_plts = []\n    patterns = (\"|||\", \"oo\", \"xx\", \"..\", \"**\")\n    if show_preprint:\n        bars = [pubmed, preprint]\n        legend_platform = [\"PubMed\", \"Preprint\"]\n    else:\n        bars = [pubmed, arxiv, biorxiv, chemrxiv, medrxiv]\n        legend_platform = [\"PubMed\", \"ArXiv\", \"BiorXiv\", \"ChemRxiv\", \"MedRxiv\"]\n    for idx in range(len(keys)):\n        bottom = 0\n\n        for bidx, b in enumerate(bars):\n            if idx == 0:\n                p = plt.bar(\n                    ind + pos[idx],\n                    b[idx],\n                    width,\n                    linewidth=1,\n                    edgecolor=\"k\",\n                    bottom=bottom,\n                )\n            else:\n                p = plt.bar(\n                    ind + pos[idx],\n                    b[idx],\n                    width,\n                    color=next(iter(plts[bidx])).get_facecolor(),\n                    linewidth=1,\n                    edgecolor=\"k\",\n                    bottom=bottom,\n                )\n\n            bottom += b[idx]\n            plts.append(p)\n        legend_plts.append(\n            plt.bar(ind + pos[idx], np.zeros((len(ind),)), color=\"k\", bottom=bottom)\n        )\n\n    plt.ylabel(\"Counts\", size=15)\n    plt.xlabel(\"Years\", size=15)\n    plt.title(f\"Keywords: {title_text}\", size=14)\n    # Customize minor tick labels\n    plt.xticks(ind, x_ticks, size=10)\n\n    legend = plt.legend(\n        legend_platform,\n        prop={\"size\": 12},\n        loc=\"upper left\",\n        title=\"Platform:\",\n        title_fontsize=13,\n        ncol=1,\n    )\n\n    # Now set the hatches to not destroy legend\n\n    for idx, stackbar in enumerate(plts):\n        pidx = int(np.floor(idx / len(bars)))\n        for bar in stackbar:\n            bar.set_hatch(patterns[pidx])\n\n    for idx, stackbar in enumerate(legend_plts):\n        for bar in stackbar:\n            bar.set_hatch(patterns[idx])\n\n    if not keyword_text:\n        keyword_text = [\"\"] * len(keys)\n\n    plt.legend(\n        legend_plts,\n        keyword_text,\n        loc=\"upper center\",\n        prop={\"size\": 12},\n        title=\"Keywords (X):\",\n        title_fontsize=13,\n    )\n    plt.gca().add_artist(legend)\n\n    get_step_size = lambda x: round(x / 10, -math.floor(math.log10(x)) + 1)\n    ymax = plt.gca().get_ylim()[1]\n    step_size = np.clip(get_step_size(ymax), 5, 1000)\n    y_steps = np.arange(0, ymax, step_size)\n\n    for y_step in y_steps:\n        plt.hlines(y_step, xmax=10, xmin=-1, color=\"black\", linewidth=0.1)\n    plt.xlim([-0.5, len(ind)])\n    plt.ylim([0, ymax * 1.02])\n\n    plt.tight_layout()\n    plt.savefig(figpath)\n    plt.show()\n</code></pre>"},{"location":"api_reference/#paperscraper.plotting.plot_single","title":"<code>plot_single(data_dict: dict, keys: str, x_ticks: List[str] = ['2015', '2016', '2017', '2018', '2019', '2020'], show_preprint: bool = False, title_text: str = '', figpath: str = 'comparison_plot.pdf', logscale: bool = False) -&gt; None</code>","text":"<p>Plot temporal evolution of number of papers per keyword</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>A dictionary with keywords as keys. Each value should be a dictionary itself, with keys for the different APIs. For example data_dict = {     'covid_19.jsonl': {         'pubmed': [0, 0, 0, 12345],         'arxiv': [0, 0, 0, 1234],         ...     }     'coronavirus.jsonl':         'pubmed': [234, 345, 456, 12345],         'arxiv': [123, 234, 345, 1234],         ...     } }</p> required <code>keys</code> <code>str</code> <p>A key which should be plotted. This has to be a subset of data_dict.keys().</p> required <code>x_ticks</code> <code>List[str]</code> <p>List of strings to be used for the x-ticks. Should have same length as data_dict[key][database]. Defaults to ['2015', '2016', '2017', '2018', '2019', '2020'], meaning that papers are aggregated per year.</p> <code>['2015', '2016', '2017', '2018', '2019', '2020']</code> <code>show_preprint</code> <code>bool</code> <p>Whether preprint servers are aggregated or not. Defaults to False.</p> <code>False</code> <code>title_text</code> <code>str</code> <p>Title for the produced figure. Defaults to ''.</p> <code>''</code> <code>figpath</code> <code>str</code> <p>Name under which figure is saved. Relative or absolute paths can be given. Defaults to 'comparison_plot.pdf'.</p> <code>'comparison_plot.pdf'</code> <code>logscale</code> <code>bool</code> <p>Whether y-axis is plotted on logscale. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a database is missing in data_dict.</p> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_single(\n    data_dict: dict,\n    keys: str,\n    x_ticks: List[str] = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"],\n    show_preprint: bool = False,\n    title_text: str = \"\",\n    figpath: str = \"comparison_plot.pdf\",\n    logscale: bool = False,\n) -&gt; None:\n    \"\"\"Plot temporal evolution of number of papers per keyword\n\n    Args:\n        data_dict: A dictionary with keywords as keys. Each value should be a\n            dictionary itself, with keys for the different APIs. For example\n            data_dict = {\n                'covid_19.jsonl': {\n                    'pubmed': [0, 0, 0, 12345],\n                    'arxiv': [0, 0, 0, 1234],\n                    ...\n                }\n                'coronavirus.jsonl':\n                    'pubmed': [234, 345, 456, 12345],\n                    'arxiv': [123, 234, 345, 1234],\n                    ...\n                }\n            }\n        keys: A key which should be plotted. This has to be a subset of data_dict.keys().\n        x_ticks (List[str]): List of strings to be used for the x-ticks. Should have\n            same length as data_dict[key][database]. Defaults to ['2015', '2016',\n            '2017', '2018', '2019', '2020'], meaning that papers are aggregated per\n            year.\n        show_preprint: Whether preprint servers are aggregated or not.\n            Defaults to False.\n        title_text: Title for the produced figure. Defaults to ''.\n        figpath (str, optional): Name under which figure is saved. Relative or absolute\n            paths can be given. Defaults to 'comparison_plot.pdf'.\n        logscale: Whether y-axis is plotted on logscale. Defaults to False.\n\n    Raises:\n        KeyError: If a database is missing in data_dict.\n    \"\"\"\n\n    sns.set_palette(sns.color_palette(\"colorblind\", 10))\n    plt.rcParams.update({\"hatch.color\": \"w\"})\n    plt.rcParams[\"figure.facecolor\"] = \"white\"\n    plt.figure(figsize=(8, 5))\n\n    arxiv, biorxiv, pubmed, medrxiv, chemrxiv, preprint = [], [], [], [], [], []\n\n    for key in keys:\n        try:\n            arxiv.append(data_dict[key][\"arxiv\"])\n            biorxiv.append(data_dict[key][\"biorxiv\"])\n            medrxiv.append(data_dict[key][\"medrxiv\"])\n            chemrxiv.append(data_dict[key][\"chemrxiv\"])\n            pubmed.append(data_dict[key][\"pubmed\"])\n        except KeyError:\n            raise KeyError(\n                f\"Did not find all DBs for {key}, only found {data_dict[key].keys()}\"\n            )\n        preprint.append(arxiv[-1] + biorxiv[-1] + medrxiv[-1] + chemrxiv[-1])\n\n    ind = np.arange(len(arxiv[0]))  # the x locations for the groups\n    width = [0.75] * len(ind)  # the width of the bars: can also be len(x) sequence\n    fnc = np.log10 if logscale else np.copy\n\n    plts = []\n    legend_plts = []\n    if show_preprint:\n        bars = [pubmed, preprint]\n        legend_platform = [\"PubMed\", \"Preprint\"]\n        if logscale:\n            sums = np.array(pubmed) + np.array(preprint)\n            logsums = np.log10(sums)\n            bars = [pubmed * logsums / sums, preprint * logsums / sums]\n\n    else:\n        bars = [pubmed, arxiv, biorxiv, chemrxiv, medrxiv]\n        legend_platform = [\"PubMed\", \"ArXiv\", \"BiorXiv\", \"ChemRxiv\", \"MedRxiv\"]\n        if logscale:\n            sums = (\n                np.array(pubmed)\n                + np.array(arxiv)\n                + np.array(biorxiv)\n                + np.array(chemrxiv)\n                + np.array(medrxiv)\n            )\n            logsums = np.log10s(sums)\n            bars = [\n                pubmed * logsums / sums,\n                arxiv * logsums / sums,\n                biorxiv * logsums / sums,\n                chemrxiv * logsums / sums,\n                medrxiv * logsums / sums,\n            ]\n    for idx in range(len(keys)):\n        bottom = 0\n\n        for bidx, b in enumerate(bars):\n            if idx == 0:\n                p = plt.bar(\n                    ind,\n                    b[idx],\n                    width,\n                    linewidth=1,\n                    edgecolor=\"k\",\n                    bottom=bottom,\n                )\n            else:\n                p = plt.bar(\n                    ind,\n                    b[idx],\n                    width,\n                    color=next(iter(plts[bidx])).get_facecolor(),\n                    linewidth=1,\n                    edgecolor=\"k\",\n                    bottom=bottom,\n                )\n\n            bottom += b[idx]\n            plts.append(p)\n        legend_plts.append(\n            plt.bar(ind, np.zeros((len(ind),)), color=\"k\", bottom=bottom)\n        )\n\n    (\n        plt.ylabel(\"Counts\", size=17)\n        if not logscale\n        else plt.ylabel(\"Counts (log scale)\", size=17)\n    )\n    plt.xlabel(\"Years\", size=17)\n    plt.title(title_text, size=17)\n    # Customize minor tick labels\n\n    plt.xticks(ind, x_ticks, size=14)\n    ymax = plt.gca().get_ylim()[1]\n    if logscale:\n        yticks = np.arange(1, ymax).astype(int)\n        plt.yticks(yticks, np.power(10, yticks))\n\n    plt.tick_params(axis=\"y\", labelsize=17)\n\n    plt.legend(\n        legend_platform,\n        prop={\"size\": 14},\n        loc=\"upper left\",\n        title=\"Platform:\",\n        title_fontsize=17,\n        ncol=1,\n    )\n\n    get_step_size = lambda x: round(x / 10, -math.floor(math.log10(x)) + 1)\n    ymax = plt.gca().get_ylim()[1]\n\n    for y_step in plt.yticks()[0]:\n        plt.hlines(y_step, xmax=10, xmin=-1, color=\"black\", linewidth=0.1)\n    plt.xlim([-0.5, len(ind)])\n    plt.ylim([0, ymax * 1.02])\n\n    plt.tight_layout()\n    plt.savefig(figpath)\n    plt.show()\n</code></pre>"},{"location":"api_reference/#paperscraper.plotting.plot_venn_two","title":"<code>plot_venn_two(sizes: List[int], labels: List[str], figpath: str = 'venn_two.pdf', title: str = '', **kwargs) -&gt; None</code>","text":"<p>Plot a single Venn Diagram with two terms.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>List[int]</code> <p>List of ints of length 3. First two elements correspond to the labels, third one to the intersection.</p> required <code>labels</code> <code>[type]</code> <p>List of str of length 2, containing names of circles.</p> required <code>figpath</code> <code>str</code> <p>Name under which figure is saved. Defaults to 'venn_two.pdf', i.e. it is inferred from labels.</p> <code>'venn_two.pdf'</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to '', i.e. it is inferred from labels.</p> <code>''</code> <code>**kwargs</code> <p>Additional keyword arguments for venn2.</p> <code>{}</code> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_venn_two(\n    sizes: List[int],\n    labels: List[str],\n    figpath: str = \"venn_two.pdf\",\n    title: str = \"\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Plot a single Venn Diagram with two terms.\n\n    Args:\n        sizes (List[int]): List of ints of length 3. First two elements correspond to\n            the labels, third one to the intersection.\n        labels ([type]): List of str of length 2, containing names of circles.\n        figpath (str): Name under which figure is saved. Defaults to 'venn_two.pdf', i.e. it is\n            inferred from labels.\n        title (str): Title of the plot. Defaults to '', i.e. it is inferred from\n            labels.\n        **kwargs: Additional keyword arguments for venn2.\n    \"\"\"\n    assert len(sizes) == 3, \"Incorrect type/length of sizes\"\n    assert len(labels) == 2, \"Incorrect type/length of labels\"\n\n    title = get_name(labels) if title == \"\" else title\n    figname = title.lower().replace(\" vs. \", \"_\") if figpath == \"\" else figpath\n    venn2(subsets=sizes, set_labels=labels, alpha=0.6, **kwargs)\n    venn2_circles(\n        subsets=sizes, linestyle=\"solid\", linewidth=0.6, color=\"grey\", **kwargs\n    )\n    if kwargs.get(\"ax\", False):\n        print(kwargs, type(kwargs))\n        print(kwargs[\"ax\"])\n        kwargs[\"ax\"].set_title(title, fontdict={\"fontweight\": \"bold\"}, size=15)\n    else:\n        plt.title(title, fontdict={\"fontweight\": \"bold\"}, size=15)\n        plt.savefig(f\"{figname}.pdf\")\n</code></pre>"},{"location":"api_reference/#paperscraper.plotting.plot_venn_three","title":"<code>plot_venn_three(sizes: List[int], labels: List[str], figpath: str = '', title: str = '', **kwargs) -&gt; None</code>","text":"<p>Plot a single Venn Diagram with two terms.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>List[int]</code> <p>List of ints of length 3. First two elements correspond to the labels, third one to the intersection.</p> required <code>labels</code> <code>List[str]</code> <p>List of str of length 2, containing names of circles.</p> required <code>figpath</code> <code>str</code> <p>Name under which figure is saved. Defaults to '', i.e. it is inferred from labels.</p> <code>''</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to '', i.e. it is inferred from labels.</p> <code>''</code> <code>**kwargs</code> <p>Additional keyword arguments for venn3.</p> <code>{}</code> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_venn_three(\n    sizes: List[int], labels: List[str], figpath: str = \"\", title: str = \"\", **kwargs\n) -&gt; None:\n    \"\"\"Plot a single Venn Diagram with two terms.\n\n    Args:\n        sizes (List[int]): List of ints of length 3. First two elements correspond to\n            the labels, third one to the intersection.\n        labels (List[str]): List of str of length 2, containing names of circles.\n        figpath (str): Name under which figure is saved. Defaults to '', i.e. it is\n            inferred from labels.\n        title (str): Title of the plot. Defaults to '', i.e. it is inferred from\n            labels.\n        **kwargs: Additional keyword arguments for venn3.\n    \"\"\"\n    assert len(sizes) == 7, \"Incorrect type/length of sizes\"\n    assert len(labels) == 3, \"Incorrect type/length of labels\"\n\n    title = get_name(labels) if title == \"\" else title\n    figname = title.lower().replace(\" vs. \", \"_\") if figpath == \"\" else figpath\n\n    venn3(subsets=sizes, set_labels=labels, alpha=0.6, **kwargs)\n    venn3_circles(\n        subsets=sizes, linestyle=\"solid\", linewidth=0.6, color=\"grey\", **kwargs\n    )\n\n    if kwargs.get(\"ax\", False):\n        kwargs[\"ax\"].set_title(title, fontdict={\"fontweight\": \"bold\"}, size=15)\n    else:\n        plt.title(title, fontdict={\"fontweight\": \"bold\"}, size=15)\n        plt.savefig(f\"{figname}.pdf\")\n</code></pre>"},{"location":"api_reference/#paperscraper.plotting.plot_multiple_venn","title":"<code>plot_multiple_venn(sizes: List[List[int]], labels: List[List[str]], figname: str, titles: List[str], suptitle: str = '', gridspec_kw: dict = {}, figsize: Iterable = (8, 4.5), **kwargs) -&gt; None</code>","text":"<p>Plots multiple Venn Diagrams next to each other</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>List[List[int]]</code> <p>List of lists with sizes, one per Venn Diagram. Lengths of lists should be either 3 (plot_venn_two) or 7 (plot_venn_two).</p> required <code>labels</code> <code>List[List[str]]</code> <p>List of Lists of str containing names of circles. Lengths of lists should be either 2 or 3.</p> required <code>figname</code> <code>str</code> <p>Name under which figure is saved. Defaults to '', i.e. it is inferred from labels.</p> required <code>titles</code> <code>List[str]</code> <p>Titles of subplots. Should have same length like labels and sizes.</p> required <code>suptitle</code> <code>str</code> <p>Title of entire plot. Defaults to '', i.e. no title.</p> <code>''</code> <code>gridspec_kw</code> <code>dict</code> <p>Additional keyword args for plt.subplots. Useful to adjust width of plots. E.g.     gridspec_kw={'width_ratios': [1, 2]} will make the second Venn Diagram double as wide as first one.</p> <code>{}</code> <code>**kwargs</code> <p>Additional keyword arguments for venn3.</p> <code>{}</code> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_multiple_venn(\n    sizes: List[List[int]],\n    labels: List[List[str]],\n    figname: str,\n    titles: List[str],\n    suptitle: str = \"\",\n    gridspec_kw: dict = {},\n    figsize: Iterable = (8, 4.5),\n    **kwargs,\n) -&gt; None:\n    \"\"\"Plots multiple Venn Diagrams next to each other\n\n    Args:\n        sizes (List[List[int]]): List of lists with sizes, one per Venn Diagram.\n            Lengths of lists should be either 3 (plot_venn_two) or 7\n            (plot_venn_two).\n        labels (List[List[str]]): List of Lists of str containing names of circles.\n            Lengths of lists should be either 2 or 3.\n        figname (str): Name under which figure is saved. Defaults to '', i.e. it is\n            inferred from labels.\n        titles (List[str]): Titles of subplots. Should have same length like labels\n            and sizes.\n        suptitle (str): Title of entire plot. Defaults to '', i.e. no title.\n        gridspec_kw (dict): Additional keyword args for plt.subplots. Useful to\n            adjust width of plots. E.g.\n                gridspec_kw={'width_ratios': [1, 2]}\n            will make the second Venn Diagram double as wide as first one.\n        **kwargs: Additional keyword arguments for venn3.\n    \"\"\"\n\n    assert len(sizes) == len(labels), \"Length of labels &amp; sizes dont match.\"\n    assert len(sizes) == len(titles), \"Length of titles &amp; sizes dont match.\"\n    assert len(sizes) &gt; 1, \"At least 2 items should be provided.\"\n    assert all(list(map(lambda x: len(x) in [2, 3], labels))), \"Wrong label sizes.\"\n    assert all(list(map(lambda x: len(x) in [3, 7], sizes))), \"Wrong label sizes.\"\n\n    fig, axes = plt.subplots(1, len(sizes), gridspec_kw=gridspec_kw, figsize=figsize)\n    plt.suptitle(suptitle, size=18, fontweight=\"bold\")\n\n    figname = titles[0].lower().replace(\" vs. \", \"_\") if figname == \"\" else figname\n\n    for idx, (size, label, title) in enumerate(zip(sizes, labels, titles)):\n        if len(label) == 2:\n            plot_venn_two(size, label, title=title, ax=axes[idx])\n        elif len(label) == 3:\n            plot_venn_three(size, label, title=title, ax=axes[idx])\n\n    plt.savefig(f\"{figname}.pdf\")\n</code></pre>"},{"location":"api_reference/#paperscraper.postprocessing","title":"<code>postprocessing</code>","text":""},{"location":"api_reference/#paperscraper.postprocessing.aggregate_paper","title":"<code>aggregate_paper(data: List[Dict[str, str]], start_year: int = 2016, bins_per_year: int = 4, filtering: bool = False, filter_keys: List = list(), unwanted_keys: List = list(), return_filtered: bool = False, filter_abstract: bool = True, last_year: int = 2021)</code>","text":"<p>Consumes a list of unstructured keyword results from a .jsonl and aggregates papers into several bins per year.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict[str, str]]</code> <p>Content of a .jsonl file, i.e., a list of dictionaries, one per paper.</p> required <code>start_year</code> <code>int</code> <p>First year of interest. Defaults to 2016.</p> <code>2016</code> <code>bins_per_year</code> <code>int</code> <p>Defaults to 4 (quarterly aggregation).</p> <code>4</code> <code>filtering</code> <code>bool</code> <p>Whether or not all papers in .jsonl are perceived as matches or whether an additional sanity checking for the keywords is performed in abstract/title. Defaults to False.</p> <code>False</code> <code>filter_keys</code> <code>list</code> <p>List of str used for filtering. Only applies if filtering is True. Defaults to empty list.</p> <code>list()</code> <code>unwanted_keys</code> <code>list</code> <p>List of str that must not occur in either title or abstract. Only applies if filtering is True.</p> <code>list()</code> <code>return_filtered</code> <code>bool</code> <p>Whether the filtered matches are also returned. Only applies if filtering is True. Defaults to False.</p> <code>False</code> <code>filter_abstract</code> <code>bool</code> <p>Whether the keyword is searched in the abstract or not. Defaults to True.</p> <code>True</code> <code>last_year</code> <code>int</code> <p>Most recent year for the aggregation. Defaults to current year. All newer entries are discarded.</p> <code>2021</code> <p>Returns:</p> Name Type Description <code>bins</code> <code>array</code> <p>Vector of length number of years (2020 - start_year) x bins_per_year.</p> Source code in <code>paperscraper/postprocessing.py</code> <pre><code>def aggregate_paper(\n    data: List[Dict[str, str]],\n    start_year: int = 2016,\n    bins_per_year: int = 4,\n    filtering: bool = False,\n    filter_keys: List = list(),\n    unwanted_keys: List = list(),\n    return_filtered: bool = False,\n    filter_abstract: bool = True,\n    last_year: int = 2021,\n):\n    \"\"\"Consumes a list of unstructured keyword results from a .jsonl and\n    aggregates papers into several bins per year.\n\n    Args:\n        data (List[Dict[str,str]]): Content of a .jsonl file, i.e., a list of\n            dictionaries, one per paper.\n        start_year (int, optional): First year of interest. Defaults to 2016.\n        bins_per_year (int, optional): Defaults to 4 (quarterly aggregation).\n        filtering (bool, optional): Whether or not all papers in .jsonl are\n            perceived as matches or whether an additional sanity checking for\n            the keywords is performed in abstract/title. Defaults to False.\n        filter_keys (list, optional): List of str used for filtering. Only\n            applies if filtering is True. Defaults to empty list.\n        unwanted_keys (list, optional): List of str that must not occur in either\n            title or abstract. Only applies if filtering is True.\n        return_filtered (bool, optional): Whether the filtered matches are also\n            returned. Only applies if filtering is True. Defaults to False.\n        filter_abstract (bool, optional): Whether the keyword is searched in the abstract\n            or not. Defaults to True.\n        last_year (int, optional): Most recent year for the aggregation. Defaults\n            to current year. All newer entries are discarded.\n\n    Returns:\n        bins (np.array): Vector of length number of years (2020 - start_year) x\n            bins_per_year.\n    \"\"\"\n\n    if not isinstance(data, list):\n        raise ValueError(f\"Expected list, received {type(data)}\")\n    if not isinstance(bins_per_year, int):\n        raise ValueError(f\"Expected int, received {type(bins_per_year)}\")\n    if 12 % bins_per_year != 0:\n        raise ValueError(f\"Can't split year into {bins_per_year} bins\")\n\n    num_years = last_year - start_year + 1\n    bins = np.zeros((num_years * bins_per_year))\n\n    if len(data) == 0:\n        return bins if not return_filtered else (bins, [])\n\n    # Remove duplicate entries (keep only the first one)\n    df = pd.DataFrame(data).sort_values(by=\"date\", ascending=True)\n    data = df.drop_duplicates(subset=\"title\", keep=\"first\").to_dict(\"records\")\n\n    dates = [dd[\"date\"] for dd in data]\n\n    filtered = []\n    for paper, date in zip(data, dates):\n        year = int(date.split(\"-\")[0])\n        if year &lt; start_year or year &gt; last_year:\n            continue\n\n        # At least one synonym per keyword needs to be in either title or\n        # abstract.\n        if filtering and filter_keys != list():\n            # Filter out papers which undesired terms\n            unwanted = False\n            for unwanted_key in unwanted_keys:\n                if unwanted_key.lower() in paper[\"title\"].lower():\n                    unwanted = True\n                if (\n                    filter_abstract\n                    and paper[\"abstract\"] is not None\n                    and unwanted_key.lower() in paper[\"abstract\"].lower()\n                ):\n                    unwanted = True\n            if unwanted:\n                continue\n\n            got_keys = []\n            for key_term in filter_keys:\n                got_key = False\n                if not isinstance(key_term, list):\n                    key_term = [key_term]\n                for key in key_term:\n                    if key.lower() in paper[\"title\"].lower():\n                        got_key = True\n                    if (\n                        filter_abstract\n                        and paper[\"abstract\"] is not None\n                        and key.lower() in paper[\"abstract\"].lower()\n                    ):\n                        got_key = True\n                got_keys.append(got_key)\n\n            if len(got_keys) != sum(got_keys):\n                continue\n\n        filtered.append(paper)\n\n        if len(date.split(\"-\")) &lt; 2:\n            logger.warning(\n                f\"Paper without month {date}, randomly assigned month.{paper['title']}\"\n            )\n            month = np.random.choice(12)\n        else:\n            month = int(date.split(\"-\")[1])\n\n        year_bin = year - start_year\n        month_bin = int(np.floor((month - 1) / (12 / bins_per_year)))\n        bins[year_bin * bins_per_year + month_bin] += 1\n\n    if return_filtered:\n        return bins, filtered\n    else:\n        return bins\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed","title":"<code>pubmed</code>","text":""},{"location":"api_reference/#paperscraper.pubmed.dump_papers","title":"<code>dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None</code>","text":"<p>Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl file with one paper per line.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>DataFrame</code> <p>A dataframe of paper metadata, one paper per row.</p> required <code>filepath</code> <code>str</code> <p>Path to dump the papers, has to end with <code>.jsonl</code>.</p> required Source code in <code>paperscraper/utils.py</code> <pre><code>def dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None:\n    \"\"\"\n    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl\n    file with one paper per line.\n\n    Args:\n        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.\n        filepath (str): Path to dump the papers, has to end with `.jsonl`.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}\")\n    if not filepath.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a filepath with .jsonl extension\")\n\n    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):\n        papers = pd.DataFrame(papers)\n        logger.warning(\n            \"Preferably pass a pd.DataFrame, not a list of dictionaries. \"\n            \"Passing a list is a legacy functionality that might become deprecated.\"\n        )\n\n    if not isinstance(papers, pd.DataFrame):\n        raise TypeError(f\"papers must be a pd.DataFrame, not {type(papers)}\")\n\n    paper_list = list(papers.T.to_dict().values())\n\n    with open(filepath, \"w\") as f:\n        for paper in paper_list:\n            f.write(json.dumps(paper) + \"\\n\")\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.get_emails","title":"<code>get_emails(paper: PubMedArticle) -&gt; List</code>","text":"<p>Extracts author email addresses from PubMedArticle.</p> <p>Parameters:</p> Name Type Description Default <code>paper</code> <code>PubMedArticle</code> <p>An object of type PubMedArticle. Requires to have an 'author' field.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A possibly empty list of emails associated to authors of the paper.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_emails(paper: PubMedArticle) -&gt; List:\n    \"\"\"\n    Extracts author email addresses from PubMedArticle.\n\n    Args:\n        paper (PubMedArticle): An object of type PubMedArticle. Requires to have\n            an 'author' field.\n\n    Returns:\n        List: A possibly empty list of emails associated to authors of the paper.\n    \"\"\"\n\n    emails = []\n    for author in paper.authors:\n        for v in author.values():\n            if v is not None and \"@\" in v:\n                parts = v.split(\"@\")\n                if len(parts) == 2:\n                    # Found one email address\n                    prefix = parts[0].split(\" \")[-1]\n                    postfix = parts[1]\n                    mail = prefix + \"@\" + postfix\n                    if not (postfix.endswith(\".\") or postfix.endswith(\" \")):\n                        emails.append(mail)\n                    else:\n                        emails.append(mail[:-1])\n                else:\n                    # Found multiple addresses\n                    for idx, part in enumerate(parts):\n                        try:\n                            if idx == 0:\n                                prefix = part.split(\" \")[-1]\n                            else:\n                                postfix = part.split(\"\\n\")[0]\n\n                                if postfix.endswith(\".\"):\n                                    postfix = postfix[:-1]\n                                    mail = prefix + \"@\" + postfix\n                                else:\n                                    current_postfix = postfix.split(\" \")[0]\n                                    mail = prefix + \"@\" + current_postfix\n                                    prefix = postfix.split(\" \")[1]\n                                emails.append(mail)\n                        except IndexError:\n                            warnings.warn(f\"Mail could not be inferred from {part}.\")\n\n    return list(set(emails))\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.get_query_from_keywords_and_date","title":"<code>get_query_from_keywords_and_date(keywords: List[Union[str, List]], start_date: str = 'None', end_date: str = 'None') -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the pubmed API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> If start_date and end_date are left as default, the function is <p>identical to get_query_from_keywords.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to pubmed API.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_query_from_keywords_and_date(\n    keywords: List[Union[str, List]], start_date: str = \"None\", end_date: str = \"None\"\n) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the pubmed API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        start_date (str): Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date (str): End date for the search. Same notation as start_date.\n\n    Note: If start_date and end_date are left as default, the function is\n        identical to get_query_from_keywords.\n\n    Returns:\n        str: query to enter to pubmed API.\n    \"\"\"\n\n    query = get_query_from_keywords(keywords)\n\n    if start_date != \"None\" and end_date != \"None\":\n        date = date_root.format(start_date, end_date)\n    elif start_date != \"None\" and end_date == \"None\":\n        date = date_root.format(start_date, \"3000\")\n    elif start_date == \"None\" and end_date != \"None\":\n        date = date_root.format(\"1000\", end_date)\n    else:\n        return query\n\n    return query + \" AND \" + date\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.get_pubmed_papers","title":"<code>get_pubmed_papers(query: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], max_results: int = 9998, *args, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Performs PubMed API request of a query and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to PubMed API. Needs to match PubMed API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. NOTE: If 'emails' is passed, an attempt is made to extract author mail addresses.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>max_results</code> <code>int</code> <p>Maximal number of results retrieved from DB. Defaults to 9998, higher values likely raise problems due to PubMedAPI, see: https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit</p> <code>9998</code> <code>args</code> <p>additional arguments for pubmed.query</p> <code>()</code> <code>kwargs</code> <p>additional arguments for pubmed.query</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame. One paper per row.</p> Source code in <code>paperscraper/pubmed/pubmed.py</code> <pre><code>def get_pubmed_papers(\n    query: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    max_results: int = 9998,\n    *args,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs PubMed API request of a query and returns list of papers with\n    fields as desired.\n\n    Args:\n        query: Query to PubMed API. Needs to match PubMed API notation.\n        fields: List of strings with fields to keep in output.\n            NOTE: If 'emails' is passed, an attempt is made to extract author mail\n            addresses.\n        max_results: Maximal number of results retrieved from DB. Defaults\n            to 9998, higher values likely raise problems due to PubMedAPI, see:\n            https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit\n        args: additional arguments for pubmed.query\n        kwargs: additional arguments for pubmed.query\n\n    Returns:\n        pd.DataFrame. One paper per row.\n\n    \"\"\"\n    if max_results &gt; 9998:\n        logger.warning(\n            f\"\\nmax_results cannot be larger than 9998, received {max_results}.\"\n            \"This will likely result in a JSONDecodeError. Considering lowering `max_results`.\\n\"\n            \"For PubMed, ESearch can only retrieve the first 9,999 records matching the query. \"\n            \"To obtain more than 9,999 PubMed records, consider using EDirect that contains additional\"\n            \"logic to batch PubMed search results automatically so that an arbitrary number can be retrieved\"\n        )\n\n    try:\n        raw = list(PUBMED.query(query, max_results=max_results, *args, **kwargs))\n    except (TypeError, ValueError, KeyError) as e:\n        logger.warning(\n            \"PubMed query returned malformed payload; treating as empty. %s\", e\n        )\n        return pd.DataFrame(columns=list(fields))\n\n    get_mails = \"emails\" in fields\n    if get_mails:\n        fields.pop(fields.index(\"emails\"))\n\n    processed = [\n        {\n            pubmed_field_mapper.get(key, key): process_fields.get(\n                pubmed_field_mapper.get(key, key), lambda x: x\n            )(value)\n            for key, value in paper.toDict().items()\n            if pubmed_field_mapper.get(key, key) in fields\n        }\n        for paper in raw\n    ]\n    if get_mails:\n        for idx, paper in enumerate(raw):\n            processed[idx].update({\"emails\": get_emails(paper)})\n\n    return pd.DataFrame(processed)\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.get_and_dump_pubmed_papers","title":"<code>get_and_dump_pubmed_papers(keywords: List[Union[str, List[str]]], output_filepath: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], start_date: str = 'None', end_date: str = 'None', *args, **kwargs) -&gt; None</code>","text":"<p>Combines get_pubmed_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>List of keywords to request pubmed API. The outer list level will be considered as AND separated keys. The inner level as OR separated.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. Defaults to ['title', 'authors', 'date', 'abstract', 'journal', 'doi']. NOTE: If 'emails' is passed, an attempt is made to extract author mail addresses.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> Source code in <code>paperscraper/pubmed/pubmed.py</code> <pre><code>def get_and_dump_pubmed_papers(\n    keywords: List[Union[str, List[str]]],\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Combines get_pubmed_papers and dump_papers.\n\n    Args:\n        keywords: List of keywords to request pubmed API.\n            The outer list level will be considered as AND separated keys.\n            The inner level as OR separated.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n            Defaults to ['title', 'authors', 'date', 'abstract',\n            'journal', 'doi'].\n            NOTE: If 'emails' is passed, an attempt is made to extract author mail\n            addresses.\n        start_date: Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date: End date for the search. Same notation as start_date.\n    \"\"\"\n    # Translate keywords into query.\n    query = get_query_from_keywords_and_date(\n        keywords, start_date=start_date, end_date=end_date\n    )\n    papers = get_pubmed_papers(query, fields, *args, **kwargs)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.pubmed","title":"<code>pubmed</code>","text":""},{"location":"api_reference/#paperscraper.pubmed.pubmed.get_pubmed_papers","title":"<code>get_pubmed_papers(query: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], max_results: int = 9998, *args, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Performs PubMed API request of a query and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to PubMed API. Needs to match PubMed API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. NOTE: If 'emails' is passed, an attempt is made to extract author mail addresses.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>max_results</code> <code>int</code> <p>Maximal number of results retrieved from DB. Defaults to 9998, higher values likely raise problems due to PubMedAPI, see: https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit</p> <code>9998</code> <code>args</code> <p>additional arguments for pubmed.query</p> <code>()</code> <code>kwargs</code> <p>additional arguments for pubmed.query</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame. One paper per row.</p> Source code in <code>paperscraper/pubmed/pubmed.py</code> <pre><code>def get_pubmed_papers(\n    query: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    max_results: int = 9998,\n    *args,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs PubMed API request of a query and returns list of papers with\n    fields as desired.\n\n    Args:\n        query: Query to PubMed API. Needs to match PubMed API notation.\n        fields: List of strings with fields to keep in output.\n            NOTE: If 'emails' is passed, an attempt is made to extract author mail\n            addresses.\n        max_results: Maximal number of results retrieved from DB. Defaults\n            to 9998, higher values likely raise problems due to PubMedAPI, see:\n            https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit\n        args: additional arguments for pubmed.query\n        kwargs: additional arguments for pubmed.query\n\n    Returns:\n        pd.DataFrame. One paper per row.\n\n    \"\"\"\n    if max_results &gt; 9998:\n        logger.warning(\n            f\"\\nmax_results cannot be larger than 9998, received {max_results}.\"\n            \"This will likely result in a JSONDecodeError. Considering lowering `max_results`.\\n\"\n            \"For PubMed, ESearch can only retrieve the first 9,999 records matching the query. \"\n            \"To obtain more than 9,999 PubMed records, consider using EDirect that contains additional\"\n            \"logic to batch PubMed search results automatically so that an arbitrary number can be retrieved\"\n        )\n\n    try:\n        raw = list(PUBMED.query(query, max_results=max_results, *args, **kwargs))\n    except (TypeError, ValueError, KeyError) as e:\n        logger.warning(\n            \"PubMed query returned malformed payload; treating as empty. %s\", e\n        )\n        return pd.DataFrame(columns=list(fields))\n\n    get_mails = \"emails\" in fields\n    if get_mails:\n        fields.pop(fields.index(\"emails\"))\n\n    processed = [\n        {\n            pubmed_field_mapper.get(key, key): process_fields.get(\n                pubmed_field_mapper.get(key, key), lambda x: x\n            )(value)\n            for key, value in paper.toDict().items()\n            if pubmed_field_mapper.get(key, key) in fields\n        }\n        for paper in raw\n    ]\n    if get_mails:\n        for idx, paper in enumerate(raw):\n            processed[idx].update({\"emails\": get_emails(paper)})\n\n    return pd.DataFrame(processed)\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.pubmed.get_and_dump_pubmed_papers","title":"<code>get_and_dump_pubmed_papers(keywords: List[Union[str, List[str]]], output_filepath: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], start_date: str = 'None', end_date: str = 'None', *args, **kwargs) -&gt; None</code>","text":"<p>Combines get_pubmed_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>List of keywords to request pubmed API. The outer list level will be considered as AND separated keys. The inner level as OR separated.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. Defaults to ['title', 'authors', 'date', 'abstract', 'journal', 'doi']. NOTE: If 'emails' is passed, an attempt is made to extract author mail addresses.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> Source code in <code>paperscraper/pubmed/pubmed.py</code> <pre><code>def get_and_dump_pubmed_papers(\n    keywords: List[Union[str, List[str]]],\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Combines get_pubmed_papers and dump_papers.\n\n    Args:\n        keywords: List of keywords to request pubmed API.\n            The outer list level will be considered as AND separated keys.\n            The inner level as OR separated.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n            Defaults to ['title', 'authors', 'date', 'abstract',\n            'journal', 'doi'].\n            NOTE: If 'emails' is passed, an attempt is made to extract author mail\n            addresses.\n        start_date: Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date: End date for the search. Same notation as start_date.\n    \"\"\"\n    # Translate keywords into query.\n    query = get_query_from_keywords_and_date(\n        keywords, start_date=start_date, end_date=end_date\n    )\n    papers = get_pubmed_papers(query, fields, *args, **kwargs)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/#paperscraper.pubmed.utils.get_query_from_keywords","title":"<code>get_query_from_keywords(keywords: List[Union[str, List]]) -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the pubmed API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to pubmed API.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_query_from_keywords(keywords: List[Union[str, List]]) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the pubmed API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n\n    Returns:\n        str: query to enter to pubmed API.\n    \"\"\"\n\n    query = \"\"\n    for i, key in enumerate(keywords):\n        if isinstance(key, str):\n            query += f\"({key}) AND \"\n        elif isinstance(key, list):\n            inter = \"\".join([f\"({syn}) OR \" for syn in key])\n            query += finalize_disjunction(inter)\n\n    query = finalize_conjunction(query)\n    return query\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.utils.get_query_from_keywords_and_date","title":"<code>get_query_from_keywords_and_date(keywords: List[Union[str, List]], start_date: str = 'None', end_date: str = 'None') -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the pubmed API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> If start_date and end_date are left as default, the function is <p>identical to get_query_from_keywords.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to pubmed API.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_query_from_keywords_and_date(\n    keywords: List[Union[str, List]], start_date: str = \"None\", end_date: str = \"None\"\n) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the pubmed API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        start_date (str): Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date (str): End date for the search. Same notation as start_date.\n\n    Note: If start_date and end_date are left as default, the function is\n        identical to get_query_from_keywords.\n\n    Returns:\n        str: query to enter to pubmed API.\n    \"\"\"\n\n    query = get_query_from_keywords(keywords)\n\n    if start_date != \"None\" and end_date != \"None\":\n        date = date_root.format(start_date, end_date)\n    elif start_date != \"None\" and end_date == \"None\":\n        date = date_root.format(start_date, \"3000\")\n    elif start_date == \"None\" and end_date != \"None\":\n        date = date_root.format(\"1000\", end_date)\n    else:\n        return query\n\n    return query + \" AND \" + date\n</code></pre>"},{"location":"api_reference/#paperscraper.pubmed.utils.get_emails","title":"<code>get_emails(paper: PubMedArticle) -&gt; List</code>","text":"<p>Extracts author email addresses from PubMedArticle.</p> <p>Parameters:</p> Name Type Description Default <code>paper</code> <code>PubMedArticle</code> <p>An object of type PubMedArticle. Requires to have an 'author' field.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A possibly empty list of emails associated to authors of the paper.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_emails(paper: PubMedArticle) -&gt; List:\n    \"\"\"\n    Extracts author email addresses from PubMedArticle.\n\n    Args:\n        paper (PubMedArticle): An object of type PubMedArticle. Requires to have\n            an 'author' field.\n\n    Returns:\n        List: A possibly empty list of emails associated to authors of the paper.\n    \"\"\"\n\n    emails = []\n    for author in paper.authors:\n        for v in author.values():\n            if v is not None and \"@\" in v:\n                parts = v.split(\"@\")\n                if len(parts) == 2:\n                    # Found one email address\n                    prefix = parts[0].split(\" \")[-1]\n                    postfix = parts[1]\n                    mail = prefix + \"@\" + postfix\n                    if not (postfix.endswith(\".\") or postfix.endswith(\" \")):\n                        emails.append(mail)\n                    else:\n                        emails.append(mail[:-1])\n                else:\n                    # Found multiple addresses\n                    for idx, part in enumerate(parts):\n                        try:\n                            if idx == 0:\n                                prefix = part.split(\" \")[-1]\n                            else:\n                                postfix = part.split(\"\\n\")[0]\n\n                                if postfix.endswith(\".\"):\n                                    postfix = postfix[:-1]\n                                    mail = prefix + \"@\" + postfix\n                                else:\n                                    current_postfix = postfix.split(\" \")[0]\n                                    mail = prefix + \"@\" + current_postfix\n                                    prefix = postfix.split(\" \")[1]\n                                emails.append(mail)\n                        except IndexError:\n                            warnings.warn(f\"Mail could not be inferred from {part}.\")\n\n    return list(set(emails))\n</code></pre>"},{"location":"api_reference/#paperscraper.scholar","title":"<code>scholar</code>","text":""},{"location":"api_reference/#paperscraper.scholar.get_citations_from_title","title":"<code>get_citations_from_title(title: str) -&gt; int</code>","text":"<p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of paper to be searched on Scholar.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If sth else than str is passed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of citations of paper.</p> Source code in <code>paperscraper/citations/citations.py</code> <pre><code>def get_citations_from_title(title: str) -&gt; int:\n    \"\"\"\n    Args:\n        title (str): Title of paper to be searched on Scholar.\n\n    Raises:\n        TypeError: If sth else than str is passed.\n\n    Returns:\n        int: Number of citations of paper.\n    \"\"\"\n\n    if not isinstance(title, str):\n        raise TypeError(f\"Pass str not {type(title)}\")\n\n    # Search for exact match\n    title = '\"' + title.strip() + '\"'\n\n    matches = scholarly.search_pubs(title)\n    counts = list(map(lambda p: int(p[\"num_citations\"]), matches))\n    if len(counts) == 0:\n        logger.warning(f\"Found no match for {title}.\")\n        return 0\n    if len(counts) &gt; 1:\n        logger.warning(f\"Found {len(counts)} matches for {title}, returning first one.\")\n    return counts[0]\n</code></pre>"},{"location":"api_reference/#paperscraper.scholar.dump_papers","title":"<code>dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None</code>","text":"<p>Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl file with one paper per line.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>DataFrame</code> <p>A dataframe of paper metadata, one paper per row.</p> required <code>filepath</code> <code>str</code> <p>Path to dump the papers, has to end with <code>.jsonl</code>.</p> required Source code in <code>paperscraper/utils.py</code> <pre><code>def dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None:\n    \"\"\"\n    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl\n    file with one paper per line.\n\n    Args:\n        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.\n        filepath (str): Path to dump the papers, has to end with `.jsonl`.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}\")\n    if not filepath.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a filepath with .jsonl extension\")\n\n    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):\n        papers = pd.DataFrame(papers)\n        logger.warning(\n            \"Preferably pass a pd.DataFrame, not a list of dictionaries. \"\n            \"Passing a list is a legacy functionality that might become deprecated.\"\n        )\n\n    if not isinstance(papers, pd.DataFrame):\n        raise TypeError(f\"papers must be a pd.DataFrame, not {type(papers)}\")\n\n    paper_list = list(papers.T.to_dict().values())\n\n    with open(filepath, \"w\") as f:\n        for paper in paper_list:\n            f.write(json.dumps(paper) + \"\\n\")\n</code></pre>"},{"location":"api_reference/#paperscraper.scholar.get_scholar_papers","title":"<code>get_scholar_papers(title: str, fields: List = ['title', 'authors', 'year', 'abstract', 'journal', 'citations'], *args, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Performs Google Scholar API request of a given title and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Query to arxiv API. Needs to match the arxiv API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'year', 'abstract', 'journal', 'citations']</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame. One paper per row.</p> Source code in <code>paperscraper/scholar/scholar.py</code> <pre><code>def get_scholar_papers(\n    title: str,\n    fields: List = [\"title\", \"authors\", \"year\", \"abstract\", \"journal\", \"citations\"],\n    *args,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs Google Scholar API request of a given title and returns list of papers with\n    fields as desired.\n\n    Args:\n        title: Query to arxiv API. Needs to match the arxiv API notation.\n        fields: List of strings with fields to keep in output.\n\n    Returns:\n        pd.DataFrame. One paper per row.\n\n    \"\"\"\n    logger.info(\n        \"NOTE: Scholar API cannot be used with Boolean logic in keywords.\"\n        \"Query should be a single string to be entered in the Scholar search field.\"\n    )\n    if not isinstance(title, str):\n        raise TypeError(f\"Pass str not {type(title)}\")\n\n    matches = scholarly.search_pubs(title)\n\n    processed = []\n    for paper in matches:\n        # Extracts title, author, year, journal, abstract\n        entry = {\n            scholar_field_mapper.get(key, key): process_fields.get(\n                scholar_field_mapper.get(key, key), lambda x: x\n            )(value)\n            for key, value in paper[\"bib\"].items()\n            if scholar_field_mapper.get(key, key) in fields\n        }\n\n        entry[\"citations\"] = paper[\"num_citations\"]\n        processed.append(entry)\n\n    return pd.DataFrame(processed)\n</code></pre>"},{"location":"api_reference/#paperscraper.scholar.get_and_dump_scholar_papers","title":"<code>get_and_dump_scholar_papers(title: str, output_filepath: str, fields: List = ['title', 'authors', 'year', 'abstract', 'journal', 'citations']) -&gt; None</code>","text":"<p>Combines get_scholar_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Paper to search for on Google Scholar.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'year', 'abstract', 'journal', 'citations']</code> Source code in <code>paperscraper/scholar/scholar.py</code> <pre><code>def get_and_dump_scholar_papers(\n    title: str,\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"year\", \"abstract\", \"journal\", \"citations\"],\n) -&gt; None:\n    \"\"\"\n    Combines get_scholar_papers and dump_papers.\n\n    Args:\n        title: Paper to search for on Google Scholar.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n    \"\"\"\n    papers = get_scholar_papers(title, fields)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api_reference/#paperscraper.scholar.scholar","title":"<code>scholar</code>","text":""},{"location":"api_reference/#paperscraper.scholar.scholar.get_scholar_papers","title":"<code>get_scholar_papers(title: str, fields: List = ['title', 'authors', 'year', 'abstract', 'journal', 'citations'], *args, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Performs Google Scholar API request of a given title and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Query to arxiv API. Needs to match the arxiv API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'year', 'abstract', 'journal', 'citations']</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame. One paper per row.</p> Source code in <code>paperscraper/scholar/scholar.py</code> <pre><code>def get_scholar_papers(\n    title: str,\n    fields: List = [\"title\", \"authors\", \"year\", \"abstract\", \"journal\", \"citations\"],\n    *args,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs Google Scholar API request of a given title and returns list of papers with\n    fields as desired.\n\n    Args:\n        title: Query to arxiv API. Needs to match the arxiv API notation.\n        fields: List of strings with fields to keep in output.\n\n    Returns:\n        pd.DataFrame. One paper per row.\n\n    \"\"\"\n    logger.info(\n        \"NOTE: Scholar API cannot be used with Boolean logic in keywords.\"\n        \"Query should be a single string to be entered in the Scholar search field.\"\n    )\n    if not isinstance(title, str):\n        raise TypeError(f\"Pass str not {type(title)}\")\n\n    matches = scholarly.search_pubs(title)\n\n    processed = []\n    for paper in matches:\n        # Extracts title, author, year, journal, abstract\n        entry = {\n            scholar_field_mapper.get(key, key): process_fields.get(\n                scholar_field_mapper.get(key, key), lambda x: x\n            )(value)\n            for key, value in paper[\"bib\"].items()\n            if scholar_field_mapper.get(key, key) in fields\n        }\n\n        entry[\"citations\"] = paper[\"num_citations\"]\n        processed.append(entry)\n\n    return pd.DataFrame(processed)\n</code></pre>"},{"location":"api_reference/#paperscraper.scholar.scholar.get_and_dump_scholar_papers","title":"<code>get_and_dump_scholar_papers(title: str, output_filepath: str, fields: List = ['title', 'authors', 'year', 'abstract', 'journal', 'citations']) -&gt; None</code>","text":"<p>Combines get_scholar_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Paper to search for on Google Scholar.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'year', 'abstract', 'journal', 'citations']</code> Source code in <code>paperscraper/scholar/scholar.py</code> <pre><code>def get_and_dump_scholar_papers(\n    title: str,\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"year\", \"abstract\", \"journal\", \"citations\"],\n) -&gt; None:\n    \"\"\"\n    Combines get_scholar_papers and dump_papers.\n\n    Args:\n        title: Paper to search for on Google Scholar.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n    \"\"\"\n    papers = get_scholar_papers(title, fields)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api_reference/#paperscraper.server_dumps","title":"<code>server_dumps</code>","text":"<p>Folder for the metadata dumps from biorxiv, medrxiv and chemrxiv API. No code here but will be populated with your local <code>.jsonl</code> files.</p>"},{"location":"api_reference/#paperscraper.tests","title":"<code>tests</code>","text":""},{"location":"api_reference/#paperscraper.tests.test_pdf","title":"<code>test_pdf</code>","text":""},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF","title":"<code>TestPDF</code>","text":"Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>class TestPDF:\n    @pytest.fixture\n    def paper_data(self):\n        return {\"doi\": \"10.48550/arXiv.2207.03928\"}\n\n    def test_basic_search(self):\n        paper_data = {\"doi\": \"10.48550/arXiv.2207.03928\"}\n        save_pdf(paper_data, filepath=\"gt4sd.pdf\", save_metadata=True)\n        assert os.path.exists(\"gt4sd.pdf\")\n        assert os.path.exists(\"gt4sd.json\")\n        os.remove(\"gt4sd.pdf\")\n        os.remove(\"gt4sd.json\")\n\n        # chemrxiv\n        paper_data = {\"doi\": \"10.26434/chemrxiv-2021-np7xj-v4\"}\n        save_pdf(paper_data, filepath=\"kinases.pdf\", save_metadata=True)\n        assert os.path.exists(\"kinases.pdf\")\n        assert os.path.exists(\"kinases.json\")\n        os.remove(\"kinases.pdf\")\n        os.remove(\"kinases.json\")\n\n        # biorxiv\n        if os.path.exists(\"taskload.pdf\"):\n            os.remove(\"taskload.pdf\")\n        paper_data = {\"doi\": \"10.1101/798496\"}\n        # NOTE: biorxiv is cloudflare controlled so standard scraping fails\n\n        # Now try with S3 routine\n        keys = load_api_keys(\"api_keys.txt\")\n        save_pdf(\n            {\"doi\": \"10.1101/786871\"},\n            filepath=\"taskload.pdf\",\n            save_metadata=False,\n            api_keys=keys,\n        )\n        assert os.path.exists(\"taskload.pdf\")\n        os.remove(\"taskload.pdf\")\n\n        # Test S3 fallback with newer DOIs (including year/month/day)\n        FALLBACKS[\"s3\"](doi=\"10.1101/2023.10.09.561414\", output_path=\"taskload.pdf\", api_keys=keys)\n        assert os.path.exists(\"taskload.pdf\")\n        os.remove(\"taskload.pdf\")\n\n        # medrxiv now also seems cloudflare-controlled. skipping test\n        # paper_data = {\"doi\": \"10.1101/2020.09.02.20187096\"}\n        # save_pdf(paper_data, filepath=\"covid_review.pdf\", save_metadata=True)\n        # assert os.path.exists(\"covid_review.pdf\")\n        # assert os.path.exists(\"covid_review.json\")\n        # os.remove(\"covid_review.pdf\")\n        # os.remove(\"covid_review.json\")\n\n        # journal with OA paper\n        paper_data = {\"doi\": \"10.1038/s42256-023-00639-z\"}\n        save_pdf(paper_data, filepath=\"regression_transformer\", save_metadata=True)\n        assert os.path.exists(\"regression_transformer.pdf\")\n        assert os.path.exists(\"regression_transformer.json\")\n        os.remove(\"regression_transformer.pdf\")\n        os.remove(\"regression_transformer.json\")\n\n        # book chapter with paywall\n        paper_data = {\"doi\": \"10.1007/978-981-97-4828-0_7\"}\n        save_pdf(paper_data, filepath=\"clm_chapter\", save_metadata=True)\n        assert not os.path.exists(\"clm_chapter.pdf\")\n        assert os.path.exists(\"clm_chapter.json\")\n        os.remove(\"clm_chapter.json\")\n\n        # journal without OA paper\n        paper_data = {\"doi\": \"10.1126/science.adk9587\"}\n        save_pdf(paper_data, filepath=\"color\", save_metadata=True)\n        assert not os.path.exists(\"color.pdf\")\n        assert not os.path.exists(\"color.json\")\n\n    def test_missing_doi(self):\n        with pytest.raises(KeyError):\n            paper_data = {\"title\": \"Sample Paper\"}\n            save_pdf(paper_data, \"sample_paper.pdf\")\n\n    def test_invalid_metadata_type(self):\n        with pytest.raises(TypeError):\n            save_pdf(paper_metadata=\"not_a_dict\", filepath=\"output.pdf\")\n\n    def test_missing_doi_key(self):\n        with pytest.raises(KeyError):\n            save_pdf(paper_metadata={}, filepath=\"output.pdf\")\n\n    def test_invalid_filepath_type(self):\n        with pytest.raises(TypeError):\n            save_pdf(paper_metadata=self.paper_data, filepath=123)\n\n    def test_incorrect_filepath_extension(self):\n        with pytest.raises(TypeError):\n            save_pdf(paper_metadata=self.paper_data, filepath=\"output.txt\")\n\n    def test_incorrect_filepath_type(self):\n        with pytest.raises(TypeError):\n            save_pdf(paper_metadata=list(self.paper_data), filepath=\"output.txt\")\n\n    def test_nonexistent_directory_in_filepath(self, paper_data):\n        with pytest.raises(ValueError):\n            save_pdf(paper_metadata=paper_data, filepath=\"/nonexistent/output.pdf\")\n\n    @patch(\"requests.get\")\n    def test_network_issues_on_doi_url_request(self, mock_get, paper_data):\n        mock_get.side_effect = Exception(\"Network error\")\n        save_pdf(paper_metadata=paper_data, filepath=\"output.pdf\")\n        assert not os.path.exists(\"output.pdf\")\n\n    @patch(\"requests.get\")\n    def test_missing_pdf_url_in_meta_tags(self, mock_get, paper_data):\n        response = MagicMock()\n        response.text = \"&lt;html&gt;&lt;/html&gt;\"\n        mock_get.return_value = response\n        save_pdf(paper_metadata=paper_data, filepath=\"output.pdf\")\n        assert not os.path.exists(\"output.pdf\")\n\n    @patch(\"requests.get\")\n    def test_network_issues_on_pdf_url_request(self, mock_get, paper_data):\n        response_doi = MagicMock()\n        response_doi.text = (\n            '&lt;meta name=\"citation_pdf_url\" content=\"http://valid.url/document.pdf\"&gt;'\n        )\n        mock_get.side_effect = [response_doi, Exception(\"Network error\")]\n        save_pdf(paper_metadata=paper_data, filepath=\"output.pdf\")\n        assert not os.path.exists(\"output.pdf\")\n\n    def test_save_pdf_from_dump_wrong_type(self):\n        with pytest.raises(TypeError):\n            save_pdf_from_dump(-1, pdf_path=SAVE_PATH, key_to_save=\"doi\")\n\n    def test_save_pdf_from_dump_wrong_output_type(self):\n        with pytest.raises(TypeError):\n            save_pdf_from_dump(TEST_FILE_PATH, pdf_path=1, key_to_save=\"doi\")\n\n    def test_save_pdf_from_dump_wrong_suffix(self):\n        with pytest.raises(ValueError):\n            save_pdf_from_dump(\n                TEST_FILE_PATH.replace(\"jsonl\", \"json\"),\n                pdf_path=SAVE_PATH,\n                key_to_save=\"doi\",\n            )\n\n    def test_save_pdf_from_dump_wrong_key(self):\n        with pytest.raises(ValueError):\n            save_pdf_from_dump(TEST_FILE_PATH, pdf_path=SAVE_PATH, key_to_save=\"doix\")\n\n    def test_save_pdf_from_dump_wrong_key_type(self):\n        with pytest.raises(TypeError):\n            save_pdf_from_dump(TEST_FILE_PATH, pdf_path=SAVE_PATH, key_to_save=[\"doix\"])\n\n    def test_save_pdf_from_dump(self):\n        os.makedirs(SAVE_PATH, exist_ok=True)\n        save_pdf_from_dump(TEST_FILE_PATH, pdf_path=SAVE_PATH, key_to_save=\"doi\")\n        shutil.rmtree(SAVE_PATH)\n\n    def test_api_keys_none_pmc(self):\n        \"\"\"Test that save_pdf works properly even when no API keys are provided. Paper in PMC.\"\"\"\n        test_doi = {\"doi\": \"10.1038/s41587-022-01613-7\"}  # DOI known to be in PMC\n        filename = SAVE_PATH + \"_pmc\"\n        # Call function with no API keys\n        save_pdf(test_doi, filepath=filename, api_keys=None)\n\n        # Verify file was created - with .xml extension from PMC fallback\n        assert os.path.exists(filename + \".xml\"), (\n            \"XML file was not created via PMC fallback\"\n        )\n        os.remove(filename + \".xml\")\n\n    def test_api_keys_none_oa(self):\n        \"\"\"Test that save_pdf works properly even when no API keys are provided. Paper available open-access.\"\"\"\n        test_doi = {\"doi\": \"10.1038/s42256-023-00639-z\"}  # DOI known to be OA\n        filename = SAVE_PATH + \"_oa\"\n        # Call function with no API keys\n        save_pdf(test_doi, filepath=filename, api_keys=None)\n\n        # Verify file was created - with .pdf extension for direct PDF download\n        assert os.path.exists(filename + \".pdf\"), (\n            \"PDF file was not created for OA content\"\n        )\n        os.remove(filename + \".pdf\")\n\n    def test_api_key_file(self):\n        test_doi = {\"doi\": \"10.1002/smll.202309431\"}  # Use a DOI from Wiley\n        with open(\"tmp_keyfile.txt\", \"w\") as f:\n            f.write(\"WILEY_TDM_API_TOKEN=INVALID_TEST_KEY_123\")\n        save_pdf(test_doi, filepath=SAVE_PATH, api_keys=\"tmp_keyfile.txt\")\n        os.remove(\"tmp_keyfile.txt\")\n\n    def test_api_key_env(self):\n        test_doi = {\"doi\": \"10.1002/smll.202309431\"}  # Use a DOI known to be in PMC\n        with patch.dict(\n            os.environ, {\"WILEY_TDM_API_TOKEN\": \"ANOTHER_INVALID_TEST_KEY\"}\n        ):\n            save_pdf(test_doi, filepath=SAVE_PATH, api_keys=None)\n\n    @pytest.mark.skipif(\n        os.getenv(\"INSTITUTIONAL_NETWORK\") != \"1\",\n        reason=\"Not in an institutional network\",\n    )\n    def test_api_key_file_academic_network(self):\n        test_doi = {\"doi\": \"10.1002/smll.202309431\"}  # Use a DOI from Wiley\n        filename = SAVE_PATH + \"_wiley\"\n        wiley_key_path = SAVE_PATH + \"_wiley_key1\"\n        success = False\n        try:\n            with open(wiley_key_path, \"w\") as f:\n                f.write(\"WILEY_TDM_API_TOKEN=INVALID_TEST_KEY_123\")\n            save_pdf(test_doi, filepath=filename, api_keys=wiley_key_path)\n            # Verify file was created - with .pdf extension for Wiley content\n            assert os.path.exists(filename + \".pdf\"), (\n                \"PDF file was not created for Wiley content\"\n            )\n            success = True\n        finally:\n            for file in [filename + \".pdf\", wiley_key_path]:\n                if os.path.exists(file):\n                    os.remove(file)\n            if not success:\n                raise ValueError(\"PDF file was not created for Wiley content\")\n\n    @pytest.mark.skipif(\n        os.getenv(\"INSTITUTIONAL_NETWORK\") != \"1\",\n        reason=\"Not in an institutional network\",\n    )\n    def test_api_key_file_env_academic_network(self):\n        test_doi = {\"doi\": \"10.1002/smll.202309431\"}  # Use a DOI from Wiley\n        filename = SAVE_PATH + \"_wiley\"\n        line = \"WILEY_TDM_API_TOKEN=INVALID_TEST_KEY_123\\n\"\n        # Append to .env file in the current directory\n        with open(\".env\", \"a\") as f:\n            f.write(line)\n\n        try:\n            save_pdf(test_doi, filepath=filename, api_keys=None)\n\n            # Verify file was created - with .pdf extension for Wiley content\n            assert os.path.exists(filename + \".pdf\"), (\n                \"PDF file was not created for Wiley content\"\n            )\n        finally:\n            # Clean up\n            if os.path.exists(filename + \".pdf\"):\n                os.remove(filename + \".pdf\")\n            with open(\".env\", \"r\") as f:\n                lines = f.readlines()\n            if lines and lines[-1] == line:\n                with open(\".env\", \"w\") as f:\n                    f.writelines(lines[:-1])\n\n    def test_fallback_bioc_pmc_real_api(self):\n        \"\"\"Test the BioC-PMC fallback with a real API call.\"\"\"\n        test_doi = \"10.1038/s41587-022-01613-7\"  # Use a DOI known to be in PMC\n        output_path = Path(\"test_bioc_pmc_output\")\n        try:\n            result = FALLBACKS[\"bioc_pmc\"](test_doi, output_path)\n            assert result is True\n            assert (output_path.with_suffix(\".xml\")).exists()\n            with open(\n                output_path.with_suffix(\".xml\"), \"r\"\n            ) as f:  # Check if the file contains XML data\n                content = f.read()\n                assert \"&lt;\" in content and \"&gt;\" in content  # Basic XML check\n                assert len(content) &gt; 100  # Should have substantial content\n        finally:\n            if (output_path.with_suffix(\".xml\")).exists():\n                os.remove(output_path.with_suffix(\".xml\"))\n\n    def test_fallback_bioc_pmc_no_pmcid(self):\n        \"\"\"Test BioC-PMC fallback when no PMCID is available.\"\"\"\n        test_doi = \"10.1002/smll.202309431\"  # This DOI should not have a PMCID\n        output_path = Path(\"test_bioc_pmc_no_pmcid\")\n        result = FALLBACKS[\"bioc_pmc\"](test_doi, output_path)\n        assert result is False\n        assert not os.path.exists(output_path.with_suffix(\".xml\"))\n\n    def test_fallback_elife_xml_real_api(self):\n        \"\"\"Test the eLife XML fallback with a real API call.\"\"\"\n        test_doi = \"10.7554/eLife.100173\"  # Use a DOI known to be in eLife\n        output_path = Path(\"test_elife_xml_output\")\n        try:\n            result = FALLBACKS[\"elife\"](test_doi, output_path)\n            assert result is True\n            assert (output_path.with_suffix(\".xml\")).exists()\n            with open(\n                output_path.with_suffix(\".xml\"), \"r\"\n            ) as f:  # Check if the file contains XML data\n                content = f.read()\n                assert \"&lt;\" in content and \"&gt;\" in content  # Basic XML check\n                assert len(content) &gt; 100  # Should have substantial content\n        finally:\n            if (output_path.with_suffix(\".xml\")).exists():\n                os.remove(output_path.with_suffix(\".xml\"))\n\n    def test_fallback_elife_nonexistent_article(self):\n        \"\"\"Test eLife XML fallback with a DOI that looks like eLife but doesn't exist.\"\"\"\n        test_doi = (\n            \"10.7554/eLife.00001\"  # Article that doesn't exist in eLife repository\n        )\n        output_path = Path(\"test_elife_nonexistent\")\n        result = FALLBACKS[\"elife\"](test_doi, output_path)\n        # Assertions - should return False and not create a file\n        assert result is False\n        assert not os.path.exists(output_path.with_suffix(\".xml\"))\n\n    @patch(\"requests.get\")\n    def test_fallback_wiley_api_mock(self, mock_get):\n        \"\"\"Test Wiley API fallback with mocked response.\"\"\"\n        mock_response = MagicMock()\n        mock_response.content = b\"%PDF-1.5 test content\"\n        mock_response.raise_for_status = MagicMock()\n        mock_get.return_value = mock_response\n        paper_metadata = {\"doi\": \"10.1002/smll.202309431\"}\n        output_path = Path(\"test_wiley_output\")\n        api_keys = {\"WILEY_TDM_API_TOKEN\": \"test_token\"}\n        try:\n            FALLBACKS[\"wiley\"](paper_metadata, output_path, api_keys)\n            assert mock_get.called\n            mock_get.assert_called_with(\n                \"https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fsmll.202309431\",\n                headers={\"Wiley-TDM-Client-Token\": \"test_token\"},\n                allow_redirects=True,\n                timeout=60,\n            )\n            pdf_path = output_path.with_suffix(\".pdf\")\n            assert os.path.exists(pdf_path)\n            with open(pdf_path, \"rb\") as f:\n                content = f.read()\n                assert content == b\"%PDF-1.5 test content\"\n        finally:\n            if os.path.exists(output_path.with_suffix(\".pdf\")):\n                os.remove(output_path.with_suffix(\".pdf\"))\n\n    def test_fallback_wiley_api_returns_boolean(self):\n        \"\"\"Test that fallback_wiley_api properly returns a boolean value.\"\"\"\n        paper_metadata = {\"doi\": \"10.1002/smll.202309431\"}\n        output_path = Path(\"test_wiley_output\")\n        api_keys = {\"WILEY_TDM_API_TOKEN\": \"INVALID_TEST_KEY_123\"}\n        result = FALLBACKS[\"wiley\"](paper_metadata, output_path, api_keys)\n        # Check the result is a boolean\n        # will be True if on university network and False otherwise\n        assert isinstance(result, bool)\n        if result and output_path.with_suffix(\".pdf\").exists():\n            os.remove(output_path.with_suffix(\".pdf\"))\n\n    @patch(\"requests.get\")\n    def test_fallback_elsevier_api_mock(self, mock_get):\n        \"\"\"Test Elsevier API fallback with mocked response.\"\"\"\n        mock_response = MagicMock()\n        mock_response.content = b\"&lt;xml&gt;Test content&lt;/xml&gt;\"\n        mock_response.raise_for_status = MagicMock()\n        mock_get.return_value = mock_response\n        paper_metadata = {\"doi\": \"10.1016/j.xops.2024.100504\"}\n        output_path = Path(\"test_elsevier_output\")\n        api_keys = {\"ELSEVIER_TDM_API_KEY\": \"test_key\"}\n        try:\n            FALLBACKS[\"elsevier\"](paper_metadata, output_path, api_keys)\n            assert mock_get.called\n            mock_get.assert_called_with(\n                \"https://api.elsevier.com/content/article/doi/10.1016/j.xops.2024.100504?apiKey=test_key&amp;httpAccept=text%2Fxml\",\n                headers={\"Accept\": \"application/xml\"},\n                timeout=60,\n            )\n            xml_path = output_path.with_suffix(\".xml\")\n            assert os.path.exists(xml_path)\n            with open(xml_path, \"rb\") as f:\n                content = f.read()\n                assert content == b\"&lt;xml&gt;Test content&lt;/xml&gt;\"\n        finally:\n            if os.path.exists(output_path.with_suffix(\".xml\")):\n                os.remove(output_path.with_suffix(\".xml\"))\n\n    def test_fallback_elsevier_api_invalid_key(self, caplog):\n        \"\"\"Test real Elsevier API connectivity by verifying invalid key response pattern.\"\"\"\n        caplog.set_level(logging.ERROR)\n        paper_metadata = {\"doi\": \"10.1016/j.xops.2024.100504\"}\n        output_path = Path(\"test_elsevier_invalid\")\n        api_keys = {\"ELSEVIER_TDM_API_KEY\": \"INVALID_TEST_KEY_123\"}\n        result = FALLBACKS[\"elsevier\"](paper_metadata, output_path, api_keys)\n        # Should return False for invalid key\n        assert result is False\n        assert not output_path.with_suffix(\".xml\").exists()\n        # Check for the specific APIKEY_INVALID error in the logs\n        assert \"invalid\" in caplog.text.lower()\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_api_keys_none_pmc","title":"<code>test_api_keys_none_pmc()</code>","text":"<p>Test that save_pdf works properly even when no API keys are provided. Paper in PMC.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>def test_api_keys_none_pmc(self):\n    \"\"\"Test that save_pdf works properly even when no API keys are provided. Paper in PMC.\"\"\"\n    test_doi = {\"doi\": \"10.1038/s41587-022-01613-7\"}  # DOI known to be in PMC\n    filename = SAVE_PATH + \"_pmc\"\n    # Call function with no API keys\n    save_pdf(test_doi, filepath=filename, api_keys=None)\n\n    # Verify file was created - with .xml extension from PMC fallback\n    assert os.path.exists(filename + \".xml\"), (\n        \"XML file was not created via PMC fallback\"\n    )\n    os.remove(filename + \".xml\")\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_api_keys_none_oa","title":"<code>test_api_keys_none_oa()</code>","text":"<p>Test that save_pdf works properly even when no API keys are provided. Paper available open-access.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>def test_api_keys_none_oa(self):\n    \"\"\"Test that save_pdf works properly even when no API keys are provided. Paper available open-access.\"\"\"\n    test_doi = {\"doi\": \"10.1038/s42256-023-00639-z\"}  # DOI known to be OA\n    filename = SAVE_PATH + \"_oa\"\n    # Call function with no API keys\n    save_pdf(test_doi, filepath=filename, api_keys=None)\n\n    # Verify file was created - with .pdf extension for direct PDF download\n    assert os.path.exists(filename + \".pdf\"), (\n        \"PDF file was not created for OA content\"\n    )\n    os.remove(filename + \".pdf\")\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_fallback_bioc_pmc_real_api","title":"<code>test_fallback_bioc_pmc_real_api()</code>","text":"<p>Test the BioC-PMC fallback with a real API call.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>def test_fallback_bioc_pmc_real_api(self):\n    \"\"\"Test the BioC-PMC fallback with a real API call.\"\"\"\n    test_doi = \"10.1038/s41587-022-01613-7\"  # Use a DOI known to be in PMC\n    output_path = Path(\"test_bioc_pmc_output\")\n    try:\n        result = FALLBACKS[\"bioc_pmc\"](test_doi, output_path)\n        assert result is True\n        assert (output_path.with_suffix(\".xml\")).exists()\n        with open(\n            output_path.with_suffix(\".xml\"), \"r\"\n        ) as f:  # Check if the file contains XML data\n            content = f.read()\n            assert \"&lt;\" in content and \"&gt;\" in content  # Basic XML check\n            assert len(content) &gt; 100  # Should have substantial content\n    finally:\n        if (output_path.with_suffix(\".xml\")).exists():\n            os.remove(output_path.with_suffix(\".xml\"))\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_fallback_bioc_pmc_no_pmcid","title":"<code>test_fallback_bioc_pmc_no_pmcid()</code>","text":"<p>Test BioC-PMC fallback when no PMCID is available.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>def test_fallback_bioc_pmc_no_pmcid(self):\n    \"\"\"Test BioC-PMC fallback when no PMCID is available.\"\"\"\n    test_doi = \"10.1002/smll.202309431\"  # This DOI should not have a PMCID\n    output_path = Path(\"test_bioc_pmc_no_pmcid\")\n    result = FALLBACKS[\"bioc_pmc\"](test_doi, output_path)\n    assert result is False\n    assert not os.path.exists(output_path.with_suffix(\".xml\"))\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_fallback_elife_xml_real_api","title":"<code>test_fallback_elife_xml_real_api()</code>","text":"<p>Test the eLife XML fallback with a real API call.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>def test_fallback_elife_xml_real_api(self):\n    \"\"\"Test the eLife XML fallback with a real API call.\"\"\"\n    test_doi = \"10.7554/eLife.100173\"  # Use a DOI known to be in eLife\n    output_path = Path(\"test_elife_xml_output\")\n    try:\n        result = FALLBACKS[\"elife\"](test_doi, output_path)\n        assert result is True\n        assert (output_path.with_suffix(\".xml\")).exists()\n        with open(\n            output_path.with_suffix(\".xml\"), \"r\"\n        ) as f:  # Check if the file contains XML data\n            content = f.read()\n            assert \"&lt;\" in content and \"&gt;\" in content  # Basic XML check\n            assert len(content) &gt; 100  # Should have substantial content\n    finally:\n        if (output_path.with_suffix(\".xml\")).exists():\n            os.remove(output_path.with_suffix(\".xml\"))\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_fallback_elife_nonexistent_article","title":"<code>test_fallback_elife_nonexistent_article()</code>","text":"<p>Test eLife XML fallback with a DOI that looks like eLife but doesn't exist.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>def test_fallback_elife_nonexistent_article(self):\n    \"\"\"Test eLife XML fallback with a DOI that looks like eLife but doesn't exist.\"\"\"\n    test_doi = (\n        \"10.7554/eLife.00001\"  # Article that doesn't exist in eLife repository\n    )\n    output_path = Path(\"test_elife_nonexistent\")\n    result = FALLBACKS[\"elife\"](test_doi, output_path)\n    # Assertions - should return False and not create a file\n    assert result is False\n    assert not os.path.exists(output_path.with_suffix(\".xml\"))\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_fallback_wiley_api_mock","title":"<code>test_fallback_wiley_api_mock(mock_get)</code>","text":"<p>Test Wiley API fallback with mocked response.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>@patch(\"requests.get\")\ndef test_fallback_wiley_api_mock(self, mock_get):\n    \"\"\"Test Wiley API fallback with mocked response.\"\"\"\n    mock_response = MagicMock()\n    mock_response.content = b\"%PDF-1.5 test content\"\n    mock_response.raise_for_status = MagicMock()\n    mock_get.return_value = mock_response\n    paper_metadata = {\"doi\": \"10.1002/smll.202309431\"}\n    output_path = Path(\"test_wiley_output\")\n    api_keys = {\"WILEY_TDM_API_TOKEN\": \"test_token\"}\n    try:\n        FALLBACKS[\"wiley\"](paper_metadata, output_path, api_keys)\n        assert mock_get.called\n        mock_get.assert_called_with(\n            \"https://api.wiley.com/onlinelibrary/tdm/v1/articles/10.1002%2Fsmll.202309431\",\n            headers={\"Wiley-TDM-Client-Token\": \"test_token\"},\n            allow_redirects=True,\n            timeout=60,\n        )\n        pdf_path = output_path.with_suffix(\".pdf\")\n        assert os.path.exists(pdf_path)\n        with open(pdf_path, \"rb\") as f:\n            content = f.read()\n            assert content == b\"%PDF-1.5 test content\"\n    finally:\n        if os.path.exists(output_path.with_suffix(\".pdf\")):\n            os.remove(output_path.with_suffix(\".pdf\"))\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_fallback_wiley_api_returns_boolean","title":"<code>test_fallback_wiley_api_returns_boolean()</code>","text":"<p>Test that fallback_wiley_api properly returns a boolean value.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>def test_fallback_wiley_api_returns_boolean(self):\n    \"\"\"Test that fallback_wiley_api properly returns a boolean value.\"\"\"\n    paper_metadata = {\"doi\": \"10.1002/smll.202309431\"}\n    output_path = Path(\"test_wiley_output\")\n    api_keys = {\"WILEY_TDM_API_TOKEN\": \"INVALID_TEST_KEY_123\"}\n    result = FALLBACKS[\"wiley\"](paper_metadata, output_path, api_keys)\n    # Check the result is a boolean\n    # will be True if on university network and False otherwise\n    assert isinstance(result, bool)\n    if result and output_path.with_suffix(\".pdf\").exists():\n        os.remove(output_path.with_suffix(\".pdf\"))\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_fallback_elsevier_api_mock","title":"<code>test_fallback_elsevier_api_mock(mock_get)</code>","text":"<p>Test Elsevier API fallback with mocked response.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>@patch(\"requests.get\")\ndef test_fallback_elsevier_api_mock(self, mock_get):\n    \"\"\"Test Elsevier API fallback with mocked response.\"\"\"\n    mock_response = MagicMock()\n    mock_response.content = b\"&lt;xml&gt;Test content&lt;/xml&gt;\"\n    mock_response.raise_for_status = MagicMock()\n    mock_get.return_value = mock_response\n    paper_metadata = {\"doi\": \"10.1016/j.xops.2024.100504\"}\n    output_path = Path(\"test_elsevier_output\")\n    api_keys = {\"ELSEVIER_TDM_API_KEY\": \"test_key\"}\n    try:\n        FALLBACKS[\"elsevier\"](paper_metadata, output_path, api_keys)\n        assert mock_get.called\n        mock_get.assert_called_with(\n            \"https://api.elsevier.com/content/article/doi/10.1016/j.xops.2024.100504?apiKey=test_key&amp;httpAccept=text%2Fxml\",\n            headers={\"Accept\": \"application/xml\"},\n            timeout=60,\n        )\n        xml_path = output_path.with_suffix(\".xml\")\n        assert os.path.exists(xml_path)\n        with open(xml_path, \"rb\") as f:\n            content = f.read()\n            assert content == b\"&lt;xml&gt;Test content&lt;/xml&gt;\"\n    finally:\n        if os.path.exists(output_path.with_suffix(\".xml\")):\n            os.remove(output_path.with_suffix(\".xml\"))\n</code></pre>"},{"location":"api_reference/#paperscraper.tests.test_pdf.TestPDF.test_fallback_elsevier_api_invalid_key","title":"<code>test_fallback_elsevier_api_invalid_key(caplog)</code>","text":"<p>Test real Elsevier API connectivity by verifying invalid key response pattern.</p> Source code in <code>paperscraper/tests/test_pdf.py</code> <pre><code>def test_fallback_elsevier_api_invalid_key(self, caplog):\n    \"\"\"Test real Elsevier API connectivity by verifying invalid key response pattern.\"\"\"\n    caplog.set_level(logging.ERROR)\n    paper_metadata = {\"doi\": \"10.1016/j.xops.2024.100504\"}\n    output_path = Path(\"test_elsevier_invalid\")\n    api_keys = {\"ELSEVIER_TDM_API_KEY\": \"INVALID_TEST_KEY_123\"}\n    result = FALLBACKS[\"elsevier\"](paper_metadata, output_path, api_keys)\n    # Should return False for invalid key\n    assert result is False\n    assert not output_path.with_suffix(\".xml\").exists()\n    # Check for the specific APIKEY_INVALID error in the logs\n    assert \"invalid\" in caplog.text.lower()\n</code></pre>"},{"location":"api_reference/#paperscraper.utils","title":"<code>utils</code>","text":""},{"location":"api_reference/#paperscraper.utils.dump_papers","title":"<code>dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None</code>","text":"<p>Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl file with one paper per line.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>DataFrame</code> <p>A dataframe of paper metadata, one paper per row.</p> required <code>filepath</code> <code>str</code> <p>Path to dump the papers, has to end with <code>.jsonl</code>.</p> required Source code in <code>paperscraper/utils.py</code> <pre><code>def dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None:\n    \"\"\"\n    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl\n    file with one paper per line.\n\n    Args:\n        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.\n        filepath (str): Path to dump the papers, has to end with `.jsonl`.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}\")\n    if not filepath.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a filepath with .jsonl extension\")\n\n    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):\n        papers = pd.DataFrame(papers)\n        logger.warning(\n            \"Preferably pass a pd.DataFrame, not a list of dictionaries. \"\n            \"Passing a list is a legacy functionality that might become deprecated.\"\n        )\n\n    if not isinstance(papers, pd.DataFrame):\n        raise TypeError(f\"papers must be a pd.DataFrame, not {type(papers)}\")\n\n    paper_list = list(papers.T.to_dict().values())\n\n    with open(filepath, \"w\") as f:\n        for paper in paper_list:\n            f.write(json.dumps(paper) + \"\\n\")\n</code></pre>"},{"location":"api_reference/#paperscraper.utils.get_filename_from_query","title":"<code>get_filename_from_query(query: List[str]) -&gt; str</code>","text":"<p>Convert a keyword query into filenames to dump the paper.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list</code> <p>List of string with keywords.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Filename.</p> Source code in <code>paperscraper/utils.py</code> <pre><code>def get_filename_from_query(query: List[str]) -&gt; str:\n    \"\"\"Convert a keyword query into filenames to dump the paper.\n\n    Args:\n        query (list): List of string with keywords.\n\n    Returns:\n        str: Filename.\n    \"\"\"\n    filename = \"_\".join([k if isinstance(k, str) else k[0] for k in query]) + \".jsonl\"\n    filename = filename.replace(\" \", \"\").lower()\n    return filename\n</code></pre>"},{"location":"api_reference/#paperscraper.utils.load_jsonl","title":"<code>load_jsonl(filepath: str) -&gt; List[Dict[str, str]]</code>","text":"<p>Load data from a <code>.jsonl</code> file, i.e., a file with one dictionary per line.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to <code>.jsonl</code> file.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries, one per paper.</p> Source code in <code>paperscraper/utils.py</code> <pre><code>def load_jsonl(filepath: str) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Load data from a `.jsonl` file, i.e., a file with one dictionary per line.\n\n    Args:\n        filepath (str): Path to `.jsonl` file.\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries, one per paper.\n    \"\"\"\n\n    with open(filepath, \"r\") as f:\n        data = [json.loads(line) for line in f.readlines()]\n    return data\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv","title":"<code>xrxiv</code>","text":"<p>bioRxiv and medRxiv utilities.</p>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_api","title":"<code>xrxiv_api</code>","text":"<p>API for bioRxiv and medRXiv.</p>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_api.XRXivApi","title":"<code>XRXivApi</code>","text":"<p>API class.</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>class XRXivApi:\n    \"\"\"API class.\"\"\"\n\n    def __init__(\n        self,\n        server: str,\n        launch_date: str,\n        api_base_url: str = \"https://api.biorxiv.org\",\n        max_retries: int = 10,\n    ):\n        \"\"\"\n        Initialize API class.\n\n        Args:\n            server (str): name of the preprint server to access.\n            launch_date (str): launch date expressed as YYYY-MM-DD.\n            api_base_url (str, optional): Base url for the API. Defaults to 'api.biorxiv.org'.\n            max_retries (int, optional): Maximal number of retries for a request before an\n                error is raised. Defaults to 10.\n        \"\"\"\n        self.server = server\n        self.api_base_url = api_base_url\n        self.launch_date = launch_date\n        self.launch_datetime = datetime.fromisoformat(self.launch_date)\n        self.get_papers_url = (\n            \"{}/details/{}\".format(self.api_base_url, self.server)\n            + \"/{start_date}/{end_date}/{cursor}\"\n        )\n        self.max_retries = max_retries\n\n    @retry_multi()\n    def call_api(self, start_date, end_date, cursor):\n        try:\n            json_response = requests.get(\n                self.get_papers_url.format(\n                    start_date=start_date, end_date=end_date, cursor=cursor\n                ),\n                timeout=10,\n            ).json()\n        except requests.exceptions.Timeout:\n            logger.info(\"Timed out, will retry\")\n            return None\n\n        return json_response\n\n    def get_papers(\n        self,\n        start_date: Optional[str] = None,\n        end_date: Optional[str] = None,\n        fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n        max_retries: int = 10,\n    ) -&gt; Generator:\n        \"\"\"\n        Get paper metadata.\n\n        Args:\n            start_date (Optional[str]): begin date. Defaults to None, a.k.a. launch date.\n            end_date (Optional[str]): end date. Defaults to None, a.k.a. today.\n            fields (List[str], optional): fields to return per paper.\n                Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n            max_retries (int): Number of retries on connection failure. Defaults to 10.\n\n        Yields:\n            Generator: a generator of paper metadata (dict) with the desired fields.\n        \"\"\"\n        try:\n            now_datetime = datetime.now()\n            if start_date:\n                start_datetime = datetime.fromisoformat(start_date)\n                if start_datetime &lt; self.launch_datetime:\n                    start_date = self.launch_date\n            else:\n                start_date = self.launch_date\n            if end_date:\n                end_datetime = datetime.fromisoformat(end_date)\n                if end_datetime &gt; now_datetime:\n                    end_date = now_datetime.strftime(\"%Y-%m-%d\")\n            else:\n                end_date = now_datetime.strftime(\"%Y-%m-%d\")\n            do_loop = True\n            cursor = 0\n            while do_loop:\n                papers = []\n                for attempt in range(max_retries):\n                    try:\n                        json_response = self.call_api(start_date, end_date, cursor)\n                        do_loop = json_response[\"messages\"][0][\"status\"] == \"ok\"\n                        if do_loop:\n                            cursor += json_response[\"messages\"][0][\"count\"]\n                            for paper in json_response[\"collection\"]:\n                                processed_paper = {\n                                    field: paper.get(field, \"\") for field in fields\n                                }\n                                papers.append(processed_paper)\n\n                        if do_loop:\n                            yield from papers\n                            break\n                    except (ConnectionError, Timeout) as e:\n                        logger.error(\n                            f\"Connection error: {e}. Retrying ({attempt + 1}/{max_retries})\"\n                        )\n                        sleep(5)\n                        continue\n                    except Exception as exc:\n                        logger.exception(f\"Failed getting papers: {exc}\")\n        except Exception as exc:\n            logger.exception(f\"Failed getting papers: {exc}\")\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_api.XRXivApi.__init__","title":"<code>__init__(server: str, launch_date: str, api_base_url: str = 'https://api.biorxiv.org', max_retries: int = 10)</code>","text":"<p>Initialize API class.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>name of the preprint server to access.</p> required <code>launch_date</code> <code>str</code> <p>launch date expressed as YYYY-MM-DD.</p> required <code>api_base_url</code> <code>str</code> <p>Base url for the API. Defaults to 'api.biorxiv.org'.</p> <code>'https://api.biorxiv.org'</code> <code>max_retries</code> <code>int</code> <p>Maximal number of retries for a request before an error is raised. Defaults to 10.</p> <code>10</code> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>def __init__(\n    self,\n    server: str,\n    launch_date: str,\n    api_base_url: str = \"https://api.biorxiv.org\",\n    max_retries: int = 10,\n):\n    \"\"\"\n    Initialize API class.\n\n    Args:\n        server (str): name of the preprint server to access.\n        launch_date (str): launch date expressed as YYYY-MM-DD.\n        api_base_url (str, optional): Base url for the API. Defaults to 'api.biorxiv.org'.\n        max_retries (int, optional): Maximal number of retries for a request before an\n            error is raised. Defaults to 10.\n    \"\"\"\n    self.server = server\n    self.api_base_url = api_base_url\n    self.launch_date = launch_date\n    self.launch_datetime = datetime.fromisoformat(self.launch_date)\n    self.get_papers_url = (\n        \"{}/details/{}\".format(self.api_base_url, self.server)\n        + \"/{start_date}/{end_date}/{cursor}\"\n    )\n    self.max_retries = max_retries\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_api.XRXivApi.get_papers","title":"<code>get_papers(start_date: Optional[str] = None, end_date: Optional[str] = None, fields: List[str] = ['title', 'doi', 'authors', 'abstract', 'date', 'journal'], max_retries: int = 10) -&gt; Generator</code>","text":"<p>Get paper metadata.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Optional[str]</code> <p>begin date. Defaults to None, a.k.a. launch date.</p> <code>None</code> <code>end_date</code> <code>Optional[str]</code> <p>end date. Defaults to None, a.k.a. today.</p> <code>None</code> <code>fields</code> <code>List[str]</code> <p>fields to return per paper. Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].</p> <code>['title', 'doi', 'authors', 'abstract', 'date', 'journal']</code> <code>max_retries</code> <code>int</code> <p>Number of retries on connection failure. Defaults to 10.</p> <code>10</code> <p>Yields:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>a generator of paper metadata (dict) with the desired fields.</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>def get_papers(\n    self,\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n    max_retries: int = 10,\n) -&gt; Generator:\n    \"\"\"\n    Get paper metadata.\n\n    Args:\n        start_date (Optional[str]): begin date. Defaults to None, a.k.a. launch date.\n        end_date (Optional[str]): end date. Defaults to None, a.k.a. today.\n        fields (List[str], optional): fields to return per paper.\n            Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n        max_retries (int): Number of retries on connection failure. Defaults to 10.\n\n    Yields:\n        Generator: a generator of paper metadata (dict) with the desired fields.\n    \"\"\"\n    try:\n        now_datetime = datetime.now()\n        if start_date:\n            start_datetime = datetime.fromisoformat(start_date)\n            if start_datetime &lt; self.launch_datetime:\n                start_date = self.launch_date\n        else:\n            start_date = self.launch_date\n        if end_date:\n            end_datetime = datetime.fromisoformat(end_date)\n            if end_datetime &gt; now_datetime:\n                end_date = now_datetime.strftime(\"%Y-%m-%d\")\n        else:\n            end_date = now_datetime.strftime(\"%Y-%m-%d\")\n        do_loop = True\n        cursor = 0\n        while do_loop:\n            papers = []\n            for attempt in range(max_retries):\n                try:\n                    json_response = self.call_api(start_date, end_date, cursor)\n                    do_loop = json_response[\"messages\"][0][\"status\"] == \"ok\"\n                    if do_loop:\n                        cursor += json_response[\"messages\"][0][\"count\"]\n                        for paper in json_response[\"collection\"]:\n                            processed_paper = {\n                                field: paper.get(field, \"\") for field in fields\n                            }\n                            papers.append(processed_paper)\n\n                    if do_loop:\n                        yield from papers\n                        break\n                except (ConnectionError, Timeout) as e:\n                    logger.error(\n                        f\"Connection error: {e}. Retrying ({attempt + 1}/{max_retries})\"\n                    )\n                    sleep(5)\n                    continue\n                except Exception as exc:\n                    logger.exception(f\"Failed getting papers: {exc}\")\n    except Exception as exc:\n        logger.exception(f\"Failed getting papers: {exc}\")\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_api.BioRxivApi","title":"<code>BioRxivApi</code>","text":"<p>               Bases: <code>XRXivApi</code></p> <p>bioRxiv API.</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>class BioRxivApi(XRXivApi):\n    \"\"\"bioRxiv API.\"\"\"\n\n    def __init__(self, max_retries: int = 10):\n        super().__init__(\n            server=\"biorxiv\",\n            launch_date=launch_dates[\"biorxiv\"],\n            max_retries=max_retries,\n        )\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_api.MedRxivApi","title":"<code>MedRxivApi</code>","text":"<p>               Bases: <code>XRXivApi</code></p> <p>medRxiv API.</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>class MedRxivApi(XRXivApi):\n    \"\"\"medRxiv API.\"\"\"\n\n    def __init__(self, max_retries: int = 10):\n        super().__init__(\n            server=\"medrxiv\",\n            launch_date=launch_dates[\"medrxiv\"],\n            max_retries=max_retries,\n        )\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_api.retry_multi","title":"<code>retry_multi()</code>","text":"<p>Retry a function several times</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>def retry_multi():\n    \"\"\"Retry a function several times\"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            num_retries = 0\n            max_retries = getattr(self, \"max_retries\", 10)\n            while num_retries &lt;= max_retries:\n                try:\n                    ret = func(self, *args, **kwargs)\n                    if ret is None:\n                        time.sleep(5)\n                        continue\n                    break\n                except HTTPError:\n                    if num_retries == max_retries:\n                        raise\n                    num_retries += 1\n                    time.sleep(5)\n            return ret\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_query","title":"<code>xrxiv_query</code>","text":"<p>Query dumps from bioRxiv and medRXiv.</p>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_query.XRXivQuery","title":"<code>XRXivQuery</code>","text":"<p>Query class.</p> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>class XRXivQuery:\n    \"\"\"Query class.\"\"\"\n\n    def __init__(\n        self,\n        dump_filepath: str,\n        fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n    ):\n        \"\"\"\n        Initialize the query class.\n\n        Args:\n            dump_filepath (str): filepath to the dump to be queried.\n            fields (List[str], optional): fields to contained in the dump per paper.\n                Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n        \"\"\"\n        self.dump_filepath = dump_filepath\n        self.fields = fields\n        self.errored = False\n\n        try:\n            self.df = pd.read_json(self.dump_filepath, lines=True)\n            self.df[\"date\"] = [date.strftime(\"%Y-%m-%d\") for date in self.df[\"date\"]]\n        except ValueError as e:\n            logger.warning(f\"Problem in reading file {dump_filepath}: {e} - Skipping!\")\n            self.errored = True\n        except KeyError as e:\n            logger.warning(f\"Key {e} missing in file from {dump_filepath} - Skipping!\")\n            self.errored = True\n\n    def search_keywords(\n        self,\n        keywords: List[Union[str, List[str]]],\n        fields: List[str] = None,\n        output_filepath: str = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Search for papers in the dump using keywords.\n\n        Args:\n            keywords (List[str, List[str]]): Items will be AND separated. If items\n                are lists themselves, they will be OR separated.\n            fields (List[str], optional): fields to be used in the query search.\n                Defaults to None, a.k.a. search in all fields excluding date.\n            output_filepath (str, optional): optional output filepath where to store\n                the hits in JSONL format. Defaults to None, a.k.a., no export to a file.\n\n        Returns:\n            pd.DataFrame: A dataframe with one paper per row.\n        \"\"\"\n        if fields is None:\n            fields = self.fields\n        fields = [field for field in fields if field != \"date\"]\n        hits_per_field = []\n        for field in fields:\n            field_data = self.df[field].str.lower()\n            hits_per_keyword = []\n            for keyword in keywords:\n                if isinstance(keyword, list):\n                    query = \"|\".join([_.lower() for _ in keyword])\n                else:\n                    query = keyword.lower()\n                hits_per_keyword.append(field_data.str.contains(query))\n            if len(hits_per_keyword):\n                keyword_hits = hits_per_keyword[0]\n                for single_keyword_hits in hits_per_keyword[1:]:\n                    keyword_hits &amp;= single_keyword_hits\n                hits_per_field.append(keyword_hits)\n        if len(hits_per_field):\n            hits = hits_per_field[0]\n            for single_hits in hits_per_field[1:]:\n                hits |= single_hits\n        if output_filepath is not None:\n            self.df[hits].to_json(output_filepath, orient=\"records\", lines=True)\n        return self.df[hits]\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_query.XRXivQuery.__init__","title":"<code>__init__(dump_filepath: str, fields: List[str] = ['title', 'doi', 'authors', 'abstract', 'date', 'journal'])</code>","text":"<p>Initialize the query class.</p> <p>Parameters:</p> Name Type Description Default <code>dump_filepath</code> <code>str</code> <p>filepath to the dump to be queried.</p> required <code>fields</code> <code>List[str]</code> <p>fields to contained in the dump per paper. Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].</p> <code>['title', 'doi', 'authors', 'abstract', 'date', 'journal']</code> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>def __init__(\n    self,\n    dump_filepath: str,\n    fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n):\n    \"\"\"\n    Initialize the query class.\n\n    Args:\n        dump_filepath (str): filepath to the dump to be queried.\n        fields (List[str], optional): fields to contained in the dump per paper.\n            Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n    \"\"\"\n    self.dump_filepath = dump_filepath\n    self.fields = fields\n    self.errored = False\n\n    try:\n        self.df = pd.read_json(self.dump_filepath, lines=True)\n        self.df[\"date\"] = [date.strftime(\"%Y-%m-%d\") for date in self.df[\"date\"]]\n    except ValueError as e:\n        logger.warning(f\"Problem in reading file {dump_filepath}: {e} - Skipping!\")\n        self.errored = True\n    except KeyError as e:\n        logger.warning(f\"Key {e} missing in file from {dump_filepath} - Skipping!\")\n        self.errored = True\n</code></pre>"},{"location":"api_reference/#paperscraper.xrxiv.xrxiv_query.XRXivQuery.search_keywords","title":"<code>search_keywords(keywords: List[Union[str, List[str]]], fields: List[str] = None, output_filepath: str = None) -&gt; pd.DataFrame</code>","text":"<p>Search for papers in the dump using keywords.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>fields</code> <code>List[str]</code> <p>fields to be used in the query search. Defaults to None, a.k.a. search in all fields excluding date.</p> <code>None</code> <code>output_filepath</code> <code>str</code> <p>optional output filepath where to store the hits in JSONL format. Defaults to None, a.k.a., no export to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with one paper per row.</p> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>def search_keywords(\n    self,\n    keywords: List[Union[str, List[str]]],\n    fields: List[str] = None,\n    output_filepath: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Search for papers in the dump using keywords.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        fields (List[str], optional): fields to be used in the query search.\n            Defaults to None, a.k.a. search in all fields excluding date.\n        output_filepath (str, optional): optional output filepath where to store\n            the hits in JSONL format. Defaults to None, a.k.a., no export to a file.\n\n    Returns:\n        pd.DataFrame: A dataframe with one paper per row.\n    \"\"\"\n    if fields is None:\n        fields = self.fields\n    fields = [field for field in fields if field != \"date\"]\n    hits_per_field = []\n    for field in fields:\n        field_data = self.df[field].str.lower()\n        hits_per_keyword = []\n        for keyword in keywords:\n            if isinstance(keyword, list):\n                query = \"|\".join([_.lower() for _ in keyword])\n            else:\n                query = keyword.lower()\n            hits_per_keyword.append(field_data.str.contains(query))\n        if len(hits_per_keyword):\n            keyword_hits = hits_per_keyword[0]\n            for single_keyword_hits in hits_per_keyword[1:]:\n                keyword_hits &amp;= single_keyword_hits\n            hits_per_field.append(keyword_hits)\n    if len(hits_per_field):\n        hits = hits_per_field[0]\n        for single_hits in hits_per_field[1:]:\n            hits |= single_hits\n    if output_filepath is not None:\n        self.df[hits].to_json(output_filepath, orient=\"records\", lines=True)\n    return self.df[hits]\n</code></pre>"},{"location":"api/","title":"API Reference","text":"<p>This section documents the public API of paperscraper.</p> <p>Below you\u2019ll find links to the documentation for each module:</p> <ul> <li><code>paperscraper</code> \u2014 Main package entry point.</li> <li><code>paperscraper.arxiv</code> \u2014 ArXiv scraping &amp; keyword search</li> <li><code>paperscraper.citations</code> \u2014 Get (self-)citations &amp; (self-)reference of papers and authors</li> <li><code>paperscraper.get_dumps</code> \u2014 Utilities to download bioRxiv, medRxiv &amp; chemRxiv metadata</li> <li><code>paperscraper.pdf</code> \u2014 Download publications as pdfs</li> <li><code>paperscraper.pubmed</code> \u2014 Pubmed keyword search</li> <li><code>paperscraper.scholar</code> \u2014 Google Scholar endpoints</li> <li><code>paperscraper.xrxiv</code> \u2014 Shared utilities for {bio,med,chem}Rxiv </li> </ul>"},{"location":"api/#citation","title":"Citation","text":"<p>If you use <code>paperscraper</code>, please cite a paper that motivated our development of this tool.</p> <p> <pre><code>@article{born2021trends,\n  title={Trends in Deep Learning for Property-driven Drug Design},\n  author={Born, Jannis and Manica, Matteo},\n  journal={Current Medicinal Chemistry},\n  volume={28},\n  number={38},\n  pages={7862--7886},\n  year={2021},\n  publisher={Bentham Science Publishers}\n}\n</code></pre> </p>"},{"location":"api/#top-level-api","title":"Top-level API","text":""},{"location":"api/#paperscraper","title":"<code>paperscraper</code>","text":"<p>Initialize the module.</p>"},{"location":"api/#paperscraper.dump_queries","title":"<code>dump_queries(keywords: List[List[Union[str, List[str]]]], dump_root: str) -&gt; None</code>","text":"<p>Performs keyword search on all available servers and dump the results.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[List[Union[str, List[str]]]]</code> <p>List of lists of keywords Each second-level list is considered a separate query. Within each query, each item (whether str or List[str]) are considered AND separated. If an item is again a list, strs are considered synonyms (OR separated).</p> required <code>dump_root</code> <code>str</code> <p>Path to root for dumping.</p> required Source code in <code>paperscraper/__init__.py</code> <pre><code>def dump_queries(keywords: List[List[Union[str, List[str]]]], dump_root: str) -&gt; None:\n    \"\"\"Performs keyword search on all available servers and dump the results.\n\n    Args:\n        keywords (List[List[Union[str, List[str]]]]): List of lists of keywords\n            Each second-level list is considered a separate query. Within each\n            query, each item (whether str or List[str]) are considered AND\n            separated. If an item is again a list, strs are considered synonyms\n            (OR separated).\n        dump_root (str): Path to root for dumping.\n    \"\"\"\n\n    for idx, keyword in enumerate(keywords):\n        for db, f in QUERY_FN_DICT.items():\n            logger.info(f\" Keyword {idx + 1}/{len(keywords)}, DB: {db}\")\n            filename = get_filename_from_query(keyword)\n            os.makedirs(os.path.join(dump_root, db), exist_ok=True)\n            f(keyword, output_filepath=os.path.join(dump_root, db, filename))\n</code></pre>"},{"location":"api/arxiv/","title":"paperscraper.arxiv","text":""},{"location":"api/arxiv/#paperscraper.arxiv","title":"<code>paperscraper.arxiv</code>","text":""},{"location":"api/arxiv/#paperscraper.arxiv.XRXivQuery","title":"<code>XRXivQuery</code>","text":"<p>Query class.</p> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>class XRXivQuery:\n    \"\"\"Query class.\"\"\"\n\n    def __init__(\n        self,\n        dump_filepath: str,\n        fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n    ):\n        \"\"\"\n        Initialize the query class.\n\n        Args:\n            dump_filepath (str): filepath to the dump to be queried.\n            fields (List[str], optional): fields to contained in the dump per paper.\n                Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n        \"\"\"\n        self.dump_filepath = dump_filepath\n        self.fields = fields\n        self.errored = False\n\n        try:\n            self.df = pd.read_json(self.dump_filepath, lines=True)\n            self.df[\"date\"] = [date.strftime(\"%Y-%m-%d\") for date in self.df[\"date\"]]\n        except ValueError as e:\n            logger.warning(f\"Problem in reading file {dump_filepath}: {e} - Skipping!\")\n            self.errored = True\n        except KeyError as e:\n            logger.warning(f\"Key {e} missing in file from {dump_filepath} - Skipping!\")\n            self.errored = True\n\n    def search_keywords(\n        self,\n        keywords: List[Union[str, List[str]]],\n        fields: List[str] = None,\n        output_filepath: str = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Search for papers in the dump using keywords.\n\n        Args:\n            keywords (List[str, List[str]]): Items will be AND separated. If items\n                are lists themselves, they will be OR separated.\n            fields (List[str], optional): fields to be used in the query search.\n                Defaults to None, a.k.a. search in all fields excluding date.\n            output_filepath (str, optional): optional output filepath where to store\n                the hits in JSONL format. Defaults to None, a.k.a., no export to a file.\n\n        Returns:\n            pd.DataFrame: A dataframe with one paper per row.\n        \"\"\"\n        if fields is None:\n            fields = self.fields\n        fields = [field for field in fields if field != \"date\"]\n        hits_per_field = []\n        for field in fields:\n            field_data = self.df[field].str.lower()\n            hits_per_keyword = []\n            for keyword in keywords:\n                if isinstance(keyword, list):\n                    query = \"|\".join([_.lower() for _ in keyword])\n                else:\n                    query = keyword.lower()\n                hits_per_keyword.append(field_data.str.contains(query))\n            if len(hits_per_keyword):\n                keyword_hits = hits_per_keyword[0]\n                for single_keyword_hits in hits_per_keyword[1:]:\n                    keyword_hits &amp;= single_keyword_hits\n                hits_per_field.append(keyword_hits)\n        if len(hits_per_field):\n            hits = hits_per_field[0]\n            for single_hits in hits_per_field[1:]:\n                hits |= single_hits\n        if output_filepath is not None:\n            self.df[hits].to_json(output_filepath, orient=\"records\", lines=True)\n        return self.df[hits]\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.XRXivQuery.__init__","title":"<code>__init__(dump_filepath: str, fields: List[str] = ['title', 'doi', 'authors', 'abstract', 'date', 'journal'])</code>","text":"<p>Initialize the query class.</p> <p>Parameters:</p> Name Type Description Default <code>dump_filepath</code> <code>str</code> <p>filepath to the dump to be queried.</p> required <code>fields</code> <code>List[str]</code> <p>fields to contained in the dump per paper. Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].</p> <code>['title', 'doi', 'authors', 'abstract', 'date', 'journal']</code> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>def __init__(\n    self,\n    dump_filepath: str,\n    fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n):\n    \"\"\"\n    Initialize the query class.\n\n    Args:\n        dump_filepath (str): filepath to the dump to be queried.\n        fields (List[str], optional): fields to contained in the dump per paper.\n            Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n    \"\"\"\n    self.dump_filepath = dump_filepath\n    self.fields = fields\n    self.errored = False\n\n    try:\n        self.df = pd.read_json(self.dump_filepath, lines=True)\n        self.df[\"date\"] = [date.strftime(\"%Y-%m-%d\") for date in self.df[\"date\"]]\n    except ValueError as e:\n        logger.warning(f\"Problem in reading file {dump_filepath}: {e} - Skipping!\")\n        self.errored = True\n    except KeyError as e:\n        logger.warning(f\"Key {e} missing in file from {dump_filepath} - Skipping!\")\n        self.errored = True\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.XRXivQuery.search_keywords","title":"<code>search_keywords(keywords: List[Union[str, List[str]]], fields: List[str] = None, output_filepath: str = None) -&gt; pd.DataFrame</code>","text":"<p>Search for papers in the dump using keywords.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>fields</code> <code>List[str]</code> <p>fields to be used in the query search. Defaults to None, a.k.a. search in all fields excluding date.</p> <code>None</code> <code>output_filepath</code> <code>str</code> <p>optional output filepath where to store the hits in JSONL format. Defaults to None, a.k.a., no export to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with one paper per row.</p> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>def search_keywords(\n    self,\n    keywords: List[Union[str, List[str]]],\n    fields: List[str] = None,\n    output_filepath: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Search for papers in the dump using keywords.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        fields (List[str], optional): fields to be used in the query search.\n            Defaults to None, a.k.a. search in all fields excluding date.\n        output_filepath (str, optional): optional output filepath where to store\n            the hits in JSONL format. Defaults to None, a.k.a., no export to a file.\n\n    Returns:\n        pd.DataFrame: A dataframe with one paper per row.\n    \"\"\"\n    if fields is None:\n        fields = self.fields\n    fields = [field for field in fields if field != \"date\"]\n    hits_per_field = []\n    for field in fields:\n        field_data = self.df[field].str.lower()\n        hits_per_keyword = []\n        for keyword in keywords:\n            if isinstance(keyword, list):\n                query = \"|\".join([_.lower() for _ in keyword])\n            else:\n                query = keyword.lower()\n            hits_per_keyword.append(field_data.str.contains(query))\n        if len(hits_per_keyword):\n            keyword_hits = hits_per_keyword[0]\n            for single_keyword_hits in hits_per_keyword[1:]:\n                keyword_hits &amp;= single_keyword_hits\n            hits_per_field.append(keyword_hits)\n    if len(hits_per_field):\n        hits = hits_per_field[0]\n        for single_hits in hits_per_field[1:]:\n            hits |= single_hits\n    if output_filepath is not None:\n        self.df[hits].to_json(output_filepath, orient=\"records\", lines=True)\n    return self.df[hits]\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.dump_papers","title":"<code>dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None</code>","text":"<p>Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl file with one paper per line.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>DataFrame</code> <p>A dataframe of paper metadata, one paper per row.</p> required <code>filepath</code> <code>str</code> <p>Path to dump the papers, has to end with <code>.jsonl</code>.</p> required Source code in <code>paperscraper/utils.py</code> <pre><code>def dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None:\n    \"\"\"\n    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl\n    file with one paper per line.\n\n    Args:\n        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.\n        filepath (str): Path to dump the papers, has to end with `.jsonl`.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}\")\n    if not filepath.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a filepath with .jsonl extension\")\n\n    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):\n        papers = pd.DataFrame(papers)\n        logger.warning(\n            \"Preferably pass a pd.DataFrame, not a list of dictionaries. \"\n            \"Passing a list is a legacy functionality that might become deprecated.\"\n        )\n\n    if not isinstance(papers, pd.DataFrame):\n        raise TypeError(f\"papers must be a pd.DataFrame, not {type(papers)}\")\n\n    paper_list = list(papers.T.to_dict().values())\n\n    with open(filepath, \"w\") as f:\n        for paper in paper_list:\n            f.write(json.dumps(paper) + \"\\n\")\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.get_query_from_keywords","title":"<code>get_query_from_keywords(keywords: List[Union[str, List[str]]], start_date: str = 'None', end_date: str = 'None') -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the arxiv API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to arxiv API.</p> Source code in <code>paperscraper/arxiv/utils.py</code> <pre><code>def get_query_from_keywords(\n    keywords: List[Union[str, List[str]]],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the arxiv API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        start_date (str): Start date for the search. Needs to be in format:\n            YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date (str): End date for the search. Same notation as start_date.\n\n    Returns:\n        str: query to enter to arxiv API.\n    \"\"\"\n\n    query = \"\"\n    for i, key in enumerate(keywords):\n        if isinstance(key, str):\n            query += f\"all:{key} AND \"\n        elif isinstance(key, list):\n            inter = \"\".join([f\"all:{syn} OR \" for syn in key])\n            query += finalize_disjunction(inter)\n\n    query = finalize_conjunction(query)\n    if start_date == \"None\" and end_date == \"None\":\n        return query\n    elif start_date == \"None\":\n        start_date = EARLIEST_START\n    elif end_date == \"None\":\n        end_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    start = format_date(start_date)\n    end = format_date(end_date)\n    date_filter = f\" AND submittedDate:[{start} TO {end}]\"\n    return query + date_filter\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.get_arxiv_papers_local","title":"<code>get_arxiv_papers_local(keywords: List[Union[str, List[str]]], fields: List[str] = None, output_filepath: str = None) -&gt; pd.DataFrame</code>","text":"<p>Search for papers in the dump using keywords.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>fields</code> <code>List[str]</code> <p>fields to be used in the query search. Defaults to None, a.k.a. search in all fields excluding date.</p> <code>None</code> <code>output_filepath</code> <code>str</code> <p>optional output filepath where to store the hits in JSONL format. Defaults to None, a.k.a., no export to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with one paper per row.</p> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_arxiv_papers_local(\n    keywords: List[Union[str, List[str]]],\n    fields: List[str] = None,\n    output_filepath: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Search for papers in the dump using keywords.\n\n    Args:\n        keywords: Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        fields: fields to be used in the query search.\n            Defaults to None, a.k.a. search in all fields excluding date.\n        output_filepath: optional output filepath where to store the hits in JSONL format.\n            Defaults to None, a.k.a., no export to a file.\n\n    Returns:\n        pd.DataFrame: A dataframe with one paper per row.\n    \"\"\"\n    search_local_arxiv()\n    if ARXIV_QUERIER is None:\n        raise ValueError(\n            \"Could not find local arxiv dump. Use `backend=api` or download dump via `paperscraper.get_dumps.arxiv\"\n        )\n    return ARXIV_QUERIER(\n        keywords=keywords, fields=fields, output_filepath=output_filepath\n    )\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.get_arxiv_papers_api","title":"<code>get_arxiv_papers_api(query: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], max_results: int = 99999, client_options: Dict = {'num_retries': 10}, search_options: Dict = dict(), verbose: bool = True) -&gt; pd.DataFrame</code>","text":"<p>Performs arxiv API request of a given query and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to arxiv API. Needs to match the arxiv API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>max_results</code> <code>int</code> <p>Maximal number of results, defaults to 99999.</p> <code>99999</code> <code>client_options</code> <code>Dict</code> <p>Optional arguments for <code>arxiv.Client</code>. E.g.: page_size (int), delay_seconds (int), num_retries (int). NOTE: Decreasing 'num_retries' will speed up processing but might result in more frequent 'UnexpectedEmptyPageErrors'.</p> <code>{'num_retries': 10}</code> <code>search_options</code> <code>Dict</code> <p>Optional arguments for <code>arxiv.Search</code>. E.g.: id_list (List), sort_by, or sort_order.</p> <code>dict()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: One row per paper.</p> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_arxiv_papers_api(\n    query: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    max_results: int = 99999,\n    client_options: Dict = {\"num_retries\": 10},\n    search_options: Dict = dict(),\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs arxiv API request of a given query and returns list of papers with\n    fields as desired.\n\n    Args:\n        query: Query to arxiv API. Needs to match the arxiv API notation.\n        fields: List of strings with fields to keep in output.\n        max_results: Maximal number of results, defaults to 99999.\n        client_options: Optional arguments for `arxiv.Client`. E.g.:\n            page_size (int), delay_seconds (int), num_retries (int).\n            NOTE: Decreasing 'num_retries' will speed up processing but might\n            result in more frequent 'UnexpectedEmptyPageErrors'.\n        search_options: Optional arguments for `arxiv.Search`. E.g.:\n            id_list (List), sort_by, or sort_order.\n\n    Returns:\n        pd.DataFrame: One row per paper.\n\n    \"\"\"\n    client = arxiv.Client(**client_options)\n    search = arxiv.Search(query=query, max_results=max_results, **search_options)\n    results = client.results(search)\n\n    processed = pd.DataFrame(\n        [\n            {\n                arxiv_field_mapper.get(key, key): process_fields.get(\n                    arxiv_field_mapper.get(key, key), lambda x: x\n                )(value)\n                for key, value in vars(paper).items()\n                if arxiv_field_mapper.get(key, key) in fields and key != \"doi\"\n            }\n            for paper in tqdm(results, desc=f\"Processing {query}\", disable=not verbose)\n        ]\n    )\n    return processed\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.get_and_dump_arxiv_papers","title":"<code>get_and_dump_arxiv_papers(keywords: List[Union[str, List[str]]], output_filepath: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], start_date: str = 'None', end_date: str = 'None', backend: Literal['api', 'local', 'infer'] = 'api', *args, **kwargs)</code>","text":"<p>Combines get_arxiv_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>List of keywords for arxiv search. The outer list level will be considered as AND separated keys, the inner level as OR separated.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. Defaults to ['title', 'authors', 'date', 'abstract', 'journal', 'doi'].</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> <code>backend</code> <code>Literal['api', 'local', 'infer']</code> <p>If <code>api</code>, the arXiv API is queried. If <code>local</code> the local arXiv dump is queried (has to be downloaded before). If <code>infer</code> the local dump will be used if exists, otherwise API will be queried. Defaults to <code>api</code> since it is faster.</p> <code>'api'</code> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_and_dump_arxiv_papers(\n    keywords: List[Union[str, List[str]]],\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n    backend: Literal[\"api\", \"local\", \"infer\"] = \"api\",\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Combines get_arxiv_papers and dump_papers.\n\n    Args:\n        keywords: List of keywords for arxiv search.\n            The outer list level will be considered as AND separated keys, the\n            inner level as OR separated.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n            Defaults to ['title', 'authors', 'date', 'abstract',\n            'journal', 'doi'].\n        start_date: Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date: End date for the search. Same notation as start_date.\n        backend: If `api`, the arXiv API is queried. If `local` the local arXiv dump\n            is queried (has to be downloaded before). If `infer` the local dump will\n            be used if exists, otherwise API will be queried. Defaults to `api`\n            since it is faster.\n        *args, **kwargs are additional arguments for `get_arxiv_papers`.\n    \"\"\"\n    # Translate keywords into query.\n    query = get_query_from_keywords(keywords, start_date=start_date, end_date=end_date)\n\n    if backend not in {\"api\", \"local\", \"infer\"}:\n        raise ValueError(\n            f\"Invalid backend: {backend}. Must be one of ['api', 'local', 'infer']\"\n        )\n    elif backend == \"infer\":\n        backend = infer_backend()\n\n    if backend == \"api\":\n        papers = get_arxiv_papers_api(query, fields, *args, **kwargs)\n    elif backend == \"local\":\n        papers = get_arxiv_papers_local(keywords, fields, *args, **kwargs)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.arxiv","title":"<code>arxiv</code>","text":""},{"location":"api/arxiv/#paperscraper.arxiv.arxiv.get_arxiv_papers_local","title":"<code>get_arxiv_papers_local(keywords: List[Union[str, List[str]]], fields: List[str] = None, output_filepath: str = None) -&gt; pd.DataFrame</code>","text":"<p>Search for papers in the dump using keywords.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>fields</code> <code>List[str]</code> <p>fields to be used in the query search. Defaults to None, a.k.a. search in all fields excluding date.</p> <code>None</code> <code>output_filepath</code> <code>str</code> <p>optional output filepath where to store the hits in JSONL format. Defaults to None, a.k.a., no export to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with one paper per row.</p> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_arxiv_papers_local(\n    keywords: List[Union[str, List[str]]],\n    fields: List[str] = None,\n    output_filepath: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Search for papers in the dump using keywords.\n\n    Args:\n        keywords: Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        fields: fields to be used in the query search.\n            Defaults to None, a.k.a. search in all fields excluding date.\n        output_filepath: optional output filepath where to store the hits in JSONL format.\n            Defaults to None, a.k.a., no export to a file.\n\n    Returns:\n        pd.DataFrame: A dataframe with one paper per row.\n    \"\"\"\n    search_local_arxiv()\n    if ARXIV_QUERIER is None:\n        raise ValueError(\n            \"Could not find local arxiv dump. Use `backend=api` or download dump via `paperscraper.get_dumps.arxiv\"\n        )\n    return ARXIV_QUERIER(\n        keywords=keywords, fields=fields, output_filepath=output_filepath\n    )\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.arxiv.get_arxiv_papers_api","title":"<code>get_arxiv_papers_api(query: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], max_results: int = 99999, client_options: Dict = {'num_retries': 10}, search_options: Dict = dict(), verbose: bool = True) -&gt; pd.DataFrame</code>","text":"<p>Performs arxiv API request of a given query and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to arxiv API. Needs to match the arxiv API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>max_results</code> <code>int</code> <p>Maximal number of results, defaults to 99999.</p> <code>99999</code> <code>client_options</code> <code>Dict</code> <p>Optional arguments for <code>arxiv.Client</code>. E.g.: page_size (int), delay_seconds (int), num_retries (int). NOTE: Decreasing 'num_retries' will speed up processing but might result in more frequent 'UnexpectedEmptyPageErrors'.</p> <code>{'num_retries': 10}</code> <code>search_options</code> <code>Dict</code> <p>Optional arguments for <code>arxiv.Search</code>. E.g.: id_list (List), sort_by, or sort_order.</p> <code>dict()</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: One row per paper.</p> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_arxiv_papers_api(\n    query: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    max_results: int = 99999,\n    client_options: Dict = {\"num_retries\": 10},\n    search_options: Dict = dict(),\n    verbose: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs arxiv API request of a given query and returns list of papers with\n    fields as desired.\n\n    Args:\n        query: Query to arxiv API. Needs to match the arxiv API notation.\n        fields: List of strings with fields to keep in output.\n        max_results: Maximal number of results, defaults to 99999.\n        client_options: Optional arguments for `arxiv.Client`. E.g.:\n            page_size (int), delay_seconds (int), num_retries (int).\n            NOTE: Decreasing 'num_retries' will speed up processing but might\n            result in more frequent 'UnexpectedEmptyPageErrors'.\n        search_options: Optional arguments for `arxiv.Search`. E.g.:\n            id_list (List), sort_by, or sort_order.\n\n    Returns:\n        pd.DataFrame: One row per paper.\n\n    \"\"\"\n    client = arxiv.Client(**client_options)\n    search = arxiv.Search(query=query, max_results=max_results, **search_options)\n    results = client.results(search)\n\n    processed = pd.DataFrame(\n        [\n            {\n                arxiv_field_mapper.get(key, key): process_fields.get(\n                    arxiv_field_mapper.get(key, key), lambda x: x\n                )(value)\n                for key, value in vars(paper).items()\n                if arxiv_field_mapper.get(key, key) in fields and key != \"doi\"\n            }\n            for paper in tqdm(results, desc=f\"Processing {query}\", disable=not verbose)\n        ]\n    )\n    return processed\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.arxiv.get_and_dump_arxiv_papers","title":"<code>get_and_dump_arxiv_papers(keywords: List[Union[str, List[str]]], output_filepath: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], start_date: str = 'None', end_date: str = 'None', backend: Literal['api', 'local', 'infer'] = 'api', *args, **kwargs)</code>","text":"<p>Combines get_arxiv_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>List of keywords for arxiv search. The outer list level will be considered as AND separated keys, the inner level as OR separated.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. Defaults to ['title', 'authors', 'date', 'abstract', 'journal', 'doi'].</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> <code>backend</code> <code>Literal['api', 'local', 'infer']</code> <p>If <code>api</code>, the arXiv API is queried. If <code>local</code> the local arXiv dump is queried (has to be downloaded before). If <code>infer</code> the local dump will be used if exists, otherwise API will be queried. Defaults to <code>api</code> since it is faster.</p> <code>'api'</code> Source code in <code>paperscraper/arxiv/arxiv.py</code> <pre><code>def get_and_dump_arxiv_papers(\n    keywords: List[Union[str, List[str]]],\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n    backend: Literal[\"api\", \"local\", \"infer\"] = \"api\",\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    Combines get_arxiv_papers and dump_papers.\n\n    Args:\n        keywords: List of keywords for arxiv search.\n            The outer list level will be considered as AND separated keys, the\n            inner level as OR separated.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n            Defaults to ['title', 'authors', 'date', 'abstract',\n            'journal', 'doi'].\n        start_date: Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date: End date for the search. Same notation as start_date.\n        backend: If `api`, the arXiv API is queried. If `local` the local arXiv dump\n            is queried (has to be downloaded before). If `infer` the local dump will\n            be used if exists, otherwise API will be queried. Defaults to `api`\n            since it is faster.\n        *args, **kwargs are additional arguments for `get_arxiv_papers`.\n    \"\"\"\n    # Translate keywords into query.\n    query = get_query_from_keywords(keywords, start_date=start_date, end_date=end_date)\n\n    if backend not in {\"api\", \"local\", \"infer\"}:\n        raise ValueError(\n            f\"Invalid backend: {backend}. Must be one of ['api', 'local', 'infer']\"\n        )\n    elif backend == \"infer\":\n        backend = infer_backend()\n\n    if backend == \"api\":\n        papers = get_arxiv_papers_api(query, fields, *args, **kwargs)\n    elif backend == \"local\":\n        papers = get_arxiv_papers_local(keywords, fields, *args, **kwargs)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.utils","title":"<code>utils</code>","text":""},{"location":"api/arxiv/#paperscraper.arxiv.utils.format_date","title":"<code>format_date(date_str: str) -&gt; str</code>","text":"<p>Converts a date in YYYY-MM-DD format to arXiv's YYYYMMDDTTTT format.</p> Source code in <code>paperscraper/arxiv/utils.py</code> <pre><code>def format_date(date_str: str) -&gt; str:\n    \"\"\"Converts a date in YYYY-MM-DD format to arXiv's YYYYMMDDTTTT format.\"\"\"\n    date_obj = datetime.strptime(date_str, \"%Y-%m-%d\")\n    return date_obj.strftime(\"%Y%m%d0000\")\n</code></pre>"},{"location":"api/arxiv/#paperscraper.arxiv.utils.get_query_from_keywords","title":"<code>get_query_from_keywords(keywords: List[Union[str, List[str]]], start_date: str = 'None', end_date: str = 'None') -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the arxiv API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to arxiv API.</p> Source code in <code>paperscraper/arxiv/utils.py</code> <pre><code>def get_query_from_keywords(\n    keywords: List[Union[str, List[str]]],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the arxiv API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        start_date (str): Start date for the search. Needs to be in format:\n            YYYY-MM-DD, e.g. '2020-07-20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date (str): End date for the search. Same notation as start_date.\n\n    Returns:\n        str: query to enter to arxiv API.\n    \"\"\"\n\n    query = \"\"\n    for i, key in enumerate(keywords):\n        if isinstance(key, str):\n            query += f\"all:{key} AND \"\n        elif isinstance(key, list):\n            inter = \"\".join([f\"all:{syn} OR \" for syn in key])\n            query += finalize_disjunction(inter)\n\n    query = finalize_conjunction(query)\n    if start_date == \"None\" and end_date == \"None\":\n        return query\n    elif start_date == \"None\":\n        start_date = EARLIEST_START\n    elif end_date == \"None\":\n        end_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n    start = format_date(start_date)\n    end = format_date(end_date)\n    date_filter = f\" AND submittedDate:[{start} TO {end}]\"\n    return query + date_filter\n</code></pre>"},{"location":"api/citations/","title":"paperscraper.citations","text":""},{"location":"api/citations/#paperscraper.citations","title":"<code>paperscraper.citations</code>","text":""},{"location":"api/citations/#paperscraper.citations.citations","title":"<code>citations</code>","text":""},{"location":"api/citations/#paperscraper.citations.citations.get_citations_by_doi","title":"<code>get_citations_by_doi(doi: str) -&gt; int</code>","text":"<p>Get the number of citations of a paper according to semantic scholar.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>the DOI of the paper.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of citations</p> Source code in <code>paperscraper/citations/citations.py</code> <pre><code>def get_citations_by_doi(doi: str) -&gt; int:\n    \"\"\"\n    Get the number of citations of a paper according to semantic scholar.\n\n    Args:\n        doi: the DOI of the paper.\n\n    Returns:\n        The number of citations\n    \"\"\"\n\n    try:\n        paper = sch.get_paper(doi)\n        citations = len(paper[\"citations\"])\n    except SemanticScholarException.ObjectNotFoundException:\n        logger.warning(f\"Could not find paper {doi}, assuming 0 citation.\")\n        citations = 0\n    except ConnectionRefusedError as e:\n        logger.warning(f\"Waiting for 10 sec since {doi} gave: {e}\")\n        sleep(10)\n        citations = len(sch.get_paper(doi)[\"citations\"])\n    finally:\n        return citations\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.citations.get_citations_from_title","title":"<code>get_citations_from_title(title: str) -&gt; int</code>","text":"<p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of paper to be searched on Scholar.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If sth else than str is passed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of citations of paper.</p> Source code in <code>paperscraper/citations/citations.py</code> <pre><code>def get_citations_from_title(title: str) -&gt; int:\n    \"\"\"\n    Args:\n        title (str): Title of paper to be searched on Scholar.\n\n    Raises:\n        TypeError: If sth else than str is passed.\n\n    Returns:\n        int: Number of citations of paper.\n    \"\"\"\n\n    if not isinstance(title, str):\n        raise TypeError(f\"Pass str not {type(title)}\")\n\n    # Search for exact match\n    title = '\"' + title.strip() + '\"'\n\n    matches = scholarly.search_pubs(title)\n    counts = list(map(lambda p: int(p[\"num_citations\"]), matches))\n    if len(counts) == 0:\n        logger.warning(f\"Found no match for {title}.\")\n        return 0\n    if len(counts) &gt; 1:\n        logger.warning(f\"Found {len(counts)} matches for {title}, returning first one.\")\n    return counts[0]\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity","title":"<code>entity</code>","text":""},{"location":"api/citations/#paperscraper.citations.entity.core","title":"<code>core</code>","text":""},{"location":"api/citations/#paperscraper.citations.entity.core.Entity","title":"<code>Entity</code>","text":"<p>An abstract entity class with a set of utilities shared by the objects that perform self-linking analyses, such as Paper and Researcher.</p> Source code in <code>paperscraper/citations/entity/core.py</code> <pre><code>class Entity:\n    \"\"\"\n    An abstract entity class with a set of utilities shared by the objects that perform\n    self-linking analyses, such as Paper and Researcher.\n    \"\"\"\n\n    @abstractmethod\n    def self_references(self):\n        \"\"\"\n        Has to be implemented by the child class. Performs a self-referencing analyses\n        for the object.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def self_citations(self):\n        \"\"\"\n        Has to be implemented by the child class. Performs a self-citation analyses\n        for the object.\n        \"\"\"\n        ...\n\n    @abstractmethod\n    def get_result(self):\n        \"\"\"\n        Has to be implemented by the child class. Provides the result of the analysis.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.core.Entity.self_references","title":"<code>self_references()</code>  <code>abstractmethod</code>","text":"<p>Has to be implemented by the child class. Performs a self-referencing analyses for the object.</p> Source code in <code>paperscraper/citations/entity/core.py</code> <pre><code>@abstractmethod\ndef self_references(self):\n    \"\"\"\n    Has to be implemented by the child class. Performs a self-referencing analyses\n    for the object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.core.Entity.self_citations","title":"<code>self_citations()</code>  <code>abstractmethod</code>","text":"<p>Has to be implemented by the child class. Performs a self-citation analyses for the object.</p> Source code in <code>paperscraper/citations/entity/core.py</code> <pre><code>@abstractmethod\ndef self_citations(self):\n    \"\"\"\n    Has to be implemented by the child class. Performs a self-citation analyses\n    for the object.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.core.Entity.get_result","title":"<code>get_result()</code>  <code>abstractmethod</code>","text":"<p>Has to be implemented by the child class. Provides the result of the analysis.</p> Source code in <code>paperscraper/citations/entity/core.py</code> <pre><code>@abstractmethod\ndef get_result(self):\n    \"\"\"\n    Has to be implemented by the child class. Provides the result of the analysis.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.paper","title":"<code>paper</code>","text":""},{"location":"api/citations/#paperscraper.citations.entity.paper.Paper","title":"<code>Paper</code>","text":"<p>               Bases: <code>Entity</code></p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>class Paper(Entity):\n    title: str = \"\"\n    doi: str = \"\"\n    authors: List[str] = []\n\n    def __init__(self, input: str, mode: ModeType = \"infer\"):\n        \"\"\"\n        Set up a Paper object for analysis.\n\n        Args:\n            input: Paper identifier. This can be the title, DOI or semantic scholar ID\n                of the paper.\n            mode: The format in which the ID was provided. Defaults to \"infer\".\n\n        Raises:\n            ValueError: If unknown mode is given.\n        \"\"\"\n        if mode not in MODES:\n            raise ValueError(f\"Unknown mode {mode} chose from {MODES}.\")\n\n        input = input.strip()\n        self.input = input\n        if mode == \"infer\":\n            mode = determine_paper_input_type(input)\n\n        if mode == \"doi\":\n            self.doi = input\n        elif mode == \"title\":\n            self.doi = get_doi_from_title(input)\n        elif mode == \"ssid\":\n            self.doi = get_doi_from_ssid(input)\n\n        if self.doi is not None:\n            out = get_title_and_id_from_doi(self.doi)\n            if out is not None:\n                self.title = out[\"title\"]\n                self.ssid = out[\"ssid\"]\n\n    def self_references(self):\n        \"\"\"\n        Extracts the self references of a paper, for each author.\n        \"\"\"\n        if isinstance(self.doi, str):\n            self.ref_result: ReferenceResult = self_references_paper(self.doi)\n\n    def self_citations(self):\n        \"\"\"\n        Extracts the self citations of a paper, for each author.\n        \"\"\"\n        if isinstance(self.doi, str):\n            self.citation_result: CitationResult = self_citations_paper(self.doi)\n\n    def get_result(self) -&gt; Optional[PaperResult]:\n        \"\"\"\n        Provides the result of the analysis.\n\n        Returns: PaperResult if available.\n        \"\"\"\n        if not hasattr(self, \"ref_result\"):\n            logger.warning(\n                f\"Can't get result since no referencing result for {self.input} exists. Run `.self_references` first.\"\n            )\n            return\n        elif not hasattr(self, \"citation_result\"):\n            logger.warning(\n                f\"Can't get result since no citation result for {self.input} exists. Run `.self_citations` first.\"\n            )\n            return\n        ref_result = self.ref_result.model_dump()\n        ref_result.pop(\"ssid\", None)\n        return PaperResult(\n            title=self.title, **ref_result, **self.citation_result.model_dump()\n        )\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.paper.Paper.__init__","title":"<code>__init__(input: str, mode: ModeType = 'infer')</code>","text":"<p>Set up a Paper object for analysis.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>Paper identifier. This can be the title, DOI or semantic scholar ID of the paper.</p> required <code>mode</code> <code>ModeType</code> <p>The format in which the ID was provided. Defaults to \"infer\".</p> <code>'infer'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unknown mode is given.</p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>def __init__(self, input: str, mode: ModeType = \"infer\"):\n    \"\"\"\n    Set up a Paper object for analysis.\n\n    Args:\n        input: Paper identifier. This can be the title, DOI or semantic scholar ID\n            of the paper.\n        mode: The format in which the ID was provided. Defaults to \"infer\".\n\n    Raises:\n        ValueError: If unknown mode is given.\n    \"\"\"\n    if mode not in MODES:\n        raise ValueError(f\"Unknown mode {mode} chose from {MODES}.\")\n\n    input = input.strip()\n    self.input = input\n    if mode == \"infer\":\n        mode = determine_paper_input_type(input)\n\n    if mode == \"doi\":\n        self.doi = input\n    elif mode == \"title\":\n        self.doi = get_doi_from_title(input)\n    elif mode == \"ssid\":\n        self.doi = get_doi_from_ssid(input)\n\n    if self.doi is not None:\n        out = get_title_and_id_from_doi(self.doi)\n        if out is not None:\n            self.title = out[\"title\"]\n            self.ssid = out[\"ssid\"]\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.paper.Paper.self_references","title":"<code>self_references()</code>","text":"<p>Extracts the self references of a paper, for each author.</p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>def self_references(self):\n    \"\"\"\n    Extracts the self references of a paper, for each author.\n    \"\"\"\n    if isinstance(self.doi, str):\n        self.ref_result: ReferenceResult = self_references_paper(self.doi)\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.paper.Paper.self_citations","title":"<code>self_citations()</code>","text":"<p>Extracts the self citations of a paper, for each author.</p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>def self_citations(self):\n    \"\"\"\n    Extracts the self citations of a paper, for each author.\n    \"\"\"\n    if isinstance(self.doi, str):\n        self.citation_result: CitationResult = self_citations_paper(self.doi)\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.paper.Paper.get_result","title":"<code>get_result() -&gt; Optional[PaperResult]</code>","text":"<p>Provides the result of the analysis.</p> <p>Returns: PaperResult if available.</p> Source code in <code>paperscraper/citations/entity/paper.py</code> <pre><code>def get_result(self) -&gt; Optional[PaperResult]:\n    \"\"\"\n    Provides the result of the analysis.\n\n    Returns: PaperResult if available.\n    \"\"\"\n    if not hasattr(self, \"ref_result\"):\n        logger.warning(\n            f\"Can't get result since no referencing result for {self.input} exists. Run `.self_references` first.\"\n        )\n        return\n    elif not hasattr(self, \"citation_result\"):\n        logger.warning(\n            f\"Can't get result since no citation result for {self.input} exists. Run `.self_citations` first.\"\n        )\n        return\n    ref_result = self.ref_result.model_dump()\n    ref_result.pop(\"ssid\", None)\n    return PaperResult(\n        title=self.title, **ref_result, **self.citation_result.model_dump()\n    )\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.researcher","title":"<code>researcher</code>","text":""},{"location":"api/citations/#paperscraper.citations.entity.researcher.Researcher","title":"<code>Researcher</code>","text":"<p>               Bases: <code>Entity</code></p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>class Researcher(Entity):\n    name: str\n    ssid: int\n    orcid: Optional[str] = None\n\n    def __init__(self, input: str, mode: ModeType = \"infer\"):\n        \"\"\"\n        Construct researcher object for self citation/reference analysis.\n\n        Args:\n            input: A researcher to search for.\n            mode: This can be a `name` `orcid` (ORCID iD) or `ssaid` (Semantic Scholar Author ID).\n                Defaults to \"infer\".\n\n        Raises:\n            ValueError: Unknown mode\n        \"\"\"\n        if mode not in MODES:\n            raise ValueError(f\"Unknown mode {mode} chose from {MODES}.\")\n\n        input = input.strip()\n        if mode == \"infer\":\n            if input.isdigit():\n                mode = \"ssaid\"\n            elif (\n                input.count(\"-\") == 3\n                and len(input) == 19\n                and all([x.isdigit() for x in input.split(\"-\")])\n            ):\n                mode = \"orcid\"\n            else:\n                mode = \"author\"\n\n        if mode == \"ssaid\":\n            self.author = sch.get_author(input)\n            self.ssid = input\n        elif mode == \"orcid\":\n            self.author = orcid_to_author_name(input)\n            self.orcid = input\n            self.ssid = author_name_to_ssaid(input)\n        elif mode == \"author\":\n            self.author = input\n            self.ssid = author_name_to_ssaid(input)\n\n        # TODO: Skip over erratum / corrigendum\n        self.ssids = get_papers_for_author(self.ssid)\n\n    def self_references(self):\n        \"\"\"\n        Sifts through all papers of a researcher and extracts the self references.\n        \"\"\"\n        # TODO: Asynchronous call to self_references\n        print(\"Going through SSIDs\", self.ssids)\n\n        # TODO: Aggregate results\n\n    def self_citations(self):\n        \"\"\"\n        Sifts through all papers of a researcher and finds how often they are self-cited.\n        \"\"\"\n        ...\n\n    def get_result(self) -&gt; ResearcherResult:\n        \"\"\"\n        Provides the result of the analysis.\n        \"\"\"\n        ...\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.researcher.Researcher.__init__","title":"<code>__init__(input: str, mode: ModeType = 'infer')</code>","text":"<p>Construct researcher object for self citation/reference analysis.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>A researcher to search for.</p> required <code>mode</code> <code>ModeType</code> <p>This can be a <code>name</code> <code>orcid</code> (ORCID iD) or <code>ssaid</code> (Semantic Scholar Author ID). Defaults to \"infer\".</p> <code>'infer'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Unknown mode</p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>def __init__(self, input: str, mode: ModeType = \"infer\"):\n    \"\"\"\n    Construct researcher object for self citation/reference analysis.\n\n    Args:\n        input: A researcher to search for.\n        mode: This can be a `name` `orcid` (ORCID iD) or `ssaid` (Semantic Scholar Author ID).\n            Defaults to \"infer\".\n\n    Raises:\n        ValueError: Unknown mode\n    \"\"\"\n    if mode not in MODES:\n        raise ValueError(f\"Unknown mode {mode} chose from {MODES}.\")\n\n    input = input.strip()\n    if mode == \"infer\":\n        if input.isdigit():\n            mode = \"ssaid\"\n        elif (\n            input.count(\"-\") == 3\n            and len(input) == 19\n            and all([x.isdigit() for x in input.split(\"-\")])\n        ):\n            mode = \"orcid\"\n        else:\n            mode = \"author\"\n\n    if mode == \"ssaid\":\n        self.author = sch.get_author(input)\n        self.ssid = input\n    elif mode == \"orcid\":\n        self.author = orcid_to_author_name(input)\n        self.orcid = input\n        self.ssid = author_name_to_ssaid(input)\n    elif mode == \"author\":\n        self.author = input\n        self.ssid = author_name_to_ssaid(input)\n\n    # TODO: Skip over erratum / corrigendum\n    self.ssids = get_papers_for_author(self.ssid)\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.researcher.Researcher.self_references","title":"<code>self_references()</code>","text":"<p>Sifts through all papers of a researcher and extracts the self references.</p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>def self_references(self):\n    \"\"\"\n    Sifts through all papers of a researcher and extracts the self references.\n    \"\"\"\n    # TODO: Asynchronous call to self_references\n    print(\"Going through SSIDs\", self.ssids)\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.researcher.Researcher.self_citations","title":"<code>self_citations()</code>","text":"<p>Sifts through all papers of a researcher and finds how often they are self-cited.</p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>def self_citations(self):\n    \"\"\"\n    Sifts through all papers of a researcher and finds how often they are self-cited.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.entity.researcher.Researcher.get_result","title":"<code>get_result() -&gt; ResearcherResult</code>","text":"<p>Provides the result of the analysis.</p> Source code in <code>paperscraper/citations/entity/researcher.py</code> <pre><code>def get_result(self) -&gt; ResearcherResult:\n    \"\"\"\n    Provides the result of the analysis.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.orcid","title":"<code>orcid</code>","text":""},{"location":"api/citations/#paperscraper.citations.orcid.orcid_to_author_name","title":"<code>orcid_to_author_name(orcid_id: str) -&gt; Optional[str]</code>","text":"<p>Given an ORCID ID (as a string, e.g. '0000-0002-1825-0097'), returns the full name of the author from the ORCID public API.</p> Source code in <code>paperscraper/citations/orcid.py</code> <pre><code>def orcid_to_author_name(orcid_id: str) -&gt; Optional[str]:\n    \"\"\"\n    Given an ORCID ID (as a string, e.g. '0000-0002-1825-0097'),\n    returns the full name of the author from the ORCID public API.\n    \"\"\"\n\n    headers = {\"Accept\": \"application/json\"}\n    response = requests.get(f\"{BASE_URL}{orcid_id}/person\", headers=headers)\n    if response.status_code == 200:\n        data = response.json()\n        given = data.get(\"name\", {}).get(\"given-names\", {}).get(\"value\", \"\")\n        family = data.get(\"name\", {}).get(\"family-name\", {}).get(\"value\", \"\")\n        full_name = f\"{given} {family}\".strip()\n        return full_name\n    logger.error(\n        f\"Error fetching ORCID data ({orcid_id}): {response.status_code} {response.text}\"\n    )\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.self_citations","title":"<code>self_citations</code>","text":""},{"location":"api/citations/#paperscraper.citations.self_citations.self_citations_paper","title":"<code>self_citations_paper(inputs: Union[str, List[str]], verbose: bool = False) -&gt; Union[CitationResult, List[CitationResult]]</code>  <code>async</code>","text":"<p>Analyze self-citations for one or more papers by DOI or Semantic Scholar ID.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>A single DOI/SSID string or a list of them.</p> required <code>verbose</code> <code>bool</code> <p>If True, logs detailed information for each paper.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[CitationResult, List[CitationResult]]</code> <p>A single CitationResult if a string was passed, else a list of CitationResults.</p> Source code in <code>paperscraper/citations/self_citations.py</code> <pre><code>@optional_async\n@retry_with_exponential_backoff(max_retries=4, base_delay=1.0)\nasync def self_citations_paper(\n    inputs: Union[str, List[str]], verbose: bool = False\n) -&gt; Union[CitationResult, List[CitationResult]]:\n    \"\"\"\n    Analyze self-citations for one or more papers by DOI or Semantic Scholar ID.\n\n    Args:\n        inputs: A single DOI/SSID string or a list of them.\n        verbose: If True, logs detailed information for each paper.\n\n    Returns:\n        A single CitationResult if a string was passed, else a list of CitationResults.\n    \"\"\"\n    single_input = isinstance(inputs, str)\n    identifiers = [inputs] if single_input else list(inputs)\n\n    async with httpx.AsyncClient(timeout=httpx.Timeout(20)) as client:\n        tasks = [_process_single(client, ident) for ident in identifiers]\n        results = await asyncio.gather(*tasks)\n\n    if verbose:\n        for res in results:\n            logger.info(\n                f'Self-citations in \"{res.ssid}\": N={res.num_citations}, Score={res.citation_score}%'\n            )\n            for author, pct in res.self_citations.items():\n                logger.info(f\"  {author}: {pct}%\")\n\n    return results[0] if single_input else results\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.self_references","title":"<code>self_references</code>","text":""},{"location":"api/citations/#paperscraper.citations.self_references.self_references_paper","title":"<code>self_references_paper(inputs: Union[str, List[str]], verbose: bool = False) -&gt; Union[ReferenceResult, List[ReferenceResult]]</code>  <code>async</code>","text":"<p>Analyze self-references for one or more papers by DOI or Semantic Scholar ID.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>Union[str, List[str]]</code> <p>A single DOI/SSID string or a list of them.</p> required <code>verbose</code> <code>bool</code> <p>If True, logs detailed information for each paper.</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ReferenceResult, List[ReferenceResult]]</code> <p>A single ReferenceResult if a string was passed, else a list of ReferenceResults.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no references are found for a given identifier.</p> Source code in <code>paperscraper/citations/self_references.py</code> <pre><code>@optional_async\n@retry_with_exponential_backoff(max_retries=4, base_delay=1.0)\nasync def self_references_paper(\n    inputs: Union[str, List[str]], verbose: bool = False\n) -&gt; Union[ReferenceResult, List[ReferenceResult]]:\n    \"\"\"\n    Analyze self-references for one or more papers by DOI or Semantic Scholar ID.\n\n    Args:\n        inputs: A single DOI/SSID string or a list of them.\n        verbose: If True, logs detailed information for each paper.\n\n    Returns:\n        A single ReferenceResult if a string was passed, else a list of ReferenceResults.\n\n    Raises:\n        ValueError: If no references are found for a given identifier.\n    \"\"\"\n    single_input = isinstance(inputs, str)\n    identifiers = [inputs] if single_input else list(inputs)\n\n    async with httpx.AsyncClient(timeout=httpx.Timeout(20)) as client:\n        tasks = [_process_single_reference(client, ident) for ident in identifiers]\n        results = await asyncio.gather(*tasks)\n\n    if verbose:\n        for res in results:\n            logger.info(\n                f'Self-references in \"{res.ssid}\": N={res.num_references}, '\n                f\"Score={res.reference_score}%\"\n            )\n            for author, pct in res.self_references.items():\n                logger.info(f\"  {author}: {pct}% self-reference\")\n\n    return results[0] if single_input else results\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.tests","title":"<code>tests</code>","text":""},{"location":"api/citations/#paperscraper.citations.tests.test_self_references","title":"<code>test_self_references</code>","text":""},{"location":"api/citations/#paperscraper.citations.tests.test_self_references.TestSelfReferences","title":"<code>TestSelfReferences</code>","text":"Source code in <code>paperscraper/citations/tests/test_self_references.py</code> <pre><code>class TestSelfReferences:\n    @pytest.fixture\n    def dois(self):\n        return [\n            \"10.1038/s41586-023-06600-9\",\n            \"10.1016/j.neunet.2014.09.003\",\n        ]\n\n    def test_single_doi(self, dois):\n        result = self_references_paper(dois[0])\n        assert isinstance(result, ReferenceResult)\n        assert isinstance(result.num_references, int)\n        assert result.num_references &gt; 0\n        assert isinstance(result.ssid, str)\n        assert isinstance(result.reference_score, float)\n        assert result.reference_score &gt; 0\n        assert isinstance(result.self_references, Dict)\n        for author, self_cites in result.self_references.items():\n            assert isinstance(author, str)\n            assert isinstance(self_cites, float)\n            assert self_cites &gt;= 0 and self_cites &lt;= 100\n\n    def test_multiple_dois(self, dois):\n        results = self_references_paper(dois[1:])\n        assert isinstance(results, list)\n        assert len(results) == len(dois[1:])\n        for ref_result in results:\n            assert isinstance(ref_result, ReferenceResult)\n            assert isinstance(ref_result.ssid, str)\n            assert isinstance(ref_result.num_references, int)\n            assert ref_result.num_references &gt; 0\n            assert ref_result.reference_score &gt; 0\n            assert isinstance(ref_result.reference_score, float)\n            for author, self_cites in ref_result.self_references.items():\n                assert isinstance(author, str)\n                assert isinstance(self_cites, float)\n                assert self_cites &gt;= 0 and self_cites &lt;= 100\n\n    def test_compare_async_and_sync_performance(self, dois):\n        \"\"\"\n        Compares the execution time of asynchronous and synchronous `self_references`\n        for a list of DOIs.\n        \"\"\"\n\n        start_time = time.perf_counter()\n        async_results = self_references_paper(dois)\n        async_duration = time.perf_counter() - start_time\n\n        # Measure synchronous execution time (three independent calls)\n        start_time = time.perf_counter()\n        sync_results = [self_references_paper(doi) for doi in dois]\n\n        sync_duration = time.perf_counter() - start_time\n\n        print(f\"Asynchronous execution time (batch): {async_duration:.2f} seconds\")\n        print(\n            f\"Synchronous execution time (independent calls): {sync_duration:.2f} seconds\"\n        )\n        for a, s in zip(async_results, sync_results):\n            assert a == s, f\"{a} vs {s}\"\n\n        assert 0.5 * async_duration &lt;= sync_duration, (\n            f\"Async execution ({async_duration:.2f}s) is slower than sync execution \"\n            f\"({sync_duration:.2f}s)\"\n        )\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.tests.test_self_references.TestSelfReferences.test_compare_async_and_sync_performance","title":"<code>test_compare_async_and_sync_performance(dois)</code>","text":"<p>Compares the execution time of asynchronous and synchronous <code>self_references</code> for a list of DOIs.</p> Source code in <code>paperscraper/citations/tests/test_self_references.py</code> <pre><code>def test_compare_async_and_sync_performance(self, dois):\n    \"\"\"\n    Compares the execution time of asynchronous and synchronous `self_references`\n    for a list of DOIs.\n    \"\"\"\n\n    start_time = time.perf_counter()\n    async_results = self_references_paper(dois)\n    async_duration = time.perf_counter() - start_time\n\n    # Measure synchronous execution time (three independent calls)\n    start_time = time.perf_counter()\n    sync_results = [self_references_paper(doi) for doi in dois]\n\n    sync_duration = time.perf_counter() - start_time\n\n    print(f\"Asynchronous execution time (batch): {async_duration:.2f} seconds\")\n    print(\n        f\"Synchronous execution time (independent calls): {sync_duration:.2f} seconds\"\n    )\n    for a, s in zip(async_results, sync_results):\n        assert a == s, f\"{a} vs {s}\"\n\n    assert 0.5 * async_duration &lt;= sync_duration, (\n        f\"Async execution ({async_duration:.2f}s) is slower than sync execution \"\n        f\"({sync_duration:.2f}s)\"\n    )\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils","title":"<code>utils</code>","text":""},{"location":"api/citations/#paperscraper.citations.utils.get_doi_from_title","title":"<code>get_doi_from_title(title: str) -&gt; Optional[str]</code>","text":"<p>Searches the DOI of a paper based on the paper title</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Paper title</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>DOI according to semantic scholar API</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def get_doi_from_title(title: str) -&gt; Optional[str]:\n    \"\"\"\n    Searches the DOI of a paper based on the paper title\n\n    Args:\n        title: Paper title\n\n    Returns:\n        DOI according to semantic scholar API\n    \"\"\"\n    response = requests.get(\n        PAPER_URL + \"search\",\n        params={\"query\": title, \"fields\": \"externalIds\", \"limit\": 1},\n    )\n    data = response.json()\n\n    if data.get(\"data\"):\n        paper = data[\"data\"][0]\n        doi = paper.get(\"externalIds\", {}).get(\"DOI\")\n        if doi:\n            return doi\n    logger.warning(f\"Did not find DOI for title={title}\")\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils.get_doi_from_ssid","title":"<code>get_doi_from_ssid(ssid: str, max_retries: int = 10) -&gt; Optional[str]</code>","text":"<p>Given a Semantic Scholar paper ID, returns the corresponding DOI if available.</p> <p>Parameters:</p> Name Type Description Default <code>ssid</code> <code>str</code> <p>The paper ID on Semantic Scholar.</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>str or None: The DOI of the paper, or None if not found or in case of an error.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def get_doi_from_ssid(ssid: str, max_retries: int = 10) -&gt; Optional[str]:\n    \"\"\"\n    Given a Semantic Scholar paper ID, returns the corresponding DOI if available.\n\n    Parameters:\n      ssid (str): The paper ID on Semantic Scholar.\n\n    Returns:\n      str or None: The DOI of the paper, or None if not found or in case of an error.\n    \"\"\"\n    logger.warning(\n        \"Semantic Scholar API is easily overloaded when passing SS IDs, provide DOIs to improve throughput.\"\n    )\n    attempts = 0\n    for attempt in tqdm(\n        range(1, max_retries + 1), desc=f\"Fetching DOI for {ssid}\", unit=\"attempt\"\n    ):\n        # Make the GET request to Semantic Scholar.\n        response = requests.get(\n            f\"{PAPER_URL}{ssid}\", params={\"fields\": \"externalIds\", \"limit\": 1}\n        )\n\n        # If successful, try to extract and return the DOI.\n        if response.status_code == 200:\n            data = response.json()\n            doi = data.get(\"externalIds\", {}).get(\"DOI\")\n            return doi\n        attempts += 1\n        sleep(10)\n    logger.warning(\n        f\"Did not find DOI for paper ID {ssid}. Code={response.status_code}, text={response.text}\"\n    )\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils.get_title_and_id_from_doi","title":"<code>get_title_and_id_from_doi(doi: str) -&gt; Dict[str, Any]</code>","text":"<p>Given a DOI, retrieves the paper's title and semantic scholar paper ID.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI of the paper (e.g., \"10.18653/v1/N18-3011\").</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>dict or None: A dictionary with keys 'title' and 'ssid'.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def get_title_and_id_from_doi(doi: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Given a DOI, retrieves the paper's title and semantic scholar paper ID.\n\n    Parameters:\n        doi (str): The DOI of the paper (e.g., \"10.18653/v1/N18-3011\").\n\n    Returns:\n        dict or None: A dictionary with keys 'title' and 'ssid'.\n    \"\"\"\n\n    # Send the GET request to Semantic Scholar\n    response = requests.get(f\"{PAPER_URL}DOI:{doi}\")\n    if response.status_code == 200:\n        data = response.json()\n        return {\"title\": data.get(\"title\"), \"ssid\": data.get(\"paperId\")}\n    logger.warning(\n        f\"Could not get authors &amp; semantic scholar ID for DOI={doi}, {response.status_code}: {response.text}\"\n    )\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils.author_name_to_ssaid","title":"<code>author_name_to_ssaid(author_name: str) -&gt; str</code>","text":"<p>Given an author name, returns the Semantic Scholar author ID.</p> <p>Parameters:</p> Name Type Description Default <code>author_name</code> <code>str</code> <p>The full name of the author.</p> required <p>Returns:</p> Type Description <code>str</code> <p>str or None: The Semantic Scholar author ID or None if no author is found.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def author_name_to_ssaid(author_name: str) -&gt; str:\n    \"\"\"\n    Given an author name, returns the Semantic Scholar author ID.\n\n    Parameters:\n        author_name (str): The full name of the author.\n\n    Returns:\n        str or None: The Semantic Scholar author ID or None if no author is found.\n    \"\"\"\n\n    response = requests.get(\n        AUTHOR_URL, params={\"query\": author_name, \"fields\": \"name\", \"limit\": 1}\n    )\n    if response.status_code == 200:\n        data = response.json()\n        authors = data.get(\"data\", [])\n        if authors:\n            # Return the Semantic Scholar author ID from the first result.\n            return authors[0].get(\"authorId\")\n\n    logger.error(\n        f\"Error in retrieving name from SS Author ID: {response.status_code} - {response.text}\"\n    )\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils.determine_paper_input_type","title":"<code>determine_paper_input_type(input: str) -&gt; Literal['ssid', 'doi', 'title']</code>","text":"<p>Determines the intended input type by the user if not explicitly given (<code>infer</code>).</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>Either a DOI or a semantic scholar paper ID or an author name.</p> required <p>Returns:</p> Type Description <code>Literal['ssid', 'doi', 'title']</code> <p>The input type</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def determine_paper_input_type(input: str) -&gt; Literal[\"ssid\", \"doi\", \"title\"]:\n    \"\"\"\n    Determines the intended input type by the user if not explicitly given (`infer`).\n\n    Args:\n        input: Either a DOI or a semantic scholar paper ID or an author name.\n\n    Returns:\n        The input type\n    \"\"\"\n    if len(input) &gt; 15 and \" \" not in input and (input.isalnum() and input.islower()):\n        mode = \"ssid\"\n    elif len(re.findall(DOI_PATTERN, input, re.IGNORECASE)) == 1:\n        mode = \"doi\"\n    else:\n        logger.info(\n            f\"Assuming `{input}` is a paper title, since it seems neither a DOI nor a paper ID\"\n        )\n        mode = \"title\"\n    return mode\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils.get_papers_for_author","title":"<code>get_papers_for_author(ss_author_id: str) -&gt; List[str]</code>  <code>async</code>","text":"<p>Given a Semantic Scholar author ID, returns a list of all Semantic Scholar paper IDs for that author.</p> <p>Parameters:</p> Name Type Description Default <code>ss_author_id</code> <code>str</code> <p>The Semantic Scholar author ID (e.g., \"1741101\").</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of paper IDs (as strings) authored by the given author.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>async def get_papers_for_author(ss_author_id: str) -&gt; List[str]:\n    \"\"\"\n    Given a Semantic Scholar author ID, returns a list of all Semantic Scholar paper IDs for that author.\n\n    Args:\n        ss_author_id (str): The Semantic Scholar author ID (e.g., \"1741101\").\n\n    Returns:\n        A list of paper IDs (as strings) authored by the given author.\n    \"\"\"\n    papers = []\n    offset = 0\n    limit = 100\n\n    async with httpx.AsyncClient() as client:\n        while True:\n            response = await client.get(\n                f\"https://api.semanticscholar.org/graph/v1/author/{ss_author_id}/papers\",\n                params={\"fields\": \"paperId\", \"offset\": offset, \"limit\": limit},\n            )\n            response.raise_for_status()\n            data = response.json()\n            page = data.get(\"data\", [])\n\n            # Extract paper IDs from the current page.\n            for paper in page:\n                if \"paperId\" in paper:\n                    papers.append(paper[\"paperId\"])\n\n            # If fewer papers were returned than the limit, we've reached the end.\n            if len(page) &lt; limit:\n                break\n\n            offset += limit\n\n    return papers\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils.find_matching","title":"<code>find_matching(first: List[Dict[str, str]], second: List[Dict[str, str]]) -&gt; List[str]</code>","text":"<p>Ingests two sets of authors and returns a list of those that match (either based on name     or on author ID).</p> <p>Parameters:</p> Name Type Description Default <code>first</code> <code>List[Dict[str, str]]</code> <p>First set of authors given as list of dict with two keys (<code>authorID</code> and <code>name</code>).</p> required <code>second</code> <code>List[Dict[str, str]]</code> <p>Second set of authors given as list of dict with two same keys.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of names of authors in first list where a match was found.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def find_matching(\n    first: List[Dict[str, str]], second: List[Dict[str, str]]\n) -&gt; List[str]:\n    \"\"\"\n    Ingests two sets of authors and returns a list of those that match (either based on name\n        or on author ID).\n\n    Args:\n        first: First set of authors given as list of dict with two keys (`authorID` and `name`).\n        second: Second set of authors given as list of dict with two same keys.\n\n    Returns:\n        List of names of authors in first list where a match was found.\n    \"\"\"\n    # Check which author IDs overlap\n    second_names = set(map(lambda x: x[\"authorId\"], second))\n    overlap_ids = {f[\"name\"] for f in first if f[\"authorId\"] in second_names}\n\n    overlap_names = {\n        f[\"name\"]\n        for f in first\n        if f[\"authorId\"] not in overlap_ids\n        and any([check_overlap(f[\"name\"], s[\"name\"]) for s in second])\n    }\n    return list(overlap_ids | overlap_names)\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils.check_overlap","title":"<code>check_overlap(n1: str, n2: str) -&gt; bool</code>","text":"<p>Check whether two author names are identical. TODO: This can be made more robust</p> <p>Parameters:</p> Name Type Description Default <code>n1</code> <code>str</code> <p>first name</p> required <code>n2</code> <code>str</code> <p>second name</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether names are identical.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def check_overlap(n1: str, n2: str) -&gt; bool:\n    \"\"\"\n    Check whether two author names are identical.\n    TODO: This can be made more robust\n\n    Args:\n        n1: first name\n        n2: second name\n\n    Returns:\n        bool: Whether names are identical.\n    \"\"\"\n    # remove initials and check for name intersection\n    s1 = {w for w in clean_name(n1).split()}\n    s2 = {w for w in clean_name(n2).split()}\n    return len(s2) &gt; 0 and len(s1 | s2) == len(s1)\n</code></pre>"},{"location":"api/citations/#paperscraper.citations.utils.clean_name","title":"<code>clean_name(s: str) -&gt; str</code>","text":"<p>Clean up a str by removing special characters.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>Input possibly containing special symbols</p> required <p>Returns:</p> Type Description <code>str</code> <p>Homogenized string.</p> Source code in <code>paperscraper/citations/utils.py</code> <pre><code>def clean_name(s: str) -&gt; str:\n    \"\"\"\n    Clean up a str by removing special characters.\n\n    Args:\n        s: Input possibly containing special symbols\n\n    Returns:\n        Homogenized string.\n    \"\"\"\n    return \"\".join(ch for ch in unidecode(s) if ch.isalpha() or ch.isspace()).lower()\n</code></pre>"},{"location":"api/get_dumps/","title":"paperscraper.get_dumps","text":""},{"location":"api/get_dumps/#paperscraper.get_dumps","title":"<code>paperscraper.get_dumps</code>","text":""},{"location":"api/get_dumps/#paperscraper.get_dumps.arxiv","title":"<code>arxiv</code>","text":"<p>Dump arxiv data in JSONL format.</p>"},{"location":"api/get_dumps/#paperscraper.get_dumps.arxiv.arxiv","title":"<code>arxiv(start_date: Optional[str] = None, end_date: Optional[str] = None, save_path: str = save_path)</code>","text":"<p>Fetches papers from arXiv based on time range, i.e., start_date and end_date. If the start_date and end_date are not provided, fetches papers from the earliest possible date to the current date. The fetched papers are stored in JSONL format.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>Start date in format YYYY-MM-DD. Defaults to None.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>End date in format YYYY-MM-DD. Defaults to None.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path to save the JSONL dump. Defaults to save_path.</p> <code>save_path</code> Source code in <code>paperscraper/get_dumps/arxiv.py</code> <pre><code>def arxiv(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    save_path: str = save_path,\n):\n    \"\"\"\n    Fetches papers from arXiv based on time range, i.e., start_date and end_date.\n    If the start_date and end_date are not provided, fetches papers from the earliest\n    possible date to the current date. The fetched papers are stored in JSONL format.\n\n    Args:\n        start_date (str, optional): Start date in format YYYY-MM-DD. Defaults to None.\n        end_date (str, optional): End date in format YYYY-MM-DD. Defaults to None.\n        save_path (str, optional): Path to save the JSONL dump. Defaults to save_path.\n    \"\"\"\n    # Set default dates\n    EARLIEST_START = \"1991-01-01\"\n    if start_date is None:\n        start_date = EARLIEST_START\n    if end_date is None:\n        end_date = datetime.today().strftime(\"%Y-%m-%d\")\n\n    # Convert dates to datetime objects\n    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n    if start_date &gt; end_date:\n        raise ValueError(\n            f\"start_date {start_date} cannot be later than end_date {end_date}\"\n        )\n\n    # Open file for writing results\n    with open(save_path, \"w\") as fp:\n        progress_bar = tqdm(total=(end_date - start_date).days + 1)\n\n        current_date = start_date\n        while current_date &lt;= end_date:\n            next_date = current_date + timedelta(days=1)\n            progress_bar.set_description(\n                f\"Fetching {current_date.strftime('%Y-%m-%d')}\"\n            )\n\n            # Format dates for query\n            query = f\"submittedDate:[{current_date.strftime('%Y%m%d0000')} TO {next_date.strftime('%Y%m%d0000')}]\"\n            try:\n                papers = get_arxiv_papers_api(\n                    query=query,\n                    fields=[\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n                    verbose=False,\n                )\n                if not papers.empty:\n                    for paper in papers.to_dict(orient=\"records\"):\n                        fp.write(json.dumps(paper) + \"\\n\")\n            except Exception as e:\n                print(f\"Arxiv scraping error: {current_date.strftime('%Y-%m-%d')}: {e}\")\n            current_date = next_date\n            progress_bar.update(1)\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.biorxiv","title":"<code>biorxiv</code>","text":"<p>Dump bioRxiv data in JSONL format.</p>"},{"location":"api/get_dumps/#paperscraper.get_dumps.biorxiv.biorxiv","title":"<code>biorxiv(start_date: Optional[str] = None, end_date: Optional[str] = None, save_path: str = save_path, max_retries: int = 10)</code>","text":"<p>Fetches papers from biorxiv based on time range, i.e., start_date and end_date. If the start_date and end_date are not provided, papers will be fetched from biorxiv from the launch date of biorxiv until the current date. The fetched papers will be stored in jsonl format in save_path.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>begin date expressed as YYYY-MM-DD. Defaults to None, i.e., earliest possible.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>end date expressed as YYYY-MM-DD. Defaults to None, i.e., today.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path where the dump is stored. Defaults to save_path.</p> <code>save_path</code> <code>max_retries</code> <code>int</code> <p>Number of retries when API shows connection issues. Defaults to 10.</p> <code>10</code> Source code in <code>paperscraper/get_dumps/biorxiv.py</code> <pre><code>def biorxiv(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    save_path: str = save_path,\n    max_retries: int = 10,\n):\n    \"\"\"Fetches papers from biorxiv based on time range, i.e., start_date and end_date.\n    If the start_date and end_date are not provided, papers will be fetched from biorxiv\n    from the launch date of biorxiv until the current date. The fetched papers will be\n    stored in jsonl format in save_path.\n\n    Args:\n        start_date (str, optional): begin date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., earliest possible.\n        end_date (str, optional): end date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., today.\n        save_path (str, optional): Path where the dump is stored.\n            Defaults to save_path.\n        max_retries (int, optional): Number of retries when API shows connection issues.\n            Defaults to 10.\n    \"\"\"\n    # create API client\n    api = BioRxivApi(max_retries=max_retries)\n\n    # dump all papers\n    with open(save_path, \"w\") as fp:\n        for index, paper in enumerate(\n            tqdm(api.get_papers(start_date=start_date, end_date=end_date))\n        ):\n            if index &gt; 0:\n                fp.write(os.linesep)\n            fp.write(json.dumps(paper))\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.chemrxiv","title":"<code>chemrxiv</code>","text":"<p>Dump chemRxiv data in JSONL format.</p>"},{"location":"api/get_dumps/#paperscraper.get_dumps.chemrxiv.chemrxiv","title":"<code>chemrxiv(start_date: Optional[str] = None, end_date: Optional[str] = None, save_path: str = save_path) -&gt; None</code>","text":"<p>Fetches papers from bichemrxiv based on time range, i.e., start_date and end_date. If the start_date and end_date are not provided, papers will be fetched from chemrxiv from the launch date of chemrxiv until the current date. The fetched papers will be stored in jsonl format in save_path.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>begin date expressed as YYYY-MM-DD. Defaults to None, i.e., earliest possible.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>end date expressed as YYYY-MM-DD. Defaults to None, i.e., today.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path where the dump is stored. Defaults to save_path.</p> <code>save_path</code> Source code in <code>paperscraper/get_dumps/chemrxiv.py</code> <pre><code>def chemrxiv(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    save_path: str = save_path,\n) -&gt; None:\n    \"\"\"Fetches papers from bichemrxiv based on time range, i.e., start_date and end_date.\n    If the start_date and end_date are not provided, papers will be fetched from chemrxiv\n    from the launch date of chemrxiv until the current date. The fetched papers will be\n    stored in jsonl format in save_path.\n\n    Args:\n        start_date (str, optional): begin date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., earliest possible.\n        end_date (str, optional): end date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., today.\n        save_path (str, optional): Path where the dump is stored.\n            Defaults to save_path.\n    \"\"\"\n\n    # create API client\n    api = ChemrxivAPI(start_date, end_date)\n    # Download the data\n    download_full(save_folder, api)\n    # Convert to JSONL format.\n    parse_dump(save_folder, save_path)\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.medrxiv","title":"<code>medrxiv</code>","text":"<p>Dump medrxiv data in JSONL format.</p>"},{"location":"api/get_dumps/#paperscraper.get_dumps.medrxiv.medrxiv","title":"<code>medrxiv(start_date: Optional[str] = None, end_date: Optional[str] = None, save_path: str = save_path, max_retries: int = 10)</code>","text":"<p>Fetches papers from medrxiv based on time range, i.e., start_date and end_date. If the start_date and end_date are not provided, then papers will be fetched from medrxiv starting from the launch date of medrxiv until current date. The fetched papers will be stored in jsonl format in save_path.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>str</code> <p>begin date expressed as YYYY-MM-DD. Defaults to None, i.e., earliest possible.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>end date expressed as YYYY-MM-DD. Defaults to None, i.e., today.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path where the dump is stored. Defaults to save_path.</p> <code>save_path</code> <code>max_retries</code> <code>int</code> <p>Number of retries when API shows connection issues. Defaults to 10.</p> <code>10</code> Source code in <code>paperscraper/get_dumps/medrxiv.py</code> <pre><code>def medrxiv(\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    save_path: str = save_path,\n    max_retries: int = 10,\n):\n    \"\"\"Fetches papers from medrxiv based on time range, i.e., start_date and end_date.\n    If the start_date and end_date are not provided, then papers will be fetched from\n    medrxiv starting from the launch date of medrxiv until current date. The fetched\n    papers will be stored in jsonl format in save_path.\n\n    Args:\n        start_date (str, optional): begin date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., earliest possible.\n        end_date (str, optional): end date expressed as YYYY-MM-DD.\n            Defaults to None, i.e., today.\n        save_path (str, optional): Path where the dump is stored.\n            Defaults to save_path.\n        max_retries (int, optional): Number of retries when API shows connection issues.\n            Defaults to 10.\n    \"\"\"\n    # create API client\n    api = MedRxivApi(max_retries=max_retries)\n    # dump all papers\n    with open(save_path, \"w\") as fp:\n        for index, paper in enumerate(\n            tqdm(api.get_papers(start_date=start_date, end_date=end_date))\n        ):\n            if index &gt; 0:\n                fp.write(os.linesep)\n            fp.write(json.dumps(paper))\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils","title":"<code>utils</code>","text":""},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv","title":"<code>chemrxiv</code>","text":""},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.get_author","title":"<code>get_author(author_list: List[Dict]) -&gt; str</code>","text":"<p>Parse ChemRxiv dump entry to extract author list</p> <p>Parameters:</p> Name Type Description Default <code>author_list</code> <code>list</code> <p>List of dicts, one per author.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>;-concatenated author list.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_author(author_list: List[Dict]) -&gt; str:\n    \"\"\"Parse ChemRxiv dump entry to extract author list\n\n    Args:\n        author_list (list): List of dicts, one per author.\n\n    Returns:\n        str: ;-concatenated author list.\n    \"\"\"\n\n    return \"; \".join([\" \".join([a[\"firstName\"], a[\"lastName\"]]) for a in author_list])\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.get_categories","title":"<code>get_categories(category_list: List[Dict]) -&gt; str</code>","text":"<p>Parse ChemRxiv dump entry to extract the categories of the paper</p> <p>Parameters:</p> Name Type Description Default <code>category_list</code> <code>list</code> <p>List of dicts, one per category.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>;-concatenated category list.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_categories(category_list: List[Dict]) -&gt; str:\n    \"\"\"Parse ChemRxiv dump entry to extract the categories of the paper\n\n    Args:\n        category_list (list): List of dicts, one per category.\n\n    Returns:\n        str: ;-concatenated category list.\n    \"\"\"\n\n    return \"; \".join([a[\"name\"] for a in category_list])\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.get_date","title":"<code>get_date(datestring: str) -&gt; str</code>","text":"<p>Get the date of a chemrxiv dump enry.</p> <p>Parameters:</p> Name Type Description Default <code>datestring</code> <code>str</code> <p>String in the format: 2021-10-15T05:12:32.356Z</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Date in the format: YYYY-MM-DD.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_date(datestring: str) -&gt; str:\n    \"\"\"Get the date of a chemrxiv dump enry.\n\n    Args:\n        datestring: String in the format: 2021-10-15T05:12:32.356Z\n\n    Returns:\n        str: Date in the format: YYYY-MM-DD.\n    \"\"\"\n    return datestring.split(\"T\")[0]\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.get_metrics","title":"<code>get_metrics(metrics_list: List[Dict]) -&gt; Dict</code>","text":"<p>Parse ChemRxiv dump entry to extract the access metrics of the paper.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_list</code> <code>List[Dict]</code> <p>A list of single-keyed, dictionaries each containing key and value for exactly one metric.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A flattened dictionary with all metrics and a timestamp</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_metrics(metrics_list: List[Dict]) -&gt; Dict:\n    \"\"\"\n    Parse ChemRxiv dump entry to extract the access metrics of the paper.\n\n    Args:\n        metrics_list (List[Dict]): A list of single-keyed, dictionaries each\n            containing key and value for exactly one metric.\n\n    Returns:\n        Dict: A flattened dictionary with all metrics and a timestamp\n    \"\"\"\n    metric_dict = {m[\"description\"]: m[\"value\"] for m in metrics_list}\n\n    # This assumes that the .jsonl is constructed at roughly the same date\n    # where this entry was obtained from the API\n    metric_dict.update({\"timestamp\": today})\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.parse_dump","title":"<code>parse_dump(source_path: str, target_path: str) -&gt; None</code>","text":"<p>Parses the dump as generated by the chemrXiv API and this repo: https://github.com/cthoyt/chemrxiv-summarize into a format that is equal to that of biorXiv and medRxiv.</p> <p>NOTE: This is a lazy parser trying to store all data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>Path to the source dump</p> required Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def parse_dump(source_path: str, target_path: str) -&gt; None:\n    \"\"\"\n    Parses the dump as generated by the chemrXiv API and this repo:\n    https://github.com/cthoyt/chemrxiv-summarize\n    into a format that is equal to that of biorXiv and medRxiv.\n\n    NOTE: This is a lazy parser trying to store all data in memory.\n\n    Args:\n        source_path: Path to the source dump\n    \"\"\"\n\n    dump = []\n    # Read source dump\n    for file_name in tqdm(os.listdir(source_path)):\n        if not file_name.endswith(\".json\"):\n            continue\n        filepath = os.path.join(source_path, file_name)\n        with open(filepath, \"r\") as f:\n            source_paper = json.load(f)\n\n        target_paper = {\n            \"title\": source_paper[\"title\"],\n            \"doi\": source_paper[\"doi\"],\n            \"published_doi\": (\n                source_paper[\"vor\"][\"vorDoi\"] if source_paper[\"vor\"] else \"N.A.\"\n            ),\n            \"published_url\": (\n                source_paper[\"vor\"][\"url\"] if source_paper[\"vor\"] else \"N.A.\"\n            ),\n            \"authors\": get_author(source_paper[\"authors\"]),\n            \"abstract\": source_paper[\"abstract\"],\n            \"date\": get_date(source_paper[\"statusDate\"]),\n            \"journal\": \"chemRxiv\",\n            \"categories\": get_categories(source_paper[\"categories\"]),\n            \"metrics\": get_metrics(source_paper[\"metrics\"]),\n            \"license\": source_paper[\"license\"][\"name\"],\n        }\n        dump.append(target_paper)\n        os.remove(filepath)\n    # Write dump\n    with open(target_path, \"w\") as f:\n        for idx, target_paper in enumerate(dump):\n            if idx &gt; 0:\n                f.write(os.linesep)\n            f.write(json.dumps(target_paper))\n    logger.info(\"Done, shutting down\")\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.chemrxiv_api","title":"<code>chemrxiv_api</code>","text":""},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.chemrxiv_api.ChemrxivAPI","title":"<code>ChemrxivAPI</code>","text":"<p>Handle OpenEngage API requests, using access. Adapted from https://github.com/fxcoudert/tools/blob/master/chemRxiv/chemRxiv.py.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>class ChemrxivAPI:\n    \"\"\"Handle OpenEngage API requests, using access.\n    Adapted from https://github.com/fxcoudert/tools/blob/master/chemRxiv/chemRxiv.py.\n    \"\"\"\n\n    base = \"https://chemrxiv.org/engage/chemrxiv/public-api/v1/\"\n\n    def __init__(\n        self,\n        start_date: Optional[str] = None,\n        end_date: Optional[str] = None,\n        page_size: Optional[int] = None,\n        max_retries: int = 10,\n    ):\n        \"\"\"\n        Initialize API class.\n\n        Args:\n            start_date (Optional[str], optional): begin date expressed as YYYY-MM-DD.\n                Defaults to None.\n            end_date (Optional[str], optional): end date expressed as YYYY-MM-DD.\n                Defaults to None.\n            page_size (int, optional): The batch size used to fetch the records from chemrxiv.\n            max_retries (int): Number of retries in case of error\n        \"\"\"\n\n        self.page_size = page_size or 50\n        self.max_retries = max_retries\n\n        # Begin Date and End Date of the search\n        launch_date = launch_dates[\"chemrxiv\"]\n        launch_datetime = datetime.fromisoformat(launch_date)\n\n        if start_date:\n            start_datetime = datetime.fromisoformat(start_date)\n            if start_datetime &lt; launch_datetime:\n                self.start_date = launch_date\n                logger.warning(\n                    f\"Begin date {start_date} is before chemrxiv launch date. Will use {launch_date} instead.\"\n                )\n            else:\n                self.start_date = start_date\n        else:\n            self.start_date = launch_date\n        if end_date:\n            end_datetime = datetime.fromisoformat(end_date)\n            if end_datetime &gt; now_datetime:\n                logger.warning(\n                    f\"End date {end_date} is in the future. Will use {now_datetime} instead.\"\n                )\n                self.end_date = now_datetime.strftime(\"%Y-%m-%d\")\n            else:\n                self.end_date = end_date\n        else:\n            self.end_date = now_datetime.strftime(\"%Y-%m-%d\")\n\n    def request(self, url, method, params=None, parse_json: bool = False):\n        \"\"\"Send an API request to open Engage.\"\"\"\n\n        headers = {\"Accept-Encoding\": \"identity\", \"Accept\": \"application/json\"}\n        retryable = (\n            ChunkedEncodingError,\n            ContentDecodingError,\n            DecodeError,\n            ReadTimeout,\n            ConnectionError,\n        )\n        transient_status = {429, 500, 502, 503, 504}\n        backoff = 0.1\n\n        for attempt in range(self.max_retries):\n            try:\n                if method.casefold() == \"get\":\n                    response = requests.get(\n                        url, params=params, headers=headers, timeout=(5, 30)\n                    )\n                elif method.casefold() == \"post\":\n                    response = requests.post(\n                        url, json=params, headers=headers, timeout=(5, 30)\n                    )\n                else:\n                    raise ConnectionError(f\"Unknown method for query: {method}\")\n                if response.status_code in transient_status:\n                    logger.warning(\n                        f\"{response.status_code} for {url} (attempt {attempt + 1}/{self.max_retries}); retrying in {backoff:.1f}s\"\n                    )\n                    if attempt + 1 == self.max_retries:\n                        response.raise_for_status()\n                    sleep(backoff)\n                    backoff = min(60.0, backoff * 2)\n                    continue\n                elif 400 &lt;= response.status_code &lt; 500:\n                    response.raise_for_status()\n                if not parse_json:\n                    return response\n\n                try:\n                    return response.json()\n                except JSONDecodeError:\n                    logger.warning(\n                        f\"JSONDecodeError for {response.url} \"\n                        f\"(attempt {attempt + 1}/{self.max_retries}); retrying in {backoff:.1f}s\"\n                    )\n                    if attempt + 1 == self.max_retries:\n                        raise\n                    sleep(backoff)\n                    backoff = min(60.0, backoff * 2)\n                    continue\n\n            except retryable as e:\n                logger.warning(\n                    f\"{e.__class__.__name__} for {url} (attempt {attempt + 1}/{self.max_retries}); \"\n                    f\"retrying in {backoff:.1f}s\"\n                )\n                if attempt + 1 == self.max_retries:\n                    raise\n                sleep(backoff)\n                backoff = min(60.0, backoff * 2)\n\n    def query(self, query, method=\"get\", params=None):\n        \"\"\"Perform a direct query.\"\"\"\n\n        return self.request(\n            urljoin(self.base, query), method, params=params, parse_json=True\n        )\n\n    def query_generator(\n        self, query, method: str = \"get\", params: Optional[Dict] = None\n    ):\n        \"\"\"Query for a list of items, with paging. Returns a generator.\"\"\"\n\n        start_datetime = datetime.fromisoformat(self.start_date)\n        end_datetime = datetime.fromisoformat(self.end_date)\n\n        def year_windows():\n            year = start_datetime.year\n            while year &lt;= end_datetime.year:\n                year_start = datetime(year, 1, 1)\n                year_end = datetime(year, 12, 31)\n                win_start = max(start_datetime, year_start)\n                win_end = min(end_datetime, year_end)\n                yield win_start.strftime(\"%Y-%m-%d\"), win_end.strftime(\"%Y-%m-%d\")\n                year += 1\n\n        params = (params or {}).copy()\n\n        for year_from, year_to in year_windows():\n            logger.info(f\"Starting to scrape data from {year_from} to {year_to}\")\n            page = 0\n            while True:\n                params.update(\n                    {\n                        \"limit\": self.page_size,\n                        \"skip\": page * self.page_size,\n                        \"searchDateFrom\": year_from,\n                        \"searchDateTo\": year_to,\n                    }\n                )\n                try:\n                    data = self.request(\n                        urljoin(self.base, query),\n                        method,\n                        params=params,\n                        parse_json=True,\n                    )\n                except requests.HTTPError as e:\n                    status = getattr(e.response, \"status_code\", None)\n                    logger.warning(\n                        f\"Stopping year window {year_from}..{year_to} at skip={page * self.page_size} \"\n                        f\"due to HTTPError {status}\"\n                    )\n                    break\n                items = data.get(\"itemHits\", [])\n                if not items:\n                    break\n                for item in items:\n                    yield item\n                page += 1\n\n    def all_preprints(self):\n        \"\"\"Return a generator to all the chemRxiv articles.\"\"\"\n        return self.query_generator(\"items\")\n\n    def preprint(self, article_id):\n        \"\"\"Information on a given preprint.\n        .. seealso:: https://docs.figshare.com/#public_article\n        \"\"\"\n        return self.query(os.path.join(\"items\", article_id))\n\n    def number_of_preprints(self):\n        return self.query(\"items\")[\"totalCount\"]\n</code></pre> <code></code> <code>__init__(start_date: Optional[str] = None, end_date: Optional[str] = None, page_size: Optional[int] = None, max_retries: int = 10)</code> \u00b6 <p>Initialize API class.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Optional[str]</code> <p>begin date expressed as YYYY-MM-DD. Defaults to None.</p> <code>None</code> <code>end_date</code> <code>Optional[str]</code> <p>end date expressed as YYYY-MM-DD. Defaults to None.</p> <code>None</code> <code>page_size</code> <code>int</code> <p>The batch size used to fetch the records from chemrxiv.</p> <code>None</code> <code>max_retries</code> <code>int</code> <p>Number of retries in case of error</p> <code>10</code> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def __init__(\n    self,\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    page_size: Optional[int] = None,\n    max_retries: int = 10,\n):\n    \"\"\"\n    Initialize API class.\n\n    Args:\n        start_date (Optional[str], optional): begin date expressed as YYYY-MM-DD.\n            Defaults to None.\n        end_date (Optional[str], optional): end date expressed as YYYY-MM-DD.\n            Defaults to None.\n        page_size (int, optional): The batch size used to fetch the records from chemrxiv.\n        max_retries (int): Number of retries in case of error\n    \"\"\"\n\n    self.page_size = page_size or 50\n    self.max_retries = max_retries\n\n    # Begin Date and End Date of the search\n    launch_date = launch_dates[\"chemrxiv\"]\n    launch_datetime = datetime.fromisoformat(launch_date)\n\n    if start_date:\n        start_datetime = datetime.fromisoformat(start_date)\n        if start_datetime &lt; launch_datetime:\n            self.start_date = launch_date\n            logger.warning(\n                f\"Begin date {start_date} is before chemrxiv launch date. Will use {launch_date} instead.\"\n            )\n        else:\n            self.start_date = start_date\n    else:\n        self.start_date = launch_date\n    if end_date:\n        end_datetime = datetime.fromisoformat(end_date)\n        if end_datetime &gt; now_datetime:\n            logger.warning(\n                f\"End date {end_date} is in the future. Will use {now_datetime} instead.\"\n            )\n            self.end_date = now_datetime.strftime(\"%Y-%m-%d\")\n        else:\n            self.end_date = end_date\n    else:\n        self.end_date = now_datetime.strftime(\"%Y-%m-%d\")\n</code></pre> <code></code> <code>request(url, method, params=None, parse_json: bool = False)</code> \u00b6 <p>Send an API request to open Engage.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def request(self, url, method, params=None, parse_json: bool = False):\n    \"\"\"Send an API request to open Engage.\"\"\"\n\n    headers = {\"Accept-Encoding\": \"identity\", \"Accept\": \"application/json\"}\n    retryable = (\n        ChunkedEncodingError,\n        ContentDecodingError,\n        DecodeError,\n        ReadTimeout,\n        ConnectionError,\n    )\n    transient_status = {429, 500, 502, 503, 504}\n    backoff = 0.1\n\n    for attempt in range(self.max_retries):\n        try:\n            if method.casefold() == \"get\":\n                response = requests.get(\n                    url, params=params, headers=headers, timeout=(5, 30)\n                )\n            elif method.casefold() == \"post\":\n                response = requests.post(\n                    url, json=params, headers=headers, timeout=(5, 30)\n                )\n            else:\n                raise ConnectionError(f\"Unknown method for query: {method}\")\n            if response.status_code in transient_status:\n                logger.warning(\n                    f\"{response.status_code} for {url} (attempt {attempt + 1}/{self.max_retries}); retrying in {backoff:.1f}s\"\n                )\n                if attempt + 1 == self.max_retries:\n                    response.raise_for_status()\n                sleep(backoff)\n                backoff = min(60.0, backoff * 2)\n                continue\n            elif 400 &lt;= response.status_code &lt; 500:\n                response.raise_for_status()\n            if not parse_json:\n                return response\n\n            try:\n                return response.json()\n            except JSONDecodeError:\n                logger.warning(\n                    f\"JSONDecodeError for {response.url} \"\n                    f\"(attempt {attempt + 1}/{self.max_retries}); retrying in {backoff:.1f}s\"\n                )\n                if attempt + 1 == self.max_retries:\n                    raise\n                sleep(backoff)\n                backoff = min(60.0, backoff * 2)\n                continue\n\n        except retryable as e:\n            logger.warning(\n                f\"{e.__class__.__name__} for {url} (attempt {attempt + 1}/{self.max_retries}); \"\n                f\"retrying in {backoff:.1f}s\"\n            )\n            if attempt + 1 == self.max_retries:\n                raise\n            sleep(backoff)\n            backoff = min(60.0, backoff * 2)\n</code></pre> <code></code> <code>query(query, method='get', params=None)</code> \u00b6 <p>Perform a direct query.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def query(self, query, method=\"get\", params=None):\n    \"\"\"Perform a direct query.\"\"\"\n\n    return self.request(\n        urljoin(self.base, query), method, params=params, parse_json=True\n    )\n</code></pre> <code></code> <code>query_generator(query, method: str = 'get', params: Optional[Dict] = None)</code> \u00b6 <p>Query for a list of items, with paging. Returns a generator.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def query_generator(\n    self, query, method: str = \"get\", params: Optional[Dict] = None\n):\n    \"\"\"Query for a list of items, with paging. Returns a generator.\"\"\"\n\n    start_datetime = datetime.fromisoformat(self.start_date)\n    end_datetime = datetime.fromisoformat(self.end_date)\n\n    def year_windows():\n        year = start_datetime.year\n        while year &lt;= end_datetime.year:\n            year_start = datetime(year, 1, 1)\n            year_end = datetime(year, 12, 31)\n            win_start = max(start_datetime, year_start)\n            win_end = min(end_datetime, year_end)\n            yield win_start.strftime(\"%Y-%m-%d\"), win_end.strftime(\"%Y-%m-%d\")\n            year += 1\n\n    params = (params or {}).copy()\n\n    for year_from, year_to in year_windows():\n        logger.info(f\"Starting to scrape data from {year_from} to {year_to}\")\n        page = 0\n        while True:\n            params.update(\n                {\n                    \"limit\": self.page_size,\n                    \"skip\": page * self.page_size,\n                    \"searchDateFrom\": year_from,\n                    \"searchDateTo\": year_to,\n                }\n            )\n            try:\n                data = self.request(\n                    urljoin(self.base, query),\n                    method,\n                    params=params,\n                    parse_json=True,\n                )\n            except requests.HTTPError as e:\n                status = getattr(e.response, \"status_code\", None)\n                logger.warning(\n                    f\"Stopping year window {year_from}..{year_to} at skip={page * self.page_size} \"\n                    f\"due to HTTPError {status}\"\n                )\n                break\n            items = data.get(\"itemHits\", [])\n            if not items:\n                break\n            for item in items:\n                yield item\n            page += 1\n</code></pre> <code></code> <code>all_preprints()</code> \u00b6 <p>Return a generator to all the chemRxiv articles.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def all_preprints(self):\n    \"\"\"Return a generator to all the chemRxiv articles.\"\"\"\n    return self.query_generator(\"items\")\n</code></pre> <code></code> <code>preprint(article_id)</code> \u00b6 <p>Information on a given preprint. .. seealso:: https://docs.figshare.com/#public_article</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/chemrxiv_api.py</code> <pre><code>def preprint(self, article_id):\n    \"\"\"Information on a given preprint.\n    .. seealso:: https://docs.figshare.com/#public_article\n    \"\"\"\n    return self.query(os.path.join(\"items\", article_id))\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.utils","title":"<code>utils</code>","text":"<p>Misc utils to download chemRxiv dump</p>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.utils.get_author","title":"<code>get_author(author_list: List[Dict]) -&gt; str</code>","text":"<p>Parse ChemRxiv dump entry to extract author list</p> <p>Parameters:</p> Name Type Description Default <code>author_list</code> <code>list</code> <p>List of dicts, one per author.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>;-concatenated author list.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_author(author_list: List[Dict]) -&gt; str:\n    \"\"\"Parse ChemRxiv dump entry to extract author list\n\n    Args:\n        author_list (list): List of dicts, one per author.\n\n    Returns:\n        str: ;-concatenated author list.\n    \"\"\"\n\n    return \"; \".join([\" \".join([a[\"firstName\"], a[\"lastName\"]]) for a in author_list])\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.utils.get_categories","title":"<code>get_categories(category_list: List[Dict]) -&gt; str</code>","text":"<p>Parse ChemRxiv dump entry to extract the categories of the paper</p> <p>Parameters:</p> Name Type Description Default <code>category_list</code> <code>list</code> <p>List of dicts, one per category.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>;-concatenated category list.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_categories(category_list: List[Dict]) -&gt; str:\n    \"\"\"Parse ChemRxiv dump entry to extract the categories of the paper\n\n    Args:\n        category_list (list): List of dicts, one per category.\n\n    Returns:\n        str: ;-concatenated category list.\n    \"\"\"\n\n    return \"; \".join([a[\"name\"] for a in category_list])\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.utils.get_date","title":"<code>get_date(datestring: str) -&gt; str</code>","text":"<p>Get the date of a chemrxiv dump enry.</p> <p>Parameters:</p> Name Type Description Default <code>datestring</code> <code>str</code> <p>String in the format: 2021-10-15T05:12:32.356Z</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Date in the format: YYYY-MM-DD.</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_date(datestring: str) -&gt; str:\n    \"\"\"Get the date of a chemrxiv dump enry.\n\n    Args:\n        datestring: String in the format: 2021-10-15T05:12:32.356Z\n\n    Returns:\n        str: Date in the format: YYYY-MM-DD.\n    \"\"\"\n    return datestring.split(\"T\")[0]\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.utils.get_metrics","title":"<code>get_metrics(metrics_list: List[Dict]) -&gt; Dict</code>","text":"<p>Parse ChemRxiv dump entry to extract the access metrics of the paper.</p> <p>Parameters:</p> Name Type Description Default <code>metrics_list</code> <code>List[Dict]</code> <p>A list of single-keyed, dictionaries each containing key and value for exactly one metric.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict</code> <p>A flattened dictionary with all metrics and a timestamp</p> Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def get_metrics(metrics_list: List[Dict]) -&gt; Dict:\n    \"\"\"\n    Parse ChemRxiv dump entry to extract the access metrics of the paper.\n\n    Args:\n        metrics_list (List[Dict]): A list of single-keyed, dictionaries each\n            containing key and value for exactly one metric.\n\n    Returns:\n        Dict: A flattened dictionary with all metrics and a timestamp\n    \"\"\"\n    metric_dict = {m[\"description\"]: m[\"value\"] for m in metrics_list}\n\n    # This assumes that the .jsonl is constructed at roughly the same date\n    # where this entry was obtained from the API\n    metric_dict.update({\"timestamp\": today})\n</code></pre>"},{"location":"api/get_dumps/#paperscraper.get_dumps.utils.chemrxiv.utils.parse_dump","title":"<code>parse_dump(source_path: str, target_path: str) -&gt; None</code>","text":"<p>Parses the dump as generated by the chemrXiv API and this repo: https://github.com/cthoyt/chemrxiv-summarize into a format that is equal to that of biorXiv and medRxiv.</p> <p>NOTE: This is a lazy parser trying to store all data in memory.</p> <p>Parameters:</p> Name Type Description Default <code>source_path</code> <code>str</code> <p>Path to the source dump</p> required Source code in <code>paperscraper/get_dumps/utils/chemrxiv/utils.py</code> <pre><code>def parse_dump(source_path: str, target_path: str) -&gt; None:\n    \"\"\"\n    Parses the dump as generated by the chemrXiv API and this repo:\n    https://github.com/cthoyt/chemrxiv-summarize\n    into a format that is equal to that of biorXiv and medRxiv.\n\n    NOTE: This is a lazy parser trying to store all data in memory.\n\n    Args:\n        source_path: Path to the source dump\n    \"\"\"\n\n    dump = []\n    # Read source dump\n    for file_name in tqdm(os.listdir(source_path)):\n        if not file_name.endswith(\".json\"):\n            continue\n        filepath = os.path.join(source_path, file_name)\n        with open(filepath, \"r\") as f:\n            source_paper = json.load(f)\n\n        target_paper = {\n            \"title\": source_paper[\"title\"],\n            \"doi\": source_paper[\"doi\"],\n            \"published_doi\": (\n                source_paper[\"vor\"][\"vorDoi\"] if source_paper[\"vor\"] else \"N.A.\"\n            ),\n            \"published_url\": (\n                source_paper[\"vor\"][\"url\"] if source_paper[\"vor\"] else \"N.A.\"\n            ),\n            \"authors\": get_author(source_paper[\"authors\"]),\n            \"abstract\": source_paper[\"abstract\"],\n            \"date\": get_date(source_paper[\"statusDate\"]),\n            \"journal\": \"chemRxiv\",\n            \"categories\": get_categories(source_paper[\"categories\"]),\n            \"metrics\": get_metrics(source_paper[\"metrics\"]),\n            \"license\": source_paper[\"license\"][\"name\"],\n        }\n        dump.append(target_paper)\n        os.remove(filepath)\n    # Write dump\n    with open(target_path, \"w\") as f:\n        for idx, target_paper in enumerate(dump):\n            if idx &gt; 0:\n                f.write(os.linesep)\n            f.write(json.dumps(target_paper))\n    logger.info(\"Done, shutting down\")\n</code></pre>"},{"location":"api/pdf/","title":"paperscraper.pdf","text":""},{"location":"api/pdf/#paperscraper.pdf","title":"<code>paperscraper.pdf</code>","text":""},{"location":"api/pdf/#paperscraper.pdf.fallbacks","title":"<code>fallbacks</code>","text":"<p>Functionalities to scrape PDF files of publications.</p>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.fallback_wiley_api","title":"<code>fallback_wiley_api(paper_metadata: Dict[str, Any], output_path: Path, api_keys: Dict[str, str], max_attempts: int = 2) -&gt; bool</code>","text":"<p>Attempt to download the PDF via the Wiley TDM API (popular publisher which blocks standard scraping attempts; API access free for academic users).</p> <p>This function uses the WILEY_TDM_API_TOKEN environment variable to authenticate with the Wiley TDM API and attempts to download the PDF for the given paper. See https://onlinelibrary.wiley.com/library-info/resources/text-and-datamining for a description on how to get your WILEY_TDM_API_TOKEN.</p> <p>Parameters:</p> Name Type Description Default <code>paper_metadata</code> <code>dict</code> <p>Dictionary containing paper metadata. Must include the 'doi' key.</p> required <code>output_path</code> <code>Path</code> <p>A pathlib.Path object representing the path where the PDF will be saved.</p> required <code>api_keys</code> <code>dict</code> <p>Preloaded API keys.</p> required <code>max_attempts</code> <code>int</code> <p>The maximum number of attempts to retry API call.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the PDF file was successfully downloaded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_wiley_api(\n    paper_metadata: Dict[str, Any],\n    output_path: Path,\n    api_keys: Dict[str, str],\n    max_attempts: int = 2,\n) -&gt; bool:\n    \"\"\"\n    Attempt to download the PDF via the Wiley TDM API (popular publisher which blocks standard scraping attempts; API access free for academic users).\n\n    This function uses the WILEY_TDM_API_TOKEN environment variable to authenticate\n    with the Wiley TDM API and attempts to download the PDF for the given paper.\n    See https://onlinelibrary.wiley.com/library-info/resources/text-and-datamining for a description on how to get your WILEY_TDM_API_TOKEN.\n\n    Args:\n        paper_metadata (dict): Dictionary containing paper metadata. Must include the 'doi' key.\n        output_path (Path): A pathlib.Path object representing the path where the PDF will be saved.\n        api_keys (dict): Preloaded API keys.\n        max_attempts (int): The maximum number of attempts to retry API call.\n\n    Returns:\n        bool: True if the PDF file was successfully downloaded, False otherwise.\n    \"\"\"\n\n    WILEY_TDM_API_TOKEN = api_keys.get(\"WILEY_TDM_API_TOKEN\")\n    encoded_doi = paper_metadata[\"doi\"].replace(\"/\", \"%2F\")\n    api_url = f\"https://api.wiley.com/onlinelibrary/tdm/v1/articles/{encoded_doi}\"\n    headers = {\"Wiley-TDM-Client-Token\": WILEY_TDM_API_TOKEN}\n\n    attempt = 0\n    success = False\n\n    while attempt &lt; max_attempts:\n        try:\n            api_response = requests.get(\n                api_url, headers=headers, allow_redirects=True, timeout=60\n            )\n            api_response.raise_for_status()\n            if api_response.content[:4] != b\"%PDF\":\n                logger.warning(\n                    f\"API returned content that is not a valid PDF for {paper_metadata['doi']}.\"\n                )\n            else:\n                with open(output_path.with_suffix(\".pdf\"), \"wb+\") as f:\n                    f.write(api_response.content)\n                logger.info(\n                    f\"Successfully downloaded PDF via Wiley API for {paper_metadata['doi']}.\"\n                )\n                success = True\n                break\n        except Exception as e2:\n            if attempt &lt; max_attempts - 1:\n                logger.info(\"Waiting 20 seconds before retrying...\")\n                time.sleep(20)\n            logger.error(\n                f\"Could not download via Wiley API (attempt {attempt + 1}/{max_attempts}): {e2}\"\n            )\n\n        attempt += 1\n\n    # **Mandatory delay of 10 seconds to comply with Wiley API rate limits**\n    logger.info(\n        \"Waiting 10 seconds before next request to comply with Wiley API rate limits...\"\n    )\n    time.sleep(10)\n    return success\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.fallback_bioc_pmc","title":"<code>fallback_bioc_pmc(doi: str, output_path: Path) -&gt; bool</code>","text":"<p>Attempt to download the XML via the BioC-PMC fallback.</p> <p>This function first converts a given DOI to a PMCID using the NCBI ID Converter API. If a PMCID is found, it constructs the corresponding PMC XML URL and attempts to download the full-text XML.</p> <p>PubMed Central\u00ae (PMC) is a free full-text archive of biomedical and life sciences journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM).</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI of the paper to retrieve.</p> required <code>output_path</code> <code>Path</code> <p>A pathlib.Path object representing the path where the XML file will be saved.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the XML file was successfully downloaded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_bioc_pmc(doi: str, output_path: Path) -&gt; bool:\n    \"\"\"\n    Attempt to download the XML via the BioC-PMC fallback.\n\n    This function first converts a given DOI to a PMCID using the NCBI ID Converter API.\n    If a PMCID is found, it constructs the corresponding PMC XML URL and attempts to\n    download the full-text XML.\n\n    PubMed Central\u00ae (PMC) is a free full-text archive of biomedical and life sciences\n    journal literature at the U.S. National Institutes of Health's National Library of Medicine (NIH/NLM).\n\n    Args:\n        doi (str): The DOI of the paper to retrieve.\n        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.\n\n    Returns:\n        bool: True if the XML file was successfully downloaded, False otherwise.\n    \"\"\"\n    ncbi_tool = \"paperscraper\"\n    ncbi_email = \"your_email@example.com\"\n\n    converter_url = \"https://www.ncbi.nlm.nih.gov/pmc/utils/idconv/v1.0/\"\n    params = {\n        \"tool\": ncbi_tool,\n        \"email\": ncbi_email,\n        \"ids\": doi,\n        \"idtype\": \"doi\",\n        \"format\": \"json\",\n    }\n    try:\n        conv_response = requests.get(converter_url, params=params, timeout=60)\n        conv_response.raise_for_status()\n        data = conv_response.json()\n        records = data.get(\"records\", [])\n        if not records or \"pmcid\" not in records[0]:\n            logger.warning(\n                f\"No PMCID available for DOI {doi}. Fallback via PMC therefore not possible.\"\n            )\n            return False\n        pmcid = records[0][\"pmcid\"]\n        logger.info(f\"Converted DOI {doi} to PMCID {pmcid}.\")\n    except Exception as conv_err:\n        logger.error(f\"Error during DOI to PMCID conversion: {conv_err}\")\n        return False\n\n    # Construct PMC XML URL\n    xml_url = f\"https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/pmcoa.cgi/BioC_xml/{pmcid}/unicode\"\n    logger.info(f\"Attempting to download XML from BioC-PMC URL: {xml_url}\")\n    try:\n        xml_response = requests.get(xml_url, timeout=60)\n        xml_response.raise_for_status()\n        xml_path = output_path.with_suffix(\".xml\")\n        # check for xml error:\n        if xml_response.content.startswith(\n            b\"[Error] : No result can be found. &lt;BR&gt;&lt;HR&gt;&lt;B&gt; - https://www.ncbi.nlm.nih.gov/research/bionlp/RESTful/\"\n        ):\n            logger.warning(f\"No XML found for DOI {doi} at BioC-PMC URL {xml_url}.\")\n            return False\n        with open(xml_path, \"wb+\") as f:\n            f.write(xml_response.content)\n        logger.info(f\"Successfully downloaded XML for DOI {doi} to {xml_path}.\")\n        return True\n    except Exception as xml_err:\n        logger.error(f\"Failed to download XML from BioC-PMC URL {xml_url}: {xml_err}\")\n        return False\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.fallback_elsevier_api","title":"<code>fallback_elsevier_api(paper_metadata: Dict[str, Any], output_path: Path, api_keys: Dict[str, str]) -&gt; bool</code>","text":"<p>Attempt to download the full text via the Elsevier TDM API. For more information, see: https://www.elsevier.com/about/policies-and-standards/text-and-data-mining (Requires an institutional subscription and an API key provided in the api_keys dictionary under the key \"ELSEVIER_TDM_API_KEY\".)</p> <p>Parameters:</p> Name Type Description Default <code>paper_metadata</code> <code>Dict[str, Any]</code> <p>Dictionary containing paper metadata. Must include the 'doi' key.</p> required <code>output_path</code> <code>Path</code> <p>A pathlib.Path object representing the path where the XML file will be saved.</p> required <code>api_keys</code> <code>Dict[str, str]</code> <p>A dictionary containing API keys. Must include the key \"ELSEVIER_TDM_API_KEY\".</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the XML file was successfully downloaded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_elsevier_api(\n    paper_metadata: Dict[str, Any], output_path: Path, api_keys: Dict[str, str]\n) -&gt; bool:\n    \"\"\"\n    Attempt to download the full text via the Elsevier TDM API.\n    For more information, see:\n    https://www.elsevier.com/about/policies-and-standards/text-and-data-mining\n    (Requires an institutional subscription and an API key provided in the api_keys dictionary under the key \"ELSEVIER_TDM_API_KEY\".)\n\n    Args:\n        paper_metadata (Dict[str, Any]): Dictionary containing paper metadata. Must include the 'doi' key.\n        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.\n        api_keys (Dict[str, str]): A dictionary containing API keys. Must include the key \"ELSEVIER_TDM_API_KEY\".\n\n    Returns:\n        bool: True if the XML file was successfully downloaded, False otherwise.\n    \"\"\"\n    elsevier_api_key = api_keys.get(\"ELSEVIER_TDM_API_KEY\")\n    doi = paper_metadata[\"doi\"]\n    api_url = f\"https://api.elsevier.com/content/article/doi/{doi}?apiKey={elsevier_api_key}&amp;httpAccept=text%2Fxml\"\n    logger.info(f\"Attempting download via Elsevier API (XML) for {doi}: {api_url}\")\n    headers = {\"Accept\": \"application/xml\"}\n    try:\n        response = requests.get(api_url, headers=headers, timeout=60)\n\n        # Check for 401 error and look for APIKEY_INVALID in the response\n        if response.status_code == 401:\n            error_text = response.text\n            if \"APIKEY_INVALID\" in error_text:\n                logger.error(\"Invalid API key. Couldn't download via Elsevier XML API\")\n            else:\n                logger.error(\"401 Unauthorized. Couldn't download via Elsevier XML API\")\n            return False\n\n        response.raise_for_status()\n\n        # Attempt to parse it with lxml to confirm it's valid XML\n        try:\n            etree.fromstring(response.content)\n        except etree.XMLSyntaxError as e:\n            logger.warning(f\"Elsevier API returned invalid XML for {doi}: {e}\")\n            return False\n\n        xml_path = output_path.with_suffix(\".xml\")\n        with open(xml_path, \"wb\") as f:\n            f.write(response.content)\n        logger.info(\n            f\"Successfully used Elsevier API to downloaded XML for {doi} to {xml_path}\"\n        )\n        return True\n    except Exception as e:\n        logger.error(f\"Could not download via Elsevier XML API: {e}\")\n        return False\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.fallback_elife_xml","title":"<code>fallback_elife_xml(doi: str, output_path: Path) -&gt; bool</code>","text":"<p>Attempt to download the XML via the eLife XML repository on GitHub.</p> <p>eLife provides open access to their XML files on GitHub, which can be used as a fallback. When multiple versions exist (revised papers), it takes the latest version (e.g., v3 instead of v1).</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI of the eLife paper to download.</p> required <code>output_path</code> <code>Path</code> <p>A pathlib.Path object representing the path where the XML file will be saved.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the XML file was successfully downloaded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_elife_xml(doi: str, output_path: Path) -&gt; bool:\n    \"\"\"\n    Attempt to download the XML via the eLife XML repository on GitHub.\n\n    eLife provides open access to their XML files on GitHub, which can be used as a fallback.\n    When multiple versions exist (revised papers), it takes the latest version (e.g., v3 instead of v1).\n\n    Args:\n        doi (str): The DOI of the eLife paper to download.\n        output_path (Path): A pathlib.Path object representing the path where the XML file will be saved.\n\n    Returns:\n        bool: True if the XML file was successfully downloaded, False otherwise.\n    \"\"\"\n    parts = doi.split(\"eLife.\")\n    if len(parts) &lt; 2:\n        logger.error(f\"Unable to parse eLife DOI: {doi}\")\n        return False\n    article_num = parts[1].strip()\n\n    index = get_elife_xml_index()\n    if article_num not in index:\n        logger.warning(f\"No eLife XML found for DOI {doi}.\")\n        return False\n    candidate_files = index[article_num]\n    latest_version, latest_download_url = max(candidate_files, key=lambda x: x[0])\n    try:\n        r = requests.get(latest_download_url, timeout=60)\n        r.raise_for_status()\n        latest_xml = r.content\n    except Exception as e:\n        logger.error(f\"Error downloading file from {latest_download_url}: {e}\")\n        return False\n\n    xml_path = output_path.with_suffix(\".xml\")\n    with open(xml_path, \"wb\") as f:\n        f.write(latest_xml)\n    logger.info(\n        f\"Successfully downloaded XML via eLife API ({latest_version}) for DOI {doi} to {xml_path}.\"\n    )\n    return True\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.get_elife_xml_index","title":"<code>get_elife_xml_index() -&gt; dict</code>","text":"<p>Fetch the eLife XML index from GitHub and return it as a dictionary.</p> <p>This function retrieves and caches the list of available eLife articles in XML format from the eLife GitHub repository. It ensures that the latest version of each article is accessible for downloading. The index is cached in memory to avoid repeated network requests when processing multiple eLife papers.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary where keys are article numbers (as strings) and values are   lists of tuples (version, download_url). Each list is sorted by version number.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def get_elife_xml_index() -&gt; dict:\n    \"\"\"\n    Fetch the eLife XML index from GitHub and return it as a dictionary.\n\n    This function retrieves and caches the list of available eLife articles in XML format\n    from the eLife GitHub repository. It ensures that the latest version of each article\n    is accessible for downloading. The index is cached in memory to avoid repeated\n    network requests when processing multiple eLife papers.\n\n    Returns:\n        dict: A dictionary where keys are article numbers (as strings) and values are\n              lists of tuples (version, download_url). Each list is sorted by version number.\n    \"\"\"\n    global ELIFE_XML_INDEX\n    if ELIFE_XML_INDEX is None:\n        logger.info(\"Fetching eLife XML index from GitHub using git tree API\")\n        ELIFE_XML_INDEX = {}\n        # Use the git tree API to get the full repository tree.\n        base_tree_url = \"https://api.github.com/repos/elifesciences/elife-article-xml/git/trees/master?recursive=1\"\n        r = requests.get(base_tree_url, timeout=60)\n        r.raise_for_status()\n        tree_data = r.json()\n        items = tree_data.get(\"tree\", [])\n        # Look for files in the 'articles' directory matching the pattern.\n        pattern = r\"articles/elife-(\\d+)-v(\\d+)\\.xml\"\n        for item in items:\n            path = item.get(\"path\", \"\")\n            match = re.match(pattern, path)\n            if match:\n                article_num_padded = match.group(1)\n                version = int(match.group(2))\n                # Construct the raw download URL.\n                download_url = f\"https://raw.githubusercontent.com/elifesciences/elife-article-xml/master/{path}\"\n                ELIFE_XML_INDEX.setdefault(article_num_padded, []).append(\n                    (version, download_url)\n                )\n        # Sort each article's file list by version.\n        for key in ELIFE_XML_INDEX:\n            ELIFE_XML_INDEX[key].sort(key=lambda x: x[0])\n    return ELIFE_XML_INDEX\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.month_folder","title":"<code>month_folder(doi: str) -&gt; str</code>","text":"<p>Query bioRxiv API to get the posting date of a given DOI. Convert a date to the BioRxiv S3 folder name, rolling over if it's the month's last day. E.g., if date is the last day of April, treat as May_YYYY.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI for which to retrieve the date.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Month and year in format <code>October_2019</code></p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def month_folder(doi: str) -&gt; str:\n    \"\"\"\n    Query bioRxiv API to get the posting date of a given DOI.\n    Convert a date to the BioRxiv S3 folder name, rolling over if it's the month's last day.\n    E.g., if date is the last day of April, treat as May_YYYY.\n\n    Args:\n        doi: The DOI for which to retrieve the date.\n\n    Returns:\n        Month and year in format `October_2019`\n    \"\"\"\n    url = f\"https://api.biorxiv.org/details/biorxiv/{doi}/na/json\"\n    resp = requests.get(url, timeout=30)\n    resp.raise_for_status()\n    date_str = resp.json()[\"collection\"][0][\"date\"]\n    date = datetime.date.fromisoformat(date_str)\n\n    # NOTE: bioRxiv papers posted on the last day of the month are archived the next day\n    last_day = calendar.monthrange(date.year, date.month)[1]\n    if date.day == last_day:\n        date = date + datetime.timedelta(days=1)\n    return date.strftime(\"%B_%Y\")\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.list_meca_keys","title":"<code>list_meca_keys(s3_client: BaseClient, bucket: str, prefix: str) -&gt; list</code>","text":"<p>List all .meca object keys under a given prefix in a requester-pays bucket.</p> <p>Parameters:</p> Name Type Description Default <code>s3_client</code> <code>BaseClient</code> <p>S3 client to get the data from.</p> required <code>bucket</code> <code>str</code> <p>bucket to get data from.</p> required <code>prefix</code> <code>str</code> <p>prefix to get data from.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of keys, one per existing .meca in the bucket.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def list_meca_keys(s3_client: BaseClient, bucket: str, prefix: str) -&gt; list:\n    \"\"\"\n    List all .meca object keys under a given prefix in a requester-pays bucket.\n\n    Args:\n        s3_client: S3 client to get the data from.\n        bucket: bucket to get data from.\n        prefix: prefix to get data from.\n\n    Returns:\n        List of keys, one per existing .meca in the bucket.\n    \"\"\"\n    keys = []\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    for page in paginator.paginate(\n        Bucket=bucket, Prefix=prefix, RequestPayer=\"requester\"\n    ):\n        for obj in page.get(\"Contents\", []):\n            if obj[\"Key\"].endswith(\".meca\"):\n                keys.append(obj[\"Key\"])\n    return keys\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.find_meca_for_doi","title":"<code>find_meca_for_doi(s3_client: BaseClient, bucket: str, key: str, doi_token: str, stop_event: threading.Event, tail_bytes: int = 131072) -&gt; bool</code>","text":"<p>Efficiently inspect manifest.xml within a .meca zip by fetching only necessary bytes. Parse via ZipFile to read manifest.xml and match DOI token.</p> <p>Parameters:</p> Name Type Description Default <code>s3_client</code> <code>BaseClient</code> <p>S3 client to get the data from.</p> required <code>bucket</code> <code>str</code> <p>bucket to get data from.</p> required <code>key</code> <code>str</code> <p>prefix to get data from.</p> required <code>doi_token</code> <code>str</code> <p>the DOI that should be matched</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Whether or not the DOI could be matched</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def find_meca_for_doi(\n    s3_client: BaseClient,\n    bucket: str,\n    key: str,\n    doi_token: str,\n    stop_event: threading.Event,\n    tail_bytes: int = 131072,\n) -&gt; bool:\n    \"\"\"\n    Efficiently inspect manifest.xml within a .meca zip by fetching only necessary bytes.\n    Parse via ZipFile to read manifest.xml and match DOI token.\n\n    Args:\n        s3_client: S3 client to get the data from.\n        bucket: bucket to get data from.\n        key: prefix to get data from.\n        doi_token: the DOI that should be matched\n\n    Returns:\n        Whether or not the DOI could be matched\n    \"\"\"\n\n    if stop_event.is_set():\n        return False\n\n    try:\n        # Try tail-only first (central directory is at end)\n        tail = s3_client.get_object(\n            Bucket=bucket,\n            Key=key,\n            Range=f\"bytes=-{tail_bytes}\",\n            RequestPayer=\"requester\",\n        )[\"Body\"].read()\n    except Exception:\n        return False\n\n    if stop_event.is_set():\n        return False\n\n    data = tail\n    try:\n        with zipfile.ZipFile(io.BytesIO(data)) as z:\n            # avoid reading file contents; inspect namelist/central directory\n            for name in z.namelist():\n                if name.endswith(\"manifest.xml\"):\n                    manifest = z.read(name)  # small file in practice\n                    token = doi_token.split(\".\")[-1].encode(\"utf-8\")\n                    return token in manifest.lower()\n    except zipfile.BadZipFile:\n        # Fallback: fetch small head slice &amp; retry zip\n        try:\n            head = s3_client.get_object(\n                Bucket=bucket, Key=key, Range=\"bytes=0-65535\", RequestPayer=\"requester\"\n            )[\"Body\"].read()\n            with zipfile.ZipFile(io.BytesIO(head + tail)) as z:\n                manifest = z.read(\"manifest.xml\")\n                token = doi_token.split(\".\")[-1].encode(\"utf-8\")\n                return token in manifest.lower()\n        except Exception:\n            return False\n    return False\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.fallbacks.fallback_s3","title":"<code>fallback_s3(doi: str, output_path: Union[str, Path], api_keys: dict, workers: int = 32) -&gt; bool</code>","text":"<p>Download a BioRxiv PDF via the requester-pays S3 bucket using range requests.</p> <p>Parameters:</p> Name Type Description Default <code>doi</code> <code>str</code> <p>The DOI for which to retrieve the PDF (e.g. '10.1101/798496').</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path where the PDF will be saved (with .pdf suffix added).</p> required <code>api_keys</code> <code>dict</code> <p>Dict containing 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY'.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if download succeeded, False otherwise.</p> Source code in <code>paperscraper/pdf/fallbacks.py</code> <pre><code>def fallback_s3(\n    doi: str, output_path: Union[str, Path], api_keys: dict, workers: int = 32\n) -&gt; bool:\n    \"\"\"\n    Download a BioRxiv PDF via the requester-pays S3 bucket using range requests.\n\n    Args:\n        doi: The DOI for which to retrieve the PDF (e.g. '10.1101/798496').\n        output_path: Path where the PDF will be saved (with .pdf suffix added).\n        api_keys: Dict containing 'AWS_ACCESS_KEY_ID' and 'AWS_SECRET_ACCESS_KEY'.\n\n    Returns:\n        True if download succeeded, False otherwise.\n    \"\"\"\n\n    s3 = boto3.client(\n        \"s3\",\n        aws_access_key_id=api_keys.get(\"AWS_ACCESS_KEY_ID\"),\n        aws_secret_access_key=api_keys.get(\"AWS_SECRET_ACCESS_KEY\"),\n        region_name=\"us-east-1\",\n        config=Config(connect_timeout=5, read_timeout=10, retries={\"max_attempts\": 3}),\n    )\n    bucket = \"biorxiv-src-monthly\"\n\n    # Derive prefix from DOI date\n    prefix = f\"Current_Content/{month_folder(doi)}/\"\n\n    # List MECA archives in that month\n    meca_keys = list_meca_keys(s3, bucket, prefix)\n    if not meca_keys:\n        return False\n\n    token = doi.split(\"/\")[-1].lower()\n\n    # Prefer keys that already contain the token\n    candidate_keys = [k for k in meca_keys if token in k.lower()]\n    # If none contain the token (older DOIs, etc.), fall back to a small prefix scan\n    if not candidate_keys:\n        candidate_keys = meca_keys[: min(500, len(meca_keys))]\n    out_pdf = Path(output_path).with_suffix(\".pdf\")\n\n    # Try candidates concurrently but keep at most `workers` in flight.\n    stop = threading.Event()\n\n    def job(k):\n        ok = _try_download_pdf_from_meca(s3, bucket, k, out_pdf, stop)\n        if ok:\n            stop.set()\n        return ok\n\n    executor = ThreadPoolExecutor(max_workers=workers)\n    found = False\n    try:\n        it = iter(candidate_keys)\n        # prime the queue with at most `workers` tasks\n        futures = set()\n        for _ in range(min(workers, len(candidate_keys))):\n            k = next(it, None)\n            if k is not None:\n                futures.add(executor.submit(job, k))\n\n        while futures and not found:\n            done, futures = wait(futures, return_when=FIRST_COMPLETED)\n            # check completed ones\n            for fut in done:\n                try:\n                    if fut.result():\n                        found = True\n                        stop.set()\n                        # cancel not-yet-started tasks\n                        for f in list(futures):\n                            f.cancel()\n                        break\n                except Exception:\n                    pass\n            # top up queue if still searching\n            while not found and len(futures) &lt; workers:\n                k = next(it, None)\n                if k is None:\n                    break\n                futures.add(executor.submit(job, k))\n    finally:\n        # don't wait for running tasks; best-effort cancel\n        executor.shutdown(wait=False, cancel_futures=True)\n\n    if not found:\n        logger.error(f\"Could not find {doi} on biorxiv\")\n        return False\n    return True\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.pdf","title":"<code>pdf</code>","text":"<p>Functionalities to scrape PDF files of publications.</p>"},{"location":"api/pdf/#paperscraper.pdf.pdf.save_pdf","title":"<code>save_pdf(paper_metadata: Dict[str, Any], filepath: Union[str, Path], save_metadata: bool = False, api_keys: Optional[Union[str, Dict[str, str]]] = None) -&gt; None</code>","text":"<p>Save a PDF file of a paper.</p> <p>Parameters:</p> Name Type Description Default <code>paper_metadata</code> <code>Dict[str, Any]</code> <p>A dictionary with the paper metadata. Must contain the <code>doi</code> key.</p> required <code>filepath</code> <code>Union[str, Path]</code> <p>Path to the PDF file to be saved (with or without suffix).</p> required <code>save_metadata</code> <code>bool</code> <p>A boolean indicating whether to save paper metadata as a separate json.</p> <code>False</code> <code>api_keys</code> <code>Optional[Union[str, Dict[str, str]]]</code> <p>Either a dictionary containing API keys (if already loaded) or a string (path to API keys file).       If None, will try to load from <code>.env</code> file and if unsuccessful, skip API-based fallbacks.</p> <code>None</code> Source code in <code>paperscraper/pdf/pdf.py</code> <pre><code>def save_pdf(\n    paper_metadata: Dict[str, Any],\n    filepath: Union[str, Path],\n    save_metadata: bool = False,\n    api_keys: Optional[Union[str, Dict[str, str]]] = None,\n) -&gt; None:\n    \"\"\"\n    Save a PDF file of a paper.\n\n    Args:\n        paper_metadata: A dictionary with the paper metadata. Must contain the `doi` key.\n        filepath: Path to the PDF file to be saved (with or without suffix).\n        save_metadata: A boolean indicating whether to save paper metadata as a separate json.\n        api_keys: Either a dictionary containing API keys (if already loaded) or a string (path to API keys file).\n                  If None, will try to load from `.env` file and if unsuccessful, skip API-based fallbacks.\n    \"\"\"\n    if not isinstance(paper_metadata, Dict):\n        raise TypeError(f\"paper_metadata must be a dict, not {type(paper_metadata)}.\")\n    if \"doi\" not in paper_metadata.keys():\n        raise KeyError(\"paper_metadata must contain the key 'doi'.\")\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}.\")\n\n    output_path = Path(filepath)\n\n    if not Path(output_path).parent.exists():\n        raise ValueError(f\"The folder: {output_path} seems to not exist.\")\n\n    # load API keys from file if not already loaded via in save_pdf_from_dump (dict)\n    if not isinstance(api_keys, dict):\n        api_keys = load_api_keys(api_keys)\n\n    doi = paper_metadata[\"doi\"]\n    url = f\"https://doi.org/{doi}\"\n    user_agent = {\"User-Agent\": \"paperscraper/1.0 (+https)\"}\n    if \"arxiv\" in doi:\n        soup = None\n        try:\n            match = re.search(\n                r\"arxiv\\.([0-9]{4}\\.[0-9]{4,5}(?:v\\d+)?)\", doi, re.IGNORECASE\n            )\n            arxiv_id = match.group(1)\n            pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n            r = requests.get(pdf_url, timeout=60, headers=user_agent)\n            r.raise_for_status()\n            if r.content[:4] == b\"%PDF\":\n                with open(output_path.with_suffix(\".pdf\"), \"wb+\") as f:\n                    f.write(r.content)\n                # If metadata requested, fetch the landing page now to extract it\n                if save_metadata:\n                    try:\n                        resp_landing = requests.get(url, timeout=60, headers=user_agent)\n                        resp_landing.raise_for_status()\n                        soup = BeautifulSoup(resp_landing.text, features=\"lxml\")\n                    except Exception as _:\n                        soup = None\n                else:\n                    return\n            else:\n                logger.warning(\n                    f\"Direct arXiv fetch returned non-PDF for {doi}. Falling back.\"\n                )\n        except Exception as e:\n            logger.warning(\n                f\"Direct arXiv PDF fetch failed for {doi}: {e}. Falling back.\"\n            )\n\n    success = False\n    try:\n        response = requests.get(url, timeout=60)\n        response.raise_for_status()\n        success = True\n    except Exception as e:\n        error = str(e)\n        logger.warning(f\"Could not download from: {url} - {e}. \")\n\n    if not success and \"biorxiv\" in error:\n        if (\n            api_keys.get(\"AWS_ACCESS_KEY_ID\") is None\n            or api_keys.get(\"AWS_SECRET_ACCESS_KEY\") is None\n        ):\n            logger.info(\n                \"BiorXiv PDFs can be downloaded from a S3 bucket with a requester-pay option. \"\n                \"Consider setting `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY` to use this option. \"\n                \"Pricing is a few cent per GB, thus each request costs &lt; 0.1 cents. \"\n                \"For details see: https://www.biorxiv.org/tdm\"\n            )\n        else:\n            success = FALLBACKS[\"s3\"](doi, output_path, api_keys)\n            if success:\n                return\n\n    if not success:\n        # always first try fallback to BioC-PMC (open access papers on PubMed Central)\n        success = FALLBACKS[\"bioc_pmc\"](doi, output_path)\n\n        # if BioC-PMC fails, try other fallbacks\n        if not success:\n            # check for specific publishers\n            if \"elife\" in error.lower():  # elife has an open XML repository on GitHub\n                FALLBACKS[\"elife\"](doi, output_path)\n            elif (\n                (\"wiley\" in error.lower())\n                and api_keys\n                and (\"WILEY_TDM_API_TOKEN\" in api_keys)\n            ):\n                FALLBACKS[\"wiley\"](paper_metadata, output_path, api_keys)\n        return\n\n    soup = BeautifulSoup(response.text, features=\"lxml\")\n    meta_pdf = soup.find(\"meta\", {\"name\": \"citation_pdf_url\"})\n    if meta_pdf and meta_pdf.get(\"content\"):\n        pdf_url = meta_pdf.get(\"content\")\n        try:\n            response = requests.get(pdf_url, timeout=60)\n            response.raise_for_status()\n\n            if response.content[:4] != b\"%PDF\":\n                logger.warning(\n                    f\"The file from {url} does not appear to be a valid PDF.\"\n                )\n                success = FALLBACKS[\"bioc_pmc\"](doi, output_path)\n                if not success:\n                    # Check for specific publishers\n                    if \"elife\" in doi.lower():\n                        logger.info(\"Attempting fallback to eLife XML repository\")\n                        FALLBACKS[\"elife\"](doi, output_path)\n                    elif api_keys and \"WILEY_TDM_API_TOKEN\" in api_keys:\n                        FALLBACKS[\"wiley\"](paper_metadata, output_path, api_keys)\n                    elif api_keys and \"ELSEVIER_TDM_API_KEY\" in api_keys:\n                        FALLBACKS[\"elsevier\"](paper_metadata, output_path, api_keys)\n            else:\n                with open(output_path.with_suffix(\".pdf\"), \"wb+\") as f:\n                    f.write(response.content)\n        except Exception as e:\n            logger.warning(f\"Could not download {pdf_url}: {e}\")\n    else:  # if no citation_pdf_url meta tag found, try other fallbacks\n        if \"elife\" in doi.lower():\n            logger.info(\n                \"DOI contains eLife, attempting fallback to eLife XML repository on GitHub.\"\n            )\n            if not FALLBACKS[\"elife\"](doi, output_path):\n                logger.warning(\n                    f\"eLife XML fallback failed for {paper_metadata['doi']}.\"\n                )\n        elif (\n            api_keys and \"ELSEVIER_TDM_API_KEY\" in api_keys\n        ):  # elsevier journals can be accessed via the Elsevier TDM API (requires API key)\n            FALLBACKS[\"elsevier\"](paper_metadata, output_path, api_keys)\n        else:\n            logger.warning(\n                f\"Retrieval failed. No citation_pdf_url meta tag found for {url} and no applicable fallback mechanism available.\"\n            )\n\n    if not save_metadata:\n        return\n\n    metadata = {}\n    # Extract title\n    title_tag = soup.find(\"meta\", {\"name\": \"citation_title\"})\n    metadata[\"title\"] = title_tag.get(\"content\") if title_tag else \"Title not found\"\n\n    # Extract authors\n    authors = []\n    for author_tag in soup.find_all(\"meta\", {\"name\": \"citation_author\"}):\n        if author_tag.get(\"content\"):\n            authors.append(author_tag[\"content\"])\n    metadata[\"authors\"] = authors if authors else [\"Author information not found\"]\n\n    # Extract abstract\n    domain = tldextract.extract(url).domain\n    abstract_keys = ABSTRACT_ATTRIBUTE.get(domain, DEFAULT_ATTRIBUTES)\n\n    for key in abstract_keys:\n        abstract_tag = soup.find(\"meta\", {\"name\": key})\n        if abstract_tag:\n            raw_abstract = BeautifulSoup(\n                abstract_tag.get(\"content\", \"None\"), \"html.parser\"\n            ).get_text(separator=\"\\n\")\n            if raw_abstract.strip().startswith(\"Abstract\"):\n                raw_abstract = raw_abstract.strip()[8:]\n            metadata[\"abstract\"] = raw_abstract.strip()\n            break\n\n    if \"abstract\" not in metadata.keys():\n        metadata[\"abstract\"] = \"Abstract not found\"\n        logger.warning(f\"Could not find abstract for {url}\")\n    elif metadata[\"abstract\"].endswith(\"...\"):\n        logger.warning(f\"Abstract truncated from {url}\")\n\n    # Save metadata to JSON\n    try:\n        with open(output_path.with_suffix(\".json\"), \"w\", encoding=\"utf-8\") as f:\n            json.dump(metadata, f, ensure_ascii=False, indent=4)\n    except Exception as e:\n        logger.error(f\"Failed to save metadata to {str(output_path)}: {e}\")\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.pdf.save_pdf_from_dump","title":"<code>save_pdf_from_dump(dump_path: str, pdf_path: str, key_to_save: str = 'doi', save_metadata: bool = False, api_keys: Optional[str] = None) -&gt; None</code>","text":"<p>Receives a path to a <code>.jsonl</code> dump with paper metadata and saves the PDF files of each paper.</p> <p>Parameters:</p> Name Type Description Default <code>dump_path</code> <code>str</code> <p>Path to a <code>.jsonl</code> file with paper metadata, one paper per line.</p> required <code>pdf_path</code> <code>str</code> <p>Path to a folder where the files will be stored.</p> required <code>key_to_save</code> <code>str</code> <p>Key in the paper metadata to use as filename. Has to be <code>doi</code> or <code>title</code>. Defaults to <code>doi</code>.</p> <code>'doi'</code> <code>save_metadata</code> <code>bool</code> <p>A boolean indicating whether to save paper metadata as a separate json.</p> <code>False</code> <code>api_keys</code> <code>Optional[str]</code> <p>Path to a file with API keys. If None, API-based fallbacks will be skipped.</p> <code>None</code> Source code in <code>paperscraper/pdf/pdf.py</code> <pre><code>def save_pdf_from_dump(\n    dump_path: str,\n    pdf_path: str,\n    key_to_save: str = \"doi\",\n    save_metadata: bool = False,\n    api_keys: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Receives a path to a `.jsonl` dump with paper metadata and saves the PDF files of\n    each paper.\n\n    Args:\n        dump_path: Path to a `.jsonl` file with paper metadata, one paper per line.\n        pdf_path: Path to a folder where the files will be stored.\n        key_to_save: Key in the paper metadata to use as filename.\n            Has to be `doi` or `title`. Defaults to `doi`.\n        save_metadata: A boolean indicating whether to save paper metadata as a separate json.\n        api_keys: Path to a file with API keys. If None, API-based fallbacks will be skipped.\n    \"\"\"\n\n    if not isinstance(dump_path, str):\n        raise TypeError(f\"dump_path must be a string, not {type(dump_path)}.\")\n    if not dump_path.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a dump_path with .jsonl extension.\")\n\n    if not isinstance(pdf_path, str):\n        raise TypeError(f\"pdf_path must be a string, not {type(pdf_path)}.\")\n\n    if not isinstance(key_to_save, str):\n        raise TypeError(f\"key_to_save must be a string, not {type(key_to_save)}.\")\n    if key_to_save not in [\"doi\", \"title\", \"date\"]:\n        raise ValueError(\"key_to_save must be one of 'doi' or 'title'.\")\n\n    papers = load_jsonl(dump_path)\n\n    if not isinstance(api_keys, dict):\n        api_keys = load_api_keys(api_keys)\n\n    pbar = tqdm(papers, total=len(papers), desc=\"Processing\")\n    for i, paper in enumerate(pbar):\n        pbar.set_description(f\"Processing paper {i + 1}/{len(papers)}\")\n\n        if \"doi\" not in paper.keys() or paper[\"doi\"] is None:\n            logger.warning(f\"Skipping {paper['title']} since no DOI available.\")\n            continue\n        filename = paper[key_to_save].replace(\"/\", \"_\")\n        pdf_file = Path(os.path.join(pdf_path, f\"{filename}.pdf\"))\n        xml_file = pdf_file.with_suffix(\".xml\")\n        if pdf_file.exists():\n            logger.info(f\"File {pdf_file} already exists. Skipping download.\")\n            continue\n        if xml_file.exists():\n            logger.info(f\"File {xml_file} already exists. Skipping download.\")\n            continue\n        output_path = str(pdf_file)\n        save_pdf(paper, output_path, save_metadata=save_metadata, api_keys=api_keys)\n</code></pre>"},{"location":"api/pdf/#paperscraper.pdf.utils","title":"<code>utils</code>","text":""},{"location":"api/pdf/#paperscraper.pdf.utils.load_api_keys","title":"<code>load_api_keys(filepath: Optional[str] = None) -&gt; Dict[str, str]</code>","text":"<p>Reads API keys from a file and returns them as a dictionary. The file should have each API key on a separate line in the format:     KEY_NAME=API_KEY_VALUE</p> Example <p>WILEY_TDM_API_TOKEN=your_wiley_token_here ELSEVIER_TDM_API_KEY=your_elsevier_key_here</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>Optional[str]</code> <p>Optional path to the file containing API keys.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: A dictionary where keys are API key names and values are their respective API keys.</p> Source code in <code>paperscraper/pdf/utils.py</code> <pre><code>def load_api_keys(filepath: Optional[str] = None) -&gt; Dict[str, str]:\n    \"\"\"\n    Reads API keys from a file and returns them as a dictionary.\n    The file should have each API key on a separate line in the format:\n        KEY_NAME=API_KEY_VALUE\n\n    Example:\n        WILEY_TDM_API_TOKEN=your_wiley_token_here\n        ELSEVIER_TDM_API_KEY=your_elsevier_key_here\n\n    Args:\n        filepath: Optional path to the file containing API keys.\n\n    Returns:\n        Dict[str, str]: A dictionary where keys are API key names and values are their respective API keys.\n    \"\"\"\n    if filepath:\n        load_dotenv(dotenv_path=filepath)\n    else:\n        load_dotenv(find_dotenv())\n\n    return {\n        \"WILEY_TDM_API_TOKEN\": os.getenv(\"WILEY_TDM_API_TOKEN\"),\n        \"ELSEVIER_TDM_API_KEY\": os.getenv(\"ELSEVIER_TDM_API_KEY\"),\n        \"AWS_ACCESS_KEY_ID\": os.getenv(\"AWS_ACCESS_KEY_ID\"),\n        \"AWS_SECRET_ACCESS_KEY\": os.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n    }\n</code></pre>"},{"location":"api/pubmed/","title":"paperscraper.pubmed","text":""},{"location":"api/pubmed/#paperscraper.pubmed","title":"<code>paperscraper.pubmed</code>","text":""},{"location":"api/pubmed/#paperscraper.pubmed.dump_papers","title":"<code>dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None</code>","text":"<p>Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl file with one paper per line.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>DataFrame</code> <p>A dataframe of paper metadata, one paper per row.</p> required <code>filepath</code> <code>str</code> <p>Path to dump the papers, has to end with <code>.jsonl</code>.</p> required Source code in <code>paperscraper/utils.py</code> <pre><code>def dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None:\n    \"\"\"\n    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl\n    file with one paper per line.\n\n    Args:\n        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.\n        filepath (str): Path to dump the papers, has to end with `.jsonl`.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}\")\n    if not filepath.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a filepath with .jsonl extension\")\n\n    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):\n        papers = pd.DataFrame(papers)\n        logger.warning(\n            \"Preferably pass a pd.DataFrame, not a list of dictionaries. \"\n            \"Passing a list is a legacy functionality that might become deprecated.\"\n        )\n\n    if not isinstance(papers, pd.DataFrame):\n        raise TypeError(f\"papers must be a pd.DataFrame, not {type(papers)}\")\n\n    paper_list = list(papers.T.to_dict().values())\n\n    with open(filepath, \"w\") as f:\n        for paper in paper_list:\n            f.write(json.dumps(paper) + \"\\n\")\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.get_emails","title":"<code>get_emails(paper: PubMedArticle) -&gt; List</code>","text":"<p>Extracts author email addresses from PubMedArticle.</p> <p>Parameters:</p> Name Type Description Default <code>paper</code> <code>PubMedArticle</code> <p>An object of type PubMedArticle. Requires to have an 'author' field.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A possibly empty list of emails associated to authors of the paper.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_emails(paper: PubMedArticle) -&gt; List:\n    \"\"\"\n    Extracts author email addresses from PubMedArticle.\n\n    Args:\n        paper (PubMedArticle): An object of type PubMedArticle. Requires to have\n            an 'author' field.\n\n    Returns:\n        List: A possibly empty list of emails associated to authors of the paper.\n    \"\"\"\n\n    emails = []\n    for author in paper.authors:\n        for v in author.values():\n            if v is not None and \"@\" in v:\n                parts = v.split(\"@\")\n                if len(parts) == 2:\n                    # Found one email address\n                    prefix = parts[0].split(\" \")[-1]\n                    postfix = parts[1]\n                    mail = prefix + \"@\" + postfix\n                    if not (postfix.endswith(\".\") or postfix.endswith(\" \")):\n                        emails.append(mail)\n                    else:\n                        emails.append(mail[:-1])\n                else:\n                    # Found multiple addresses\n                    for idx, part in enumerate(parts):\n                        try:\n                            if idx == 0:\n                                prefix = part.split(\" \")[-1]\n                            else:\n                                postfix = part.split(\"\\n\")[0]\n\n                                if postfix.endswith(\".\"):\n                                    postfix = postfix[:-1]\n                                    mail = prefix + \"@\" + postfix\n                                else:\n                                    current_postfix = postfix.split(\" \")[0]\n                                    mail = prefix + \"@\" + current_postfix\n                                    prefix = postfix.split(\" \")[1]\n                                emails.append(mail)\n                        except IndexError:\n                            warnings.warn(f\"Mail could not be inferred from {part}.\")\n\n    return list(set(emails))\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.get_query_from_keywords_and_date","title":"<code>get_query_from_keywords_and_date(keywords: List[Union[str, List]], start_date: str = 'None', end_date: str = 'None') -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the pubmed API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> If start_date and end_date are left as default, the function is <p>identical to get_query_from_keywords.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to pubmed API.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_query_from_keywords_and_date(\n    keywords: List[Union[str, List]], start_date: str = \"None\", end_date: str = \"None\"\n) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the pubmed API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        start_date (str): Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date (str): End date for the search. Same notation as start_date.\n\n    Note: If start_date and end_date are left as default, the function is\n        identical to get_query_from_keywords.\n\n    Returns:\n        str: query to enter to pubmed API.\n    \"\"\"\n\n    query = get_query_from_keywords(keywords)\n\n    if start_date != \"None\" and end_date != \"None\":\n        date = date_root.format(start_date, end_date)\n    elif start_date != \"None\" and end_date == \"None\":\n        date = date_root.format(start_date, \"3000\")\n    elif start_date == \"None\" and end_date != \"None\":\n        date = date_root.format(\"1000\", end_date)\n    else:\n        return query\n\n    return query + \" AND \" + date\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.get_pubmed_papers","title":"<code>get_pubmed_papers(query: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], max_results: int = 9998, *args, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Performs PubMed API request of a query and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to PubMed API. Needs to match PubMed API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. NOTE: If 'emails' is passed, an attempt is made to extract author mail addresses.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>max_results</code> <code>int</code> <p>Maximal number of results retrieved from DB. Defaults to 9998, higher values likely raise problems due to PubMedAPI, see: https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit</p> <code>9998</code> <code>args</code> <p>additional arguments for pubmed.query</p> <code>()</code> <code>kwargs</code> <p>additional arguments for pubmed.query</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame. One paper per row.</p> Source code in <code>paperscraper/pubmed/pubmed.py</code> <pre><code>def get_pubmed_papers(\n    query: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    max_results: int = 9998,\n    *args,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs PubMed API request of a query and returns list of papers with\n    fields as desired.\n\n    Args:\n        query: Query to PubMed API. Needs to match PubMed API notation.\n        fields: List of strings with fields to keep in output.\n            NOTE: If 'emails' is passed, an attempt is made to extract author mail\n            addresses.\n        max_results: Maximal number of results retrieved from DB. Defaults\n            to 9998, higher values likely raise problems due to PubMedAPI, see:\n            https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit\n        args: additional arguments for pubmed.query\n        kwargs: additional arguments for pubmed.query\n\n    Returns:\n        pd.DataFrame. One paper per row.\n\n    \"\"\"\n    if max_results &gt; 9998:\n        logger.warning(\n            f\"\\nmax_results cannot be larger than 9998, received {max_results}.\"\n            \"This will likely result in a JSONDecodeError. Considering lowering `max_results`.\\n\"\n            \"For PubMed, ESearch can only retrieve the first 9,999 records matching the query. \"\n            \"To obtain more than 9,999 PubMed records, consider using EDirect that contains additional\"\n            \"logic to batch PubMed search results automatically so that an arbitrary number can be retrieved\"\n        )\n\n    try:\n        raw = list(PUBMED.query(query, max_results=max_results, *args, **kwargs))\n    except (TypeError, ValueError, KeyError) as e:\n        logger.warning(\n            \"PubMed query returned malformed payload; treating as empty. %s\", e\n        )\n        return pd.DataFrame(columns=list(fields))\n\n    get_mails = \"emails\" in fields\n    if get_mails:\n        fields.pop(fields.index(\"emails\"))\n\n    processed = [\n        {\n            pubmed_field_mapper.get(key, key): process_fields.get(\n                pubmed_field_mapper.get(key, key), lambda x: x\n            )(value)\n            for key, value in paper.toDict().items()\n            if pubmed_field_mapper.get(key, key) in fields\n        }\n        for paper in raw\n    ]\n    if get_mails:\n        for idx, paper in enumerate(raw):\n            processed[idx].update({\"emails\": get_emails(paper)})\n\n    return pd.DataFrame(processed)\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.get_and_dump_pubmed_papers","title":"<code>get_and_dump_pubmed_papers(keywords: List[Union[str, List[str]]], output_filepath: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], start_date: str = 'None', end_date: str = 'None', *args, **kwargs) -&gt; None</code>","text":"<p>Combines get_pubmed_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>List of keywords to request pubmed API. The outer list level will be considered as AND separated keys. The inner level as OR separated.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. Defaults to ['title', 'authors', 'date', 'abstract', 'journal', 'doi']. NOTE: If 'emails' is passed, an attempt is made to extract author mail addresses.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> Source code in <code>paperscraper/pubmed/pubmed.py</code> <pre><code>def get_and_dump_pubmed_papers(\n    keywords: List[Union[str, List[str]]],\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Combines get_pubmed_papers and dump_papers.\n\n    Args:\n        keywords: List of keywords to request pubmed API.\n            The outer list level will be considered as AND separated keys.\n            The inner level as OR separated.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n            Defaults to ['title', 'authors', 'date', 'abstract',\n            'journal', 'doi'].\n            NOTE: If 'emails' is passed, an attempt is made to extract author mail\n            addresses.\n        start_date: Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date: End date for the search. Same notation as start_date.\n    \"\"\"\n    # Translate keywords into query.\n    query = get_query_from_keywords_and_date(\n        keywords, start_date=start_date, end_date=end_date\n    )\n    papers = get_pubmed_papers(query, fields, *args, **kwargs)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.pubmed","title":"<code>pubmed</code>","text":""},{"location":"api/pubmed/#paperscraper.pubmed.pubmed.get_pubmed_papers","title":"<code>get_pubmed_papers(query: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], max_results: int = 9998, *args, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Performs PubMed API request of a query and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to PubMed API. Needs to match PubMed API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. NOTE: If 'emails' is passed, an attempt is made to extract author mail addresses.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>max_results</code> <code>int</code> <p>Maximal number of results retrieved from DB. Defaults to 9998, higher values likely raise problems due to PubMedAPI, see: https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit</p> <code>9998</code> <code>args</code> <p>additional arguments for pubmed.query</p> <code>()</code> <code>kwargs</code> <p>additional arguments for pubmed.query</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame. One paper per row.</p> Source code in <code>paperscraper/pubmed/pubmed.py</code> <pre><code>def get_pubmed_papers(\n    query: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    max_results: int = 9998,\n    *args,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs PubMed API request of a query and returns list of papers with\n    fields as desired.\n\n    Args:\n        query: Query to PubMed API. Needs to match PubMed API notation.\n        fields: List of strings with fields to keep in output.\n            NOTE: If 'emails' is passed, an attempt is made to extract author mail\n            addresses.\n        max_results: Maximal number of results retrieved from DB. Defaults\n            to 9998, higher values likely raise problems due to PubMedAPI, see:\n            https://stackoverflow.com/questions/75353091/biopython-entrez-article-limit\n        args: additional arguments for pubmed.query\n        kwargs: additional arguments for pubmed.query\n\n    Returns:\n        pd.DataFrame. One paper per row.\n\n    \"\"\"\n    if max_results &gt; 9998:\n        logger.warning(\n            f\"\\nmax_results cannot be larger than 9998, received {max_results}.\"\n            \"This will likely result in a JSONDecodeError. Considering lowering `max_results`.\\n\"\n            \"For PubMed, ESearch can only retrieve the first 9,999 records matching the query. \"\n            \"To obtain more than 9,999 PubMed records, consider using EDirect that contains additional\"\n            \"logic to batch PubMed search results automatically so that an arbitrary number can be retrieved\"\n        )\n\n    try:\n        raw = list(PUBMED.query(query, max_results=max_results, *args, **kwargs))\n    except (TypeError, ValueError, KeyError) as e:\n        logger.warning(\n            \"PubMed query returned malformed payload; treating as empty. %s\", e\n        )\n        return pd.DataFrame(columns=list(fields))\n\n    get_mails = \"emails\" in fields\n    if get_mails:\n        fields.pop(fields.index(\"emails\"))\n\n    processed = [\n        {\n            pubmed_field_mapper.get(key, key): process_fields.get(\n                pubmed_field_mapper.get(key, key), lambda x: x\n            )(value)\n            for key, value in paper.toDict().items()\n            if pubmed_field_mapper.get(key, key) in fields\n        }\n        for paper in raw\n    ]\n    if get_mails:\n        for idx, paper in enumerate(raw):\n            processed[idx].update({\"emails\": get_emails(paper)})\n\n    return pd.DataFrame(processed)\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.pubmed.get_and_dump_pubmed_papers","title":"<code>get_and_dump_pubmed_papers(keywords: List[Union[str, List[str]]], output_filepath: str, fields: List = ['title', 'authors', 'date', 'abstract', 'journal', 'doi'], start_date: str = 'None', end_date: str = 'None', *args, **kwargs) -&gt; None</code>","text":"<p>Combines get_pubmed_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[Union[str, List[str]]]</code> <p>List of keywords to request pubmed API. The outer list level will be considered as AND separated keys. The inner level as OR separated.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output. Defaults to ['title', 'authors', 'date', 'abstract', 'journal', 'doi']. NOTE: If 'emails' is passed, an attempt is made to extract author mail addresses.</p> <code>['title', 'authors', 'date', 'abstract', 'journal', 'doi']</code> <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> Source code in <code>paperscraper/pubmed/pubmed.py</code> <pre><code>def get_and_dump_pubmed_papers(\n    keywords: List[Union[str, List[str]]],\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"date\", \"abstract\", \"journal\", \"doi\"],\n    start_date: str = \"None\",\n    end_date: str = \"None\",\n    *args,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Combines get_pubmed_papers and dump_papers.\n\n    Args:\n        keywords: List of keywords to request pubmed API.\n            The outer list level will be considered as AND separated keys.\n            The inner level as OR separated.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n            Defaults to ['title', 'authors', 'date', 'abstract',\n            'journal', 'doi'].\n            NOTE: If 'emails' is passed, an attempt is made to extract author mail\n            addresses.\n        start_date: Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date: End date for the search. Same notation as start_date.\n    \"\"\"\n    # Translate keywords into query.\n    query = get_query_from_keywords_and_date(\n        keywords, start_date=start_date, end_date=end_date\n    )\n    papers = get_pubmed_papers(query, fields, *args, **kwargs)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.utils","title":"<code>utils</code>","text":""},{"location":"api/pubmed/#paperscraper.pubmed.utils.get_query_from_keywords","title":"<code>get_query_from_keywords(keywords: List[Union[str, List]]) -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the pubmed API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to pubmed API.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_query_from_keywords(keywords: List[Union[str, List]]) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the pubmed API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n\n    Returns:\n        str: query to enter to pubmed API.\n    \"\"\"\n\n    query = \"\"\n    for i, key in enumerate(keywords):\n        if isinstance(key, str):\n            query += f\"({key}) AND \"\n        elif isinstance(key, list):\n            inter = \"\".join([f\"({syn}) OR \" for syn in key])\n            query += finalize_disjunction(inter)\n\n    query = finalize_conjunction(query)\n    return query\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.utils.get_query_from_keywords_and_date","title":"<code>get_query_from_keywords_and_date(keywords: List[Union[str, List]], start_date: str = 'None', end_date: str = 'None') -&gt; str</code>","text":"<p>Receives a list of keywords and returns the query for the pubmed API.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>start_date</code> <code>str</code> <p>Start date for the search. Needs to be in format: YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific dates are used.</p> <code>'None'</code> <code>end_date</code> <code>str</code> <p>End date for the search. Same notation as start_date.</p> <code>'None'</code> If start_date and end_date are left as default, the function is <p>identical to get_query_from_keywords.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>query to enter to pubmed API.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_query_from_keywords_and_date(\n    keywords: List[Union[str, List]], start_date: str = \"None\", end_date: str = \"None\"\n) -&gt; str:\n    \"\"\"Receives a list of keywords and returns the query for the pubmed API.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        start_date (str): Start date for the search. Needs to be in format:\n            YYYY/MM/DD, e.g. '2020/07/20'. Defaults to 'None', i.e. no specific\n            dates are used.\n        end_date (str): End date for the search. Same notation as start_date.\n\n    Note: If start_date and end_date are left as default, the function is\n        identical to get_query_from_keywords.\n\n    Returns:\n        str: query to enter to pubmed API.\n    \"\"\"\n\n    query = get_query_from_keywords(keywords)\n\n    if start_date != \"None\" and end_date != \"None\":\n        date = date_root.format(start_date, end_date)\n    elif start_date != \"None\" and end_date == \"None\":\n        date = date_root.format(start_date, \"3000\")\n    elif start_date == \"None\" and end_date != \"None\":\n        date = date_root.format(\"1000\", end_date)\n    else:\n        return query\n\n    return query + \" AND \" + date\n</code></pre>"},{"location":"api/pubmed/#paperscraper.pubmed.utils.get_emails","title":"<code>get_emails(paper: PubMedArticle) -&gt; List</code>","text":"<p>Extracts author email addresses from PubMedArticle.</p> <p>Parameters:</p> Name Type Description Default <code>paper</code> <code>PubMedArticle</code> <p>An object of type PubMedArticle. Requires to have an 'author' field.</p> required <p>Returns:</p> Name Type Description <code>List</code> <code>List</code> <p>A possibly empty list of emails associated to authors of the paper.</p> Source code in <code>paperscraper/pubmed/utils.py</code> <pre><code>def get_emails(paper: PubMedArticle) -&gt; List:\n    \"\"\"\n    Extracts author email addresses from PubMedArticle.\n\n    Args:\n        paper (PubMedArticle): An object of type PubMedArticle. Requires to have\n            an 'author' field.\n\n    Returns:\n        List: A possibly empty list of emails associated to authors of the paper.\n    \"\"\"\n\n    emails = []\n    for author in paper.authors:\n        for v in author.values():\n            if v is not None and \"@\" in v:\n                parts = v.split(\"@\")\n                if len(parts) == 2:\n                    # Found one email address\n                    prefix = parts[0].split(\" \")[-1]\n                    postfix = parts[1]\n                    mail = prefix + \"@\" + postfix\n                    if not (postfix.endswith(\".\") or postfix.endswith(\" \")):\n                        emails.append(mail)\n                    else:\n                        emails.append(mail[:-1])\n                else:\n                    # Found multiple addresses\n                    for idx, part in enumerate(parts):\n                        try:\n                            if idx == 0:\n                                prefix = part.split(\" \")[-1]\n                            else:\n                                postfix = part.split(\"\\n\")[0]\n\n                                if postfix.endswith(\".\"):\n                                    postfix = postfix[:-1]\n                                    mail = prefix + \"@\" + postfix\n                                else:\n                                    current_postfix = postfix.split(\" \")[0]\n                                    mail = prefix + \"@\" + current_postfix\n                                    prefix = postfix.split(\" \")[1]\n                                emails.append(mail)\n                        except IndexError:\n                            warnings.warn(f\"Mail could not be inferred from {part}.\")\n\n    return list(set(emails))\n</code></pre>"},{"location":"api/scholar/","title":"paperscraper.scholar","text":""},{"location":"api/scholar/#paperscraper.scholar","title":"<code>paperscraper.scholar</code>","text":""},{"location":"api/scholar/#paperscraper.scholar.get_citations_from_title","title":"<code>get_citations_from_title(title: str) -&gt; int</code>","text":"<p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Title of paper to be searched on Scholar.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If sth else than str is passed.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of citations of paper.</p> Source code in <code>paperscraper/citations/citations.py</code> <pre><code>def get_citations_from_title(title: str) -&gt; int:\n    \"\"\"\n    Args:\n        title (str): Title of paper to be searched on Scholar.\n\n    Raises:\n        TypeError: If sth else than str is passed.\n\n    Returns:\n        int: Number of citations of paper.\n    \"\"\"\n\n    if not isinstance(title, str):\n        raise TypeError(f\"Pass str not {type(title)}\")\n\n    # Search for exact match\n    title = '\"' + title.strip() + '\"'\n\n    matches = scholarly.search_pubs(title)\n    counts = list(map(lambda p: int(p[\"num_citations\"]), matches))\n    if len(counts) == 0:\n        logger.warning(f\"Found no match for {title}.\")\n        return 0\n    if len(counts) &gt; 1:\n        logger.warning(f\"Found {len(counts)} matches for {title}, returning first one.\")\n    return counts[0]\n</code></pre>"},{"location":"api/scholar/#paperscraper.scholar.dump_papers","title":"<code>dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None</code>","text":"<p>Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl file with one paper per line.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>DataFrame</code> <p>A dataframe of paper metadata, one paper per row.</p> required <code>filepath</code> <code>str</code> <p>Path to dump the papers, has to end with <code>.jsonl</code>.</p> required Source code in <code>paperscraper/utils.py</code> <pre><code>def dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None:\n    \"\"\"\n    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl\n    file with one paper per line.\n\n    Args:\n        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.\n        filepath (str): Path to dump the papers, has to end with `.jsonl`.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}\")\n    if not filepath.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a filepath with .jsonl extension\")\n\n    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):\n        papers = pd.DataFrame(papers)\n        logger.warning(\n            \"Preferably pass a pd.DataFrame, not a list of dictionaries. \"\n            \"Passing a list is a legacy functionality that might become deprecated.\"\n        )\n\n    if not isinstance(papers, pd.DataFrame):\n        raise TypeError(f\"papers must be a pd.DataFrame, not {type(papers)}\")\n\n    paper_list = list(papers.T.to_dict().values())\n\n    with open(filepath, \"w\") as f:\n        for paper in paper_list:\n            f.write(json.dumps(paper) + \"\\n\")\n</code></pre>"},{"location":"api/scholar/#paperscraper.scholar.get_scholar_papers","title":"<code>get_scholar_papers(title: str, fields: List = ['title', 'authors', 'year', 'abstract', 'journal', 'citations'], *args, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Performs Google Scholar API request of a given title and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Query to arxiv API. Needs to match the arxiv API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'year', 'abstract', 'journal', 'citations']</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame. One paper per row.</p> Source code in <code>paperscraper/scholar/scholar.py</code> <pre><code>def get_scholar_papers(\n    title: str,\n    fields: List = [\"title\", \"authors\", \"year\", \"abstract\", \"journal\", \"citations\"],\n    *args,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs Google Scholar API request of a given title and returns list of papers with\n    fields as desired.\n\n    Args:\n        title: Query to arxiv API. Needs to match the arxiv API notation.\n        fields: List of strings with fields to keep in output.\n\n    Returns:\n        pd.DataFrame. One paper per row.\n\n    \"\"\"\n    logger.info(\n        \"NOTE: Scholar API cannot be used with Boolean logic in keywords.\"\n        \"Query should be a single string to be entered in the Scholar search field.\"\n    )\n    if not isinstance(title, str):\n        raise TypeError(f\"Pass str not {type(title)}\")\n\n    matches = scholarly.search_pubs(title)\n\n    processed = []\n    for paper in matches:\n        # Extracts title, author, year, journal, abstract\n        entry = {\n            scholar_field_mapper.get(key, key): process_fields.get(\n                scholar_field_mapper.get(key, key), lambda x: x\n            )(value)\n            for key, value in paper[\"bib\"].items()\n            if scholar_field_mapper.get(key, key) in fields\n        }\n\n        entry[\"citations\"] = paper[\"num_citations\"]\n        processed.append(entry)\n\n    return pd.DataFrame(processed)\n</code></pre>"},{"location":"api/scholar/#paperscraper.scholar.get_and_dump_scholar_papers","title":"<code>get_and_dump_scholar_papers(title: str, output_filepath: str, fields: List = ['title', 'authors', 'year', 'abstract', 'journal', 'citations']) -&gt; None</code>","text":"<p>Combines get_scholar_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Paper to search for on Google Scholar.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'year', 'abstract', 'journal', 'citations']</code> Source code in <code>paperscraper/scholar/scholar.py</code> <pre><code>def get_and_dump_scholar_papers(\n    title: str,\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"year\", \"abstract\", \"journal\", \"citations\"],\n) -&gt; None:\n    \"\"\"\n    Combines get_scholar_papers and dump_papers.\n\n    Args:\n        title: Paper to search for on Google Scholar.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n    \"\"\"\n    papers = get_scholar_papers(title, fields)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api/scholar/#paperscraper.scholar.scholar","title":"<code>scholar</code>","text":""},{"location":"api/scholar/#paperscraper.scholar.scholar.get_scholar_papers","title":"<code>get_scholar_papers(title: str, fields: List = ['title', 'authors', 'year', 'abstract', 'journal', 'citations'], *args, **kwargs) -&gt; pd.DataFrame</code>","text":"<p>Performs Google Scholar API request of a given title and returns list of papers with fields as desired.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Query to arxiv API. Needs to match the arxiv API notation.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'year', 'abstract', 'journal', 'citations']</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame. One paper per row.</p> Source code in <code>paperscraper/scholar/scholar.py</code> <pre><code>def get_scholar_papers(\n    title: str,\n    fields: List = [\"title\", \"authors\", \"year\", \"abstract\", \"journal\", \"citations\"],\n    *args,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Performs Google Scholar API request of a given title and returns list of papers with\n    fields as desired.\n\n    Args:\n        title: Query to arxiv API. Needs to match the arxiv API notation.\n        fields: List of strings with fields to keep in output.\n\n    Returns:\n        pd.DataFrame. One paper per row.\n\n    \"\"\"\n    logger.info(\n        \"NOTE: Scholar API cannot be used with Boolean logic in keywords.\"\n        \"Query should be a single string to be entered in the Scholar search field.\"\n    )\n    if not isinstance(title, str):\n        raise TypeError(f\"Pass str not {type(title)}\")\n\n    matches = scholarly.search_pubs(title)\n\n    processed = []\n    for paper in matches:\n        # Extracts title, author, year, journal, abstract\n        entry = {\n            scholar_field_mapper.get(key, key): process_fields.get(\n                scholar_field_mapper.get(key, key), lambda x: x\n            )(value)\n            for key, value in paper[\"bib\"].items()\n            if scholar_field_mapper.get(key, key) in fields\n        }\n\n        entry[\"citations\"] = paper[\"num_citations\"]\n        processed.append(entry)\n\n    return pd.DataFrame(processed)\n</code></pre>"},{"location":"api/scholar/#paperscraper.scholar.scholar.get_and_dump_scholar_papers","title":"<code>get_and_dump_scholar_papers(title: str, output_filepath: str, fields: List = ['title', 'authors', 'year', 'abstract', 'journal', 'citations']) -&gt; None</code>","text":"<p>Combines get_scholar_papers and dump_papers.</p> <p>Parameters:</p> Name Type Description Default <code>title</code> <code>str</code> <p>Paper to search for on Google Scholar.</p> required <code>output_filepath</code> <code>str</code> <p>Path where the dump will be saved.</p> required <code>fields</code> <code>List</code> <p>List of strings with fields to keep in output.</p> <code>['title', 'authors', 'year', 'abstract', 'journal', 'citations']</code> Source code in <code>paperscraper/scholar/scholar.py</code> <pre><code>def get_and_dump_scholar_papers(\n    title: str,\n    output_filepath: str,\n    fields: List = [\"title\", \"authors\", \"year\", \"abstract\", \"journal\", \"citations\"],\n) -&gt; None:\n    \"\"\"\n    Combines get_scholar_papers and dump_papers.\n\n    Args:\n        title: Paper to search for on Google Scholar.\n        output_filepath: Path where the dump will be saved.\n        fields: List of strings with fields to keep in output.\n    \"\"\"\n    papers = get_scholar_papers(title, fields)\n    dump_papers(papers, output_filepath)\n</code></pre>"},{"location":"api/xrxiv/","title":"paperscraper.xrxiv","text":""},{"location":"api/xrxiv/#paperscraper.xrxiv","title":"<code>paperscraper.xrxiv</code>","text":"<p>bioRxiv and medRxiv utilities.</p>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_api","title":"<code>xrxiv_api</code>","text":"<p>API for bioRxiv and medRXiv.</p>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_api.XRXivApi","title":"<code>XRXivApi</code>","text":"<p>API class.</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>class XRXivApi:\n    \"\"\"API class.\"\"\"\n\n    def __init__(\n        self,\n        server: str,\n        launch_date: str,\n        api_base_url: str = \"https://api.biorxiv.org\",\n        max_retries: int = 10,\n    ):\n        \"\"\"\n        Initialize API class.\n\n        Args:\n            server (str): name of the preprint server to access.\n            launch_date (str): launch date expressed as YYYY-MM-DD.\n            api_base_url (str, optional): Base url for the API. Defaults to 'api.biorxiv.org'.\n            max_retries (int, optional): Maximal number of retries for a request before an\n                error is raised. Defaults to 10.\n        \"\"\"\n        self.server = server\n        self.api_base_url = api_base_url\n        self.launch_date = launch_date\n        self.launch_datetime = datetime.fromisoformat(self.launch_date)\n        self.get_papers_url = (\n            \"{}/details/{}\".format(self.api_base_url, self.server)\n            + \"/{start_date}/{end_date}/{cursor}\"\n        )\n        self.max_retries = max_retries\n\n    @retry_multi()\n    def call_api(self, start_date, end_date, cursor):\n        try:\n            json_response = requests.get(\n                self.get_papers_url.format(\n                    start_date=start_date, end_date=end_date, cursor=cursor\n                ),\n                timeout=10,\n            ).json()\n        except requests.exceptions.Timeout:\n            logger.info(\"Timed out, will retry\")\n            return None\n\n        return json_response\n\n    def get_papers(\n        self,\n        start_date: Optional[str] = None,\n        end_date: Optional[str] = None,\n        fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n        max_retries: int = 10,\n    ) -&gt; Generator:\n        \"\"\"\n        Get paper metadata.\n\n        Args:\n            start_date (Optional[str]): begin date. Defaults to None, a.k.a. launch date.\n            end_date (Optional[str]): end date. Defaults to None, a.k.a. today.\n            fields (List[str], optional): fields to return per paper.\n                Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n            max_retries (int): Number of retries on connection failure. Defaults to 10.\n\n        Yields:\n            Generator: a generator of paper metadata (dict) with the desired fields.\n        \"\"\"\n        try:\n            now_datetime = datetime.now()\n            if start_date:\n                start_datetime = datetime.fromisoformat(start_date)\n                if start_datetime &lt; self.launch_datetime:\n                    start_date = self.launch_date\n            else:\n                start_date = self.launch_date\n            if end_date:\n                end_datetime = datetime.fromisoformat(end_date)\n                if end_datetime &gt; now_datetime:\n                    end_date = now_datetime.strftime(\"%Y-%m-%d\")\n            else:\n                end_date = now_datetime.strftime(\"%Y-%m-%d\")\n            do_loop = True\n            cursor = 0\n            while do_loop:\n                papers = []\n                for attempt in range(max_retries):\n                    try:\n                        json_response = self.call_api(start_date, end_date, cursor)\n                        do_loop = json_response[\"messages\"][0][\"status\"] == \"ok\"\n                        if do_loop:\n                            cursor += json_response[\"messages\"][0][\"count\"]\n                            for paper in json_response[\"collection\"]:\n                                processed_paper = {\n                                    field: paper.get(field, \"\") for field in fields\n                                }\n                                papers.append(processed_paper)\n\n                        if do_loop:\n                            yield from papers\n                            break\n                    except (ConnectionError, Timeout) as e:\n                        logger.error(\n                            f\"Connection error: {e}. Retrying ({attempt + 1}/{max_retries})\"\n                        )\n                        sleep(5)\n                        continue\n                    except Exception as exc:\n                        logger.exception(f\"Failed getting papers: {exc}\")\n        except Exception as exc:\n            logger.exception(f\"Failed getting papers: {exc}\")\n</code></pre>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_api.XRXivApi.__init__","title":"<code>__init__(server: str, launch_date: str, api_base_url: str = 'https://api.biorxiv.org', max_retries: int = 10)</code>","text":"<p>Initialize API class.</p> <p>Parameters:</p> Name Type Description Default <code>server</code> <code>str</code> <p>name of the preprint server to access.</p> required <code>launch_date</code> <code>str</code> <p>launch date expressed as YYYY-MM-DD.</p> required <code>api_base_url</code> <code>str</code> <p>Base url for the API. Defaults to 'api.biorxiv.org'.</p> <code>'https://api.biorxiv.org'</code> <code>max_retries</code> <code>int</code> <p>Maximal number of retries for a request before an error is raised. Defaults to 10.</p> <code>10</code> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>def __init__(\n    self,\n    server: str,\n    launch_date: str,\n    api_base_url: str = \"https://api.biorxiv.org\",\n    max_retries: int = 10,\n):\n    \"\"\"\n    Initialize API class.\n\n    Args:\n        server (str): name of the preprint server to access.\n        launch_date (str): launch date expressed as YYYY-MM-DD.\n        api_base_url (str, optional): Base url for the API. Defaults to 'api.biorxiv.org'.\n        max_retries (int, optional): Maximal number of retries for a request before an\n            error is raised. Defaults to 10.\n    \"\"\"\n    self.server = server\n    self.api_base_url = api_base_url\n    self.launch_date = launch_date\n    self.launch_datetime = datetime.fromisoformat(self.launch_date)\n    self.get_papers_url = (\n        \"{}/details/{}\".format(self.api_base_url, self.server)\n        + \"/{start_date}/{end_date}/{cursor}\"\n    )\n    self.max_retries = max_retries\n</code></pre>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_api.XRXivApi.get_papers","title":"<code>get_papers(start_date: Optional[str] = None, end_date: Optional[str] = None, fields: List[str] = ['title', 'doi', 'authors', 'abstract', 'date', 'journal'], max_retries: int = 10) -&gt; Generator</code>","text":"<p>Get paper metadata.</p> <p>Parameters:</p> Name Type Description Default <code>start_date</code> <code>Optional[str]</code> <p>begin date. Defaults to None, a.k.a. launch date.</p> <code>None</code> <code>end_date</code> <code>Optional[str]</code> <p>end date. Defaults to None, a.k.a. today.</p> <code>None</code> <code>fields</code> <code>List[str]</code> <p>fields to return per paper. Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].</p> <code>['title', 'doi', 'authors', 'abstract', 'date', 'journal']</code> <code>max_retries</code> <code>int</code> <p>Number of retries on connection failure. Defaults to 10.</p> <code>10</code> <p>Yields:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>a generator of paper metadata (dict) with the desired fields.</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>def get_papers(\n    self,\n    start_date: Optional[str] = None,\n    end_date: Optional[str] = None,\n    fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n    max_retries: int = 10,\n) -&gt; Generator:\n    \"\"\"\n    Get paper metadata.\n\n    Args:\n        start_date (Optional[str]): begin date. Defaults to None, a.k.a. launch date.\n        end_date (Optional[str]): end date. Defaults to None, a.k.a. today.\n        fields (List[str], optional): fields to return per paper.\n            Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n        max_retries (int): Number of retries on connection failure. Defaults to 10.\n\n    Yields:\n        Generator: a generator of paper metadata (dict) with the desired fields.\n    \"\"\"\n    try:\n        now_datetime = datetime.now()\n        if start_date:\n            start_datetime = datetime.fromisoformat(start_date)\n            if start_datetime &lt; self.launch_datetime:\n                start_date = self.launch_date\n        else:\n            start_date = self.launch_date\n        if end_date:\n            end_datetime = datetime.fromisoformat(end_date)\n            if end_datetime &gt; now_datetime:\n                end_date = now_datetime.strftime(\"%Y-%m-%d\")\n        else:\n            end_date = now_datetime.strftime(\"%Y-%m-%d\")\n        do_loop = True\n        cursor = 0\n        while do_loop:\n            papers = []\n            for attempt in range(max_retries):\n                try:\n                    json_response = self.call_api(start_date, end_date, cursor)\n                    do_loop = json_response[\"messages\"][0][\"status\"] == \"ok\"\n                    if do_loop:\n                        cursor += json_response[\"messages\"][0][\"count\"]\n                        for paper in json_response[\"collection\"]:\n                            processed_paper = {\n                                field: paper.get(field, \"\") for field in fields\n                            }\n                            papers.append(processed_paper)\n\n                    if do_loop:\n                        yield from papers\n                        break\n                except (ConnectionError, Timeout) as e:\n                    logger.error(\n                        f\"Connection error: {e}. Retrying ({attempt + 1}/{max_retries})\"\n                    )\n                    sleep(5)\n                    continue\n                except Exception as exc:\n                    logger.exception(f\"Failed getting papers: {exc}\")\n    except Exception as exc:\n        logger.exception(f\"Failed getting papers: {exc}\")\n</code></pre>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_api.BioRxivApi","title":"<code>BioRxivApi</code>","text":"<p>               Bases: <code>XRXivApi</code></p> <p>bioRxiv API.</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>class BioRxivApi(XRXivApi):\n    \"\"\"bioRxiv API.\"\"\"\n\n    def __init__(self, max_retries: int = 10):\n        super().__init__(\n            server=\"biorxiv\",\n            launch_date=launch_dates[\"biorxiv\"],\n            max_retries=max_retries,\n        )\n</code></pre>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_api.MedRxivApi","title":"<code>MedRxivApi</code>","text":"<p>               Bases: <code>XRXivApi</code></p> <p>medRxiv API.</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>class MedRxivApi(XRXivApi):\n    \"\"\"medRxiv API.\"\"\"\n\n    def __init__(self, max_retries: int = 10):\n        super().__init__(\n            server=\"medrxiv\",\n            launch_date=launch_dates[\"medrxiv\"],\n            max_retries=max_retries,\n        )\n</code></pre>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_api.retry_multi","title":"<code>retry_multi()</code>","text":"<p>Retry a function several times</p> Source code in <code>paperscraper/xrxiv/xrxiv_api.py</code> <pre><code>def retry_multi():\n    \"\"\"Retry a function several times\"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            num_retries = 0\n            max_retries = getattr(self, \"max_retries\", 10)\n            while num_retries &lt;= max_retries:\n                try:\n                    ret = func(self, *args, **kwargs)\n                    if ret is None:\n                        time.sleep(5)\n                        continue\n                    break\n                except HTTPError:\n                    if num_retries == max_retries:\n                        raise\n                    num_retries += 1\n                    time.sleep(5)\n            return ret\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_query","title":"<code>xrxiv_query</code>","text":"<p>Query dumps from bioRxiv and medRXiv.</p>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_query.XRXivQuery","title":"<code>XRXivQuery</code>","text":"<p>Query class.</p> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>class XRXivQuery:\n    \"\"\"Query class.\"\"\"\n\n    def __init__(\n        self,\n        dump_filepath: str,\n        fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n    ):\n        \"\"\"\n        Initialize the query class.\n\n        Args:\n            dump_filepath (str): filepath to the dump to be queried.\n            fields (List[str], optional): fields to contained in the dump per paper.\n                Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n        \"\"\"\n        self.dump_filepath = dump_filepath\n        self.fields = fields\n        self.errored = False\n\n        try:\n            self.df = pd.read_json(self.dump_filepath, lines=True)\n            self.df[\"date\"] = [date.strftime(\"%Y-%m-%d\") for date in self.df[\"date\"]]\n        except ValueError as e:\n            logger.warning(f\"Problem in reading file {dump_filepath}: {e} - Skipping!\")\n            self.errored = True\n        except KeyError as e:\n            logger.warning(f\"Key {e} missing in file from {dump_filepath} - Skipping!\")\n            self.errored = True\n\n    def search_keywords(\n        self,\n        keywords: List[Union[str, List[str]]],\n        fields: List[str] = None,\n        output_filepath: str = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Search for papers in the dump using keywords.\n\n        Args:\n            keywords (List[str, List[str]]): Items will be AND separated. If items\n                are lists themselves, they will be OR separated.\n            fields (List[str], optional): fields to be used in the query search.\n                Defaults to None, a.k.a. search in all fields excluding date.\n            output_filepath (str, optional): optional output filepath where to store\n                the hits in JSONL format. Defaults to None, a.k.a., no export to a file.\n\n        Returns:\n            pd.DataFrame: A dataframe with one paper per row.\n        \"\"\"\n        if fields is None:\n            fields = self.fields\n        fields = [field for field in fields if field != \"date\"]\n        hits_per_field = []\n        for field in fields:\n            field_data = self.df[field].str.lower()\n            hits_per_keyword = []\n            for keyword in keywords:\n                if isinstance(keyword, list):\n                    query = \"|\".join([_.lower() for _ in keyword])\n                else:\n                    query = keyword.lower()\n                hits_per_keyword.append(field_data.str.contains(query))\n            if len(hits_per_keyword):\n                keyword_hits = hits_per_keyword[0]\n                for single_keyword_hits in hits_per_keyword[1:]:\n                    keyword_hits &amp;= single_keyword_hits\n                hits_per_field.append(keyword_hits)\n        if len(hits_per_field):\n            hits = hits_per_field[0]\n            for single_hits in hits_per_field[1:]:\n                hits |= single_hits\n        if output_filepath is not None:\n            self.df[hits].to_json(output_filepath, orient=\"records\", lines=True)\n        return self.df[hits]\n</code></pre>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_query.XRXivQuery.__init__","title":"<code>__init__(dump_filepath: str, fields: List[str] = ['title', 'doi', 'authors', 'abstract', 'date', 'journal'])</code>","text":"<p>Initialize the query class.</p> <p>Parameters:</p> Name Type Description Default <code>dump_filepath</code> <code>str</code> <p>filepath to the dump to be queried.</p> required <code>fields</code> <code>List[str]</code> <p>fields to contained in the dump per paper. Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].</p> <code>['title', 'doi', 'authors', 'abstract', 'date', 'journal']</code> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>def __init__(\n    self,\n    dump_filepath: str,\n    fields: List[str] = [\"title\", \"doi\", \"authors\", \"abstract\", \"date\", \"journal\"],\n):\n    \"\"\"\n    Initialize the query class.\n\n    Args:\n        dump_filepath (str): filepath to the dump to be queried.\n        fields (List[str], optional): fields to contained in the dump per paper.\n            Defaults to ['title', 'doi', 'authors', 'abstract', 'date', 'journal'].\n    \"\"\"\n    self.dump_filepath = dump_filepath\n    self.fields = fields\n    self.errored = False\n\n    try:\n        self.df = pd.read_json(self.dump_filepath, lines=True)\n        self.df[\"date\"] = [date.strftime(\"%Y-%m-%d\") for date in self.df[\"date\"]]\n    except ValueError as e:\n        logger.warning(f\"Problem in reading file {dump_filepath}: {e} - Skipping!\")\n        self.errored = True\n    except KeyError as e:\n        logger.warning(f\"Key {e} missing in file from {dump_filepath} - Skipping!\")\n        self.errored = True\n</code></pre>"},{"location":"api/xrxiv/#paperscraper.xrxiv.xrxiv_query.XRXivQuery.search_keywords","title":"<code>search_keywords(keywords: List[Union[str, List[str]]], fields: List[str] = None, output_filepath: str = None) -&gt; pd.DataFrame</code>","text":"<p>Search for papers in the dump using keywords.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>List[str, List[str]]</code> <p>Items will be AND separated. If items are lists themselves, they will be OR separated.</p> required <code>fields</code> <code>List[str]</code> <p>fields to be used in the query search. Defaults to None, a.k.a. search in all fields excluding date.</p> <code>None</code> <code>output_filepath</code> <code>str</code> <p>optional output filepath where to store the hits in JSONL format. Defaults to None, a.k.a., no export to a file.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A dataframe with one paper per row.</p> Source code in <code>paperscraper/xrxiv/xrxiv_query.py</code> <pre><code>def search_keywords(\n    self,\n    keywords: List[Union[str, List[str]]],\n    fields: List[str] = None,\n    output_filepath: str = None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Search for papers in the dump using keywords.\n\n    Args:\n        keywords (List[str, List[str]]): Items will be AND separated. If items\n            are lists themselves, they will be OR separated.\n        fields (List[str], optional): fields to be used in the query search.\n            Defaults to None, a.k.a. search in all fields excluding date.\n        output_filepath (str, optional): optional output filepath where to store\n            the hits in JSONL format. Defaults to None, a.k.a., no export to a file.\n\n    Returns:\n        pd.DataFrame: A dataframe with one paper per row.\n    \"\"\"\n    if fields is None:\n        fields = self.fields\n    fields = [field for field in fields if field != \"date\"]\n    hits_per_field = []\n    for field in fields:\n        field_data = self.df[field].str.lower()\n        hits_per_keyword = []\n        for keyword in keywords:\n            if isinstance(keyword, list):\n                query = \"|\".join([_.lower() for _ in keyword])\n            else:\n                query = keyword.lower()\n            hits_per_keyword.append(field_data.str.contains(query))\n        if len(hits_per_keyword):\n            keyword_hits = hits_per_keyword[0]\n            for single_keyword_hits in hits_per_keyword[1:]:\n                keyword_hits &amp;= single_keyword_hits\n            hits_per_field.append(keyword_hits)\n    if len(hits_per_field):\n        hits = hits_per_field[0]\n        for single_hits in hits_per_field[1:]:\n            hits |= single_hits\n    if output_filepath is not None:\n        self.df[hits].to_json(output_filepath, orient=\"records\", lines=True)\n    return self.df[hits]\n</code></pre>"},{"location":"api/paperscraper/","title":"paperscraper (top-level utilities)","text":"<p>Below are the modules that live directly under <code>paperscraper/</code> (not in subpackages):</p>"},{"location":"api/paperscraper/#async_utils","title":"async_utils","text":""},{"location":"api/paperscraper/#paperscraper.async_utils","title":"<code>paperscraper.async_utils</code>","text":""},{"location":"api/paperscraper/#paperscraper.async_utils.optional_async","title":"<code>optional_async(func: Callable[..., Awaitable[T]]) -&gt; Callable[..., Union[T, Awaitable[T]]]</code>","text":"<p>Allows an async function to be called from sync code (blocks until done) or from within an async context (returns a coroutine to await).</p> Source code in <code>paperscraper/async_utils.py</code> <pre><code>def optional_async(\n    func: Callable[..., Awaitable[T]],\n) -&gt; Callable[..., Union[T, Awaitable[T]]]:\n    \"\"\"\n    Allows an async function to be called from sync code (blocks until done)\n    or from within an async context (returns a coroutine to await).\n    \"\"\"\n\n    @wraps(func)\n    def wrapper(*args, **kwargs) -&gt; Union[T, Awaitable[T]]:\n        coro = func(*args, **kwargs)\n        try:\n            # If we're already in an asyncio loop, hand back the coroutine:\n            asyncio.get_running_loop()\n            return coro  # caller must await it\n        except RuntimeError:\n            # Otherwise, schedule on the background loop and block\n            future = asyncio.run_coroutine_threadsafe(coro, _background_loop)\n            return future.result()\n\n    return wrapper\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.async_utils.retry_with_exponential_backoff","title":"<code>retry_with_exponential_backoff(*, max_retries: int = 5, base_delay: float = 1.0) -&gt; Callable[[F], F]</code>","text":"<p>Decorator factory that retries an <code>async def</code> on HTTP 429, with exponential backoff.</p> <p>Parameters:</p> Name Type Description Default <code>max_retries</code> <code>int</code> <p>how many times to retry before giving up.</p> <code>5</code> <code>base_delay</code> <code>float</code> <p>initial delay in seconds; next delays will be duplication of previous.</p> <code>1.0</code> <pre><code>@retry_with_exponential_backoff(max_retries=3, base_delay=0.5)\nasync def fetch_data(...):\n    ...\n</code></pre> Source code in <code>paperscraper/async_utils.py</code> <pre><code>def retry_with_exponential_backoff(\n    *, max_retries: int = 5, base_delay: float = 1.0\n) -&gt; Callable[[F], F]:\n    \"\"\"\n    Decorator factory that retries an `async def` on HTTP 429, with exponential backoff.\n\n    Args:\n        max_retries: how many times to retry before giving up.\n        base_delay: initial delay in seconds; next delays will be duplication of previous.\n\n    Usage:\n\n        @retry_with_exponential_backoff(max_retries=3, base_delay=0.5)\n        async def fetch_data(...):\n            ...\n\n    \"\"\"\n\n    def decorator(func: F) -&gt; F:\n        @wraps(func)\n        async def wrapper(*args, **kwargs) -&gt; Any:\n            delay = base_delay\n            for attempt in range(max_retries):\n                try:\n                    return await func(*args, **kwargs)\n                except httpx.HTTPStatusError as e:\n                    # only retry on 429\n                    status = e.response.status_code if e.response is not None else None\n                    if status != 429 or attempt == max_retries - 1:\n                        raise\n                # backoff\n                await asyncio.sleep(delay)\n                delay *= 2\n            # in theory we never reach here\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/paperscraper/#impact","title":"impact","text":""},{"location":"api/paperscraper/#paperscraper.impact","title":"<code>paperscraper.impact</code>","text":""},{"location":"api/paperscraper/#paperscraper.impact.Impactor","title":"<code>Impactor</code>","text":"Source code in <code>paperscraper/impact.py</code> <pre><code>class Impactor:\n    def __init__(self):\n        \"\"\"\n        Initialize the Impactor class with an instance of the Factor class.\n        This allows access to the database of journal impact factors.\n        \"\"\"\n        self.fa = Factor()\n        self.all_journals = self.fa.search(\"%\")\n        self.metadata = pd.DataFrame(self.all_journals, dtype=str)\n        logger.info(f\"Loaded metadata for {len(self.metadata)} journals\")\n\n    def search(\n        self,\n        query: str,\n        threshold: int = 100,\n        sort_by: Optional[str] = None,\n        min_impact: float = 0.0,\n        max_impact: float = float(\"inf\"),\n        return_all: bool = False,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Search for journals matching the given query with an optional fuzziness\n            level and sorting.\n\n        Args:\n            query: The journal name or abbreviation to search for.\n            threshold: The threshold for fuzzy matching. If set to 100, exact matching\n                is performed. If set below 100, fuzzy matching is used. Defaults to 100.\n            sort_by: Criterion for sorting results, one of 'impact', 'journal' and 'score'.\n            min_impact: Minimum impact factor for journals to be considered, defaults to 0.\n            max_impact: Maximum impact factor for journals to be considered, defaults to infinity.\n            return_all: If True, returns all columns of the DataFrame for each match.\n\n        Returns:\n            List[dict]: A list of dictionaries containing the journal information.\n\n        \"\"\"\n        # Validation of parameters\n        if not isinstance(query, str) or not isinstance(threshold, int):\n            raise TypeError(\n                f\"Query must be a str and threshold must be an int, not {type(query)} and {type(threshold)}\"\n            )\n        if threshold &lt; 0 or threshold &gt; 100:\n            raise ValueError(\n                f\"Fuzziness threshold must be between 0 and 100, not {threshold}\"\n            )\n\n        if str.isdigit(query) and threshold &gt;= 100:\n            # When querying with NLM ID, exact matching does not work since impact_factor\n            # strips off leading zeros, so we use fuzzy matching instead\n            threshold = 99\n\n        # Define a function to calculate fuzziness score\n        def calculate_fuzziness_score(row):\n            return max(fuzz.partial_ratio(query, str(value)) for value in row.values)\n\n        # Search with or without fuzzy matching\n        if threshold &gt;= 100:\n            matched_df = self.metadata[\n                self.metadata.apply(\n                    lambda x: query.lower() in x.astype(str).str.lower().values, axis=1\n                )\n            ].copy()\n            # Exact matches get a default score of 100\n            matched_df[\"score\"] = 100\n        else:\n            matched_df = self.metadata[\n                self.metadata.apply(\n                    lambda x: calculate_fuzziness_score(x) &gt;= threshold, axis=1\n                )\n            ].copy()\n            matched_df[\"score\"] = matched_df.apply(calculate_fuzziness_score, axis=1)\n\n        # Sorting based on the specified criterion\n        if sort_by == \"score\":\n            matched_df = matched_df.sort_values(by=\"score\", ascending=False)\n        elif sort_by == \"journal\":\n            matched_df = matched_df.sort_values(by=\"journal\")\n        elif sort_by == \"impact\":\n            matched_df = matched_df.sort_values(by=\"factor\", ascending=False)\n\n        matched_df[\"factor\"] = pd.to_numeric(matched_df[\"factor\"])\n        matched_df = matched_df[\n            (matched_df[\"factor\"] &gt;= min_impact) &amp; (matched_df[\"factor\"] &lt;= max_impact)\n        ]\n\n        # Prepare the final result\n        results = [\n            (\n                row.to_dict()\n                if return_all\n                else {\n                    \"journal\": row[\"journal\"],\n                    \"factor\": row[\"factor\"],\n                    \"score\": row[\"score\"],\n                }\n            )\n            for _, row in matched_df.iterrows()\n        ]\n\n        return results\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.impact.Impactor.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the Impactor class with an instance of the Factor class. This allows access to the database of journal impact factors.</p> Source code in <code>paperscraper/impact.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initialize the Impactor class with an instance of the Factor class.\n    This allows access to the database of journal impact factors.\n    \"\"\"\n    self.fa = Factor()\n    self.all_journals = self.fa.search(\"%\")\n    self.metadata = pd.DataFrame(self.all_journals, dtype=str)\n    logger.info(f\"Loaded metadata for {len(self.metadata)} journals\")\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.impact.Impactor.search","title":"<code>search(query: str, threshold: int = 100, sort_by: Optional[str] = None, min_impact: float = 0.0, max_impact: float = float('inf'), return_all: bool = False) -&gt; List[Dict[str, Any]]</code>","text":"<p>Search for journals matching the given query with an optional fuzziness     level and sorting.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The journal name or abbreviation to search for.</p> required <code>threshold</code> <code>int</code> <p>The threshold for fuzzy matching. If set to 100, exact matching is performed. If set below 100, fuzzy matching is used. Defaults to 100.</p> <code>100</code> <code>sort_by</code> <code>Optional[str]</code> <p>Criterion for sorting results, one of 'impact', 'journal' and 'score'.</p> <code>None</code> <code>min_impact</code> <code>float</code> <p>Minimum impact factor for journals to be considered, defaults to 0.</p> <code>0.0</code> <code>max_impact</code> <code>float</code> <p>Maximum impact factor for journals to be considered, defaults to infinity.</p> <code>float('inf')</code> <code>return_all</code> <code>bool</code> <p>If True, returns all columns of the DataFrame for each match.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List[dict]: A list of dictionaries containing the journal information.</p> Source code in <code>paperscraper/impact.py</code> <pre><code>def search(\n    self,\n    query: str,\n    threshold: int = 100,\n    sort_by: Optional[str] = None,\n    min_impact: float = 0.0,\n    max_impact: float = float(\"inf\"),\n    return_all: bool = False,\n) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Search for journals matching the given query with an optional fuzziness\n        level and sorting.\n\n    Args:\n        query: The journal name or abbreviation to search for.\n        threshold: The threshold for fuzzy matching. If set to 100, exact matching\n            is performed. If set below 100, fuzzy matching is used. Defaults to 100.\n        sort_by: Criterion for sorting results, one of 'impact', 'journal' and 'score'.\n        min_impact: Minimum impact factor for journals to be considered, defaults to 0.\n        max_impact: Maximum impact factor for journals to be considered, defaults to infinity.\n        return_all: If True, returns all columns of the DataFrame for each match.\n\n    Returns:\n        List[dict]: A list of dictionaries containing the journal information.\n\n    \"\"\"\n    # Validation of parameters\n    if not isinstance(query, str) or not isinstance(threshold, int):\n        raise TypeError(\n            f\"Query must be a str and threshold must be an int, not {type(query)} and {type(threshold)}\"\n        )\n    if threshold &lt; 0 or threshold &gt; 100:\n        raise ValueError(\n            f\"Fuzziness threshold must be between 0 and 100, not {threshold}\"\n        )\n\n    if str.isdigit(query) and threshold &gt;= 100:\n        # When querying with NLM ID, exact matching does not work since impact_factor\n        # strips off leading zeros, so we use fuzzy matching instead\n        threshold = 99\n\n    # Define a function to calculate fuzziness score\n    def calculate_fuzziness_score(row):\n        return max(fuzz.partial_ratio(query, str(value)) for value in row.values)\n\n    # Search with or without fuzzy matching\n    if threshold &gt;= 100:\n        matched_df = self.metadata[\n            self.metadata.apply(\n                lambda x: query.lower() in x.astype(str).str.lower().values, axis=1\n            )\n        ].copy()\n        # Exact matches get a default score of 100\n        matched_df[\"score\"] = 100\n    else:\n        matched_df = self.metadata[\n            self.metadata.apply(\n                lambda x: calculate_fuzziness_score(x) &gt;= threshold, axis=1\n            )\n        ].copy()\n        matched_df[\"score\"] = matched_df.apply(calculate_fuzziness_score, axis=1)\n\n    # Sorting based on the specified criterion\n    if sort_by == \"score\":\n        matched_df = matched_df.sort_values(by=\"score\", ascending=False)\n    elif sort_by == \"journal\":\n        matched_df = matched_df.sort_values(by=\"journal\")\n    elif sort_by == \"impact\":\n        matched_df = matched_df.sort_values(by=\"factor\", ascending=False)\n\n    matched_df[\"factor\"] = pd.to_numeric(matched_df[\"factor\"])\n    matched_df = matched_df[\n        (matched_df[\"factor\"] &gt;= min_impact) &amp; (matched_df[\"factor\"] &lt;= max_impact)\n    ]\n\n    # Prepare the final result\n    results = [\n        (\n            row.to_dict()\n            if return_all\n            else {\n                \"journal\": row[\"journal\"],\n                \"factor\": row[\"factor\"],\n                \"score\": row[\"score\"],\n            }\n        )\n        for _, row in matched_df.iterrows()\n    ]\n\n    return results\n</code></pre>"},{"location":"api/paperscraper/#load_dumps","title":"load_dumps","text":""},{"location":"api/paperscraper/#paperscraper.load_dumps","title":"<code>paperscraper.load_dumps</code>","text":""},{"location":"api/paperscraper/#plotting","title":"plotting","text":""},{"location":"api/paperscraper/#paperscraper.plotting","title":"<code>paperscraper.plotting</code>","text":""},{"location":"api/paperscraper/#paperscraper.plotting.plot_comparison","title":"<code>plot_comparison(data_dict: dict, keys: List[str], x_ticks: List[str] = ['2015', '2016', '2017', '2018', '2019', '2020'], show_preprint: bool = False, title_text: str = '', keyword_text: Optional[List[str]] = None, figpath: str = 'comparison_plot.pdf') -&gt; None</code>","text":"<p>Plot temporal evolution of number of papers per keyword</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>A dictionary with keywords as keys. Each value should be a dictionary itself, with keys for the different APIs. For example data_dict = {     'covid_19.jsonl': {         'pubmed': [0, 0, 0, 12345],         'arxiv': [0, 0, 0, 1234],         ...     }     'coronavirus.jsonl':         'pubmed': [234, 345, 456, 12345],         'arxiv': [123, 234, 345, 1234],         ...     } }</p> required <code>keys</code> <code>List[str]</code> <p>List of keys which should be plotted. This has to be a subset of data_dict.keys().</p> required <code>x_ticks</code> <code>List[str]</code> <p>List of strings to be used for the x-ticks. Should have same length as data_dict[key][database]. Defaults to ['2015', '2016', '2017', '2018', '2019', '2020'], meaning that papers are aggregated per year.</p> <code>['2015', '2016', '2017', '2018', '2019', '2020']</code> <code>show_preprint</code> <code>bool</code> <p>Whether preprint servers are aggregated or not. Defaults to False.</p> <code>False</code> <code>title_text</code> <code>str</code> <p>Title for the produced figure. Defaults to ''.</p> <code>''</code> <code>keyword_text</code> <code>Optional[List[str]]</code> <p>Figure caption per keyword. Defaults to None, i.e. empty strings will be used.</p> <code>None</code> <code>figpath</code> <code>str</code> <p>Name under which figure is saved. Relative or absolute paths can be given. Defaults to 'comparison_plot.pdf'.</p> <code>'comparison_plot.pdf'</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a database is missing in data_dict.</p> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_comparison(\n    data_dict: dict,\n    keys: List[str],\n    x_ticks: List[str] = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"],\n    show_preprint: bool = False,\n    title_text: str = \"\",\n    keyword_text: Optional[List[str]] = None,\n    figpath: str = \"comparison_plot.pdf\",\n) -&gt; None:\n    \"\"\"Plot temporal evolution of number of papers per keyword\n\n    Args:\n        data_dict: A dictionary with keywords as keys. Each value should be a\n            dictionary itself, with keys for the different APIs. For example\n            data_dict = {\n                'covid_19.jsonl': {\n                    'pubmed': [0, 0, 0, 12345],\n                    'arxiv': [0, 0, 0, 1234],\n                    ...\n                }\n                'coronavirus.jsonl':\n                    'pubmed': [234, 345, 456, 12345],\n                    'arxiv': [123, 234, 345, 1234],\n                    ...\n                }\n            }\n        keys: List of keys which should be plotted. This has to be a subset of data_dict.keys().\n        x_ticks: List of strings to be used for the x-ticks. Should have same length as\n            data_dict[key][database]. Defaults to ['2015', '2016', '2017', '2018', '2019', '2020'],\n            meaning that papers are aggregated per year.\n        show_preprint: Whether preprint servers are aggregated or not.\n            Defaults to False.\n        title_text: Title for the produced figure. Defaults to ''.\n        keyword_text: Figure caption per keyword. Defaults to None, i.e. empty strings will be used.\n        figpath: Name under which figure is saved. Relative or absolute\n            paths can be given. Defaults to 'comparison_plot.pdf'.\n\n    Raises:\n        KeyError: If a database is missing in data_dict.\n    \"\"\"\n\n    sns.set_palette(sns.color_palette(\"colorblind\", 10))\n    plt.rcParams.update({\"hatch.color\": \"w\"})\n    plt.rcParams[\"figure.facecolor\"] = \"white\"\n    plt.figure(figsize=(8, 5))\n\n    arxiv, biorxiv, pubmed, medrxiv, chemrxiv, preprint = [], [], [], [], [], []\n\n    for key in keys:\n        try:\n            arxiv.append(data_dict[key][\"arxiv\"])\n            biorxiv.append(data_dict[key][\"biorxiv\"])\n            medrxiv.append(data_dict[key][\"medrxiv\"])\n            chemrxiv.append(data_dict[key][\"chemrxiv\"])\n            pubmed.append(data_dict[key][\"pubmed\"])\n        except KeyError:\n            raise KeyError(\n                f\"Did not find all DBs for {key}, only found {data_dict[key].keys()}\"\n            )\n        preprint.append(arxiv[-1] + biorxiv[-1] + medrxiv[-1] + chemrxiv[-1])\n\n    ind = np.arange(len(arxiv[0]))  # the x locations for the groups\n    width = [0.2] * len(ind)  # the width of the bars: can also be len(x) sequence\n    if len(keys) == 2:\n        pos = [-0.2, 0.2]\n    elif len(keys) == 3:\n        pos = [-0.3, 0.0, 0.3]\n\n    plts = []\n    legend_plts = []\n    patterns = (\"|||\", \"oo\", \"xx\", \"..\", \"**\")\n    if show_preprint:\n        bars = [pubmed, preprint]\n        legend_platform = [\"PubMed\", \"Preprint\"]\n    else:\n        bars = [pubmed, arxiv, biorxiv, chemrxiv, medrxiv]\n        legend_platform = [\"PubMed\", \"ArXiv\", \"BiorXiv\", \"ChemRxiv\", \"MedRxiv\"]\n    for idx in range(len(keys)):\n        bottom = 0\n\n        for bidx, b in enumerate(bars):\n            if idx == 0:\n                p = plt.bar(\n                    ind + pos[idx],\n                    b[idx],\n                    width,\n                    linewidth=1,\n                    edgecolor=\"k\",\n                    bottom=bottom,\n                )\n            else:\n                p = plt.bar(\n                    ind + pos[idx],\n                    b[idx],\n                    width,\n                    color=next(iter(plts[bidx])).get_facecolor(),\n                    linewidth=1,\n                    edgecolor=\"k\",\n                    bottom=bottom,\n                )\n\n            bottom += b[idx]\n            plts.append(p)\n        legend_plts.append(\n            plt.bar(ind + pos[idx], np.zeros((len(ind),)), color=\"k\", bottom=bottom)\n        )\n\n    plt.ylabel(\"Counts\", size=15)\n    plt.xlabel(\"Years\", size=15)\n    plt.title(f\"Keywords: {title_text}\", size=14)\n    # Customize minor tick labels\n    plt.xticks(ind, x_ticks, size=10)\n\n    legend = plt.legend(\n        legend_platform,\n        prop={\"size\": 12},\n        loc=\"upper left\",\n        title=\"Platform:\",\n        title_fontsize=13,\n        ncol=1,\n    )\n\n    # Now set the hatches to not destroy legend\n\n    for idx, stackbar in enumerate(plts):\n        pidx = int(np.floor(idx / len(bars)))\n        for bar in stackbar:\n            bar.set_hatch(patterns[pidx])\n\n    for idx, stackbar in enumerate(legend_plts):\n        for bar in stackbar:\n            bar.set_hatch(patterns[idx])\n\n    if not keyword_text:\n        keyword_text = [\"\"] * len(keys)\n\n    plt.legend(\n        legend_plts,\n        keyword_text,\n        loc=\"upper center\",\n        prop={\"size\": 12},\n        title=\"Keywords (X):\",\n        title_fontsize=13,\n    )\n    plt.gca().add_artist(legend)\n\n    get_step_size = lambda x: round(x / 10, -math.floor(math.log10(x)) + 1)\n    ymax = plt.gca().get_ylim()[1]\n    step_size = np.clip(get_step_size(ymax), 5, 1000)\n    y_steps = np.arange(0, ymax, step_size)\n\n    for y_step in y_steps:\n        plt.hlines(y_step, xmax=10, xmin=-1, color=\"black\", linewidth=0.1)\n    plt.xlim([-0.5, len(ind)])\n    plt.ylim([0, ymax * 1.02])\n\n    plt.tight_layout()\n    plt.savefig(figpath)\n    plt.show()\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.plotting.plot_single","title":"<code>plot_single(data_dict: dict, keys: str, x_ticks: List[str] = ['2015', '2016', '2017', '2018', '2019', '2020'], show_preprint: bool = False, title_text: str = '', figpath: str = 'comparison_plot.pdf', logscale: bool = False) -&gt; None</code>","text":"<p>Plot temporal evolution of number of papers per keyword</p> <p>Parameters:</p> Name Type Description Default <code>data_dict</code> <code>dict</code> <p>A dictionary with keywords as keys. Each value should be a dictionary itself, with keys for the different APIs. For example data_dict = {     'covid_19.jsonl': {         'pubmed': [0, 0, 0, 12345],         'arxiv': [0, 0, 0, 1234],         ...     }     'coronavirus.jsonl':         'pubmed': [234, 345, 456, 12345],         'arxiv': [123, 234, 345, 1234],         ...     } }</p> required <code>keys</code> <code>str</code> <p>A key which should be plotted. This has to be a subset of data_dict.keys().</p> required <code>x_ticks</code> <code>List[str]</code> <p>List of strings to be used for the x-ticks. Should have same length as data_dict[key][database]. Defaults to ['2015', '2016', '2017', '2018', '2019', '2020'], meaning that papers are aggregated per year.</p> <code>['2015', '2016', '2017', '2018', '2019', '2020']</code> <code>show_preprint</code> <code>bool</code> <p>Whether preprint servers are aggregated or not. Defaults to False.</p> <code>False</code> <code>title_text</code> <code>str</code> <p>Title for the produced figure. Defaults to ''.</p> <code>''</code> <code>figpath</code> <code>str</code> <p>Name under which figure is saved. Relative or absolute paths can be given. Defaults to 'comparison_plot.pdf'.</p> <code>'comparison_plot.pdf'</code> <code>logscale</code> <code>bool</code> <p>Whether y-axis is plotted on logscale. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a database is missing in data_dict.</p> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_single(\n    data_dict: dict,\n    keys: str,\n    x_ticks: List[str] = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"],\n    show_preprint: bool = False,\n    title_text: str = \"\",\n    figpath: str = \"comparison_plot.pdf\",\n    logscale: bool = False,\n) -&gt; None:\n    \"\"\"Plot temporal evolution of number of papers per keyword\n\n    Args:\n        data_dict: A dictionary with keywords as keys. Each value should be a\n            dictionary itself, with keys for the different APIs. For example\n            data_dict = {\n                'covid_19.jsonl': {\n                    'pubmed': [0, 0, 0, 12345],\n                    'arxiv': [0, 0, 0, 1234],\n                    ...\n                }\n                'coronavirus.jsonl':\n                    'pubmed': [234, 345, 456, 12345],\n                    'arxiv': [123, 234, 345, 1234],\n                    ...\n                }\n            }\n        keys: A key which should be plotted. This has to be a subset of data_dict.keys().\n        x_ticks (List[str]): List of strings to be used for the x-ticks. Should have\n            same length as data_dict[key][database]. Defaults to ['2015', '2016',\n            '2017', '2018', '2019', '2020'], meaning that papers are aggregated per\n            year.\n        show_preprint: Whether preprint servers are aggregated or not.\n            Defaults to False.\n        title_text: Title for the produced figure. Defaults to ''.\n        figpath (str, optional): Name under which figure is saved. Relative or absolute\n            paths can be given. Defaults to 'comparison_plot.pdf'.\n        logscale: Whether y-axis is plotted on logscale. Defaults to False.\n\n    Raises:\n        KeyError: If a database is missing in data_dict.\n    \"\"\"\n\n    sns.set_palette(sns.color_palette(\"colorblind\", 10))\n    plt.rcParams.update({\"hatch.color\": \"w\"})\n    plt.rcParams[\"figure.facecolor\"] = \"white\"\n    plt.figure(figsize=(8, 5))\n\n    arxiv, biorxiv, pubmed, medrxiv, chemrxiv, preprint = [], [], [], [], [], []\n\n    for key in keys:\n        try:\n            arxiv.append(data_dict[key][\"arxiv\"])\n            biorxiv.append(data_dict[key][\"biorxiv\"])\n            medrxiv.append(data_dict[key][\"medrxiv\"])\n            chemrxiv.append(data_dict[key][\"chemrxiv\"])\n            pubmed.append(data_dict[key][\"pubmed\"])\n        except KeyError:\n            raise KeyError(\n                f\"Did not find all DBs for {key}, only found {data_dict[key].keys()}\"\n            )\n        preprint.append(arxiv[-1] + biorxiv[-1] + medrxiv[-1] + chemrxiv[-1])\n\n    ind = np.arange(len(arxiv[0]))  # the x locations for the groups\n    width = [0.75] * len(ind)  # the width of the bars: can also be len(x) sequence\n    fnc = np.log10 if logscale else np.copy\n\n    plts = []\n    legend_plts = []\n    if show_preprint:\n        bars = [pubmed, preprint]\n        legend_platform = [\"PubMed\", \"Preprint\"]\n        if logscale:\n            sums = np.array(pubmed) + np.array(preprint)\n            logsums = np.log10(sums)\n            bars = [pubmed * logsums / sums, preprint * logsums / sums]\n\n    else:\n        bars = [pubmed, arxiv, biorxiv, chemrxiv, medrxiv]\n        legend_platform = [\"PubMed\", \"ArXiv\", \"BiorXiv\", \"ChemRxiv\", \"MedRxiv\"]\n        if logscale:\n            sums = (\n                np.array(pubmed)\n                + np.array(arxiv)\n                + np.array(biorxiv)\n                + np.array(chemrxiv)\n                + np.array(medrxiv)\n            )\n            logsums = np.log10s(sums)\n            bars = [\n                pubmed * logsums / sums,\n                arxiv * logsums / sums,\n                biorxiv * logsums / sums,\n                chemrxiv * logsums / sums,\n                medrxiv * logsums / sums,\n            ]\n    for idx in range(len(keys)):\n        bottom = 0\n\n        for bidx, b in enumerate(bars):\n            if idx == 0:\n                p = plt.bar(\n                    ind,\n                    b[idx],\n                    width,\n                    linewidth=1,\n                    edgecolor=\"k\",\n                    bottom=bottom,\n                )\n            else:\n                p = plt.bar(\n                    ind,\n                    b[idx],\n                    width,\n                    color=next(iter(plts[bidx])).get_facecolor(),\n                    linewidth=1,\n                    edgecolor=\"k\",\n                    bottom=bottom,\n                )\n\n            bottom += b[idx]\n            plts.append(p)\n        legend_plts.append(\n            plt.bar(ind, np.zeros((len(ind),)), color=\"k\", bottom=bottom)\n        )\n\n    (\n        plt.ylabel(\"Counts\", size=17)\n        if not logscale\n        else plt.ylabel(\"Counts (log scale)\", size=17)\n    )\n    plt.xlabel(\"Years\", size=17)\n    plt.title(title_text, size=17)\n    # Customize minor tick labels\n\n    plt.xticks(ind, x_ticks, size=14)\n    ymax = plt.gca().get_ylim()[1]\n    if logscale:\n        yticks = np.arange(1, ymax).astype(int)\n        plt.yticks(yticks, np.power(10, yticks))\n\n    plt.tick_params(axis=\"y\", labelsize=17)\n\n    plt.legend(\n        legend_platform,\n        prop={\"size\": 14},\n        loc=\"upper left\",\n        title=\"Platform:\",\n        title_fontsize=17,\n        ncol=1,\n    )\n\n    get_step_size = lambda x: round(x / 10, -math.floor(math.log10(x)) + 1)\n    ymax = plt.gca().get_ylim()[1]\n\n    for y_step in plt.yticks()[0]:\n        plt.hlines(y_step, xmax=10, xmin=-1, color=\"black\", linewidth=0.1)\n    plt.xlim([-0.5, len(ind)])\n    plt.ylim([0, ymax * 1.02])\n\n    plt.tight_layout()\n    plt.savefig(figpath)\n    plt.show()\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.plotting.plot_venn_two","title":"<code>plot_venn_two(sizes: List[int], labels: List[str], figpath: str = 'venn_two.pdf', title: str = '', **kwargs) -&gt; None</code>","text":"<p>Plot a single Venn Diagram with two terms.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>List[int]</code> <p>List of ints of length 3. First two elements correspond to the labels, third one to the intersection.</p> required <code>labels</code> <code>[type]</code> <p>List of str of length 2, containing names of circles.</p> required <code>figpath</code> <code>str</code> <p>Name under which figure is saved. Defaults to 'venn_two.pdf', i.e. it is inferred from labels.</p> <code>'venn_two.pdf'</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to '', i.e. it is inferred from labels.</p> <code>''</code> <code>**kwargs</code> <p>Additional keyword arguments for venn2.</p> <code>{}</code> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_venn_two(\n    sizes: List[int],\n    labels: List[str],\n    figpath: str = \"venn_two.pdf\",\n    title: str = \"\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Plot a single Venn Diagram with two terms.\n\n    Args:\n        sizes (List[int]): List of ints of length 3. First two elements correspond to\n            the labels, third one to the intersection.\n        labels ([type]): List of str of length 2, containing names of circles.\n        figpath (str): Name under which figure is saved. Defaults to 'venn_two.pdf', i.e. it is\n            inferred from labels.\n        title (str): Title of the plot. Defaults to '', i.e. it is inferred from\n            labels.\n        **kwargs: Additional keyword arguments for venn2.\n    \"\"\"\n    assert len(sizes) == 3, \"Incorrect type/length of sizes\"\n    assert len(labels) == 2, \"Incorrect type/length of labels\"\n\n    title = get_name(labels) if title == \"\" else title\n    figname = title.lower().replace(\" vs. \", \"_\") if figpath == \"\" else figpath\n    venn2(subsets=sizes, set_labels=labels, alpha=0.6, **kwargs)\n    venn2_circles(\n        subsets=sizes, linestyle=\"solid\", linewidth=0.6, color=\"grey\", **kwargs\n    )\n    if kwargs.get(\"ax\", False):\n        print(kwargs, type(kwargs))\n        print(kwargs[\"ax\"])\n        kwargs[\"ax\"].set_title(title, fontdict={\"fontweight\": \"bold\"}, size=15)\n    else:\n        plt.title(title, fontdict={\"fontweight\": \"bold\"}, size=15)\n        plt.savefig(f\"{figname}.pdf\")\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.plotting.plot_venn_three","title":"<code>plot_venn_three(sizes: List[int], labels: List[str], figpath: str = '', title: str = '', **kwargs) -&gt; None</code>","text":"<p>Plot a single Venn Diagram with two terms.</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>List[int]</code> <p>List of ints of length 3. First two elements correspond to the labels, third one to the intersection.</p> required <code>labels</code> <code>List[str]</code> <p>List of str of length 2, containing names of circles.</p> required <code>figpath</code> <code>str</code> <p>Name under which figure is saved. Defaults to '', i.e. it is inferred from labels.</p> <code>''</code> <code>title</code> <code>str</code> <p>Title of the plot. Defaults to '', i.e. it is inferred from labels.</p> <code>''</code> <code>**kwargs</code> <p>Additional keyword arguments for venn3.</p> <code>{}</code> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_venn_three(\n    sizes: List[int], labels: List[str], figpath: str = \"\", title: str = \"\", **kwargs\n) -&gt; None:\n    \"\"\"Plot a single Venn Diagram with two terms.\n\n    Args:\n        sizes (List[int]): List of ints of length 3. First two elements correspond to\n            the labels, third one to the intersection.\n        labels (List[str]): List of str of length 2, containing names of circles.\n        figpath (str): Name under which figure is saved. Defaults to '', i.e. it is\n            inferred from labels.\n        title (str): Title of the plot. Defaults to '', i.e. it is inferred from\n            labels.\n        **kwargs: Additional keyword arguments for venn3.\n    \"\"\"\n    assert len(sizes) == 7, \"Incorrect type/length of sizes\"\n    assert len(labels) == 3, \"Incorrect type/length of labels\"\n\n    title = get_name(labels) if title == \"\" else title\n    figname = title.lower().replace(\" vs. \", \"_\") if figpath == \"\" else figpath\n\n    venn3(subsets=sizes, set_labels=labels, alpha=0.6, **kwargs)\n    venn3_circles(\n        subsets=sizes, linestyle=\"solid\", linewidth=0.6, color=\"grey\", **kwargs\n    )\n\n    if kwargs.get(\"ax\", False):\n        kwargs[\"ax\"].set_title(title, fontdict={\"fontweight\": \"bold\"}, size=15)\n    else:\n        plt.title(title, fontdict={\"fontweight\": \"bold\"}, size=15)\n        plt.savefig(f\"{figname}.pdf\")\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.plotting.plot_multiple_venn","title":"<code>plot_multiple_venn(sizes: List[List[int]], labels: List[List[str]], figname: str, titles: List[str], suptitle: str = '', gridspec_kw: dict = {}, figsize: Iterable = (8, 4.5), **kwargs) -&gt; None</code>","text":"<p>Plots multiple Venn Diagrams next to each other</p> <p>Parameters:</p> Name Type Description Default <code>sizes</code> <code>List[List[int]]</code> <p>List of lists with sizes, one per Venn Diagram. Lengths of lists should be either 3 (plot_venn_two) or 7 (plot_venn_two).</p> required <code>labels</code> <code>List[List[str]]</code> <p>List of Lists of str containing names of circles. Lengths of lists should be either 2 or 3.</p> required <code>figname</code> <code>str</code> <p>Name under which figure is saved. Defaults to '', i.e. it is inferred from labels.</p> required <code>titles</code> <code>List[str]</code> <p>Titles of subplots. Should have same length like labels and sizes.</p> required <code>suptitle</code> <code>str</code> <p>Title of entire plot. Defaults to '', i.e. no title.</p> <code>''</code> <code>gridspec_kw</code> <code>dict</code> <p>Additional keyword args for plt.subplots. Useful to adjust width of plots. E.g.     gridspec_kw={'width_ratios': [1, 2]} will make the second Venn Diagram double as wide as first one.</p> <code>{}</code> <code>**kwargs</code> <p>Additional keyword arguments for venn3.</p> <code>{}</code> Source code in <code>paperscraper/plotting.py</code> <pre><code>def plot_multiple_venn(\n    sizes: List[List[int]],\n    labels: List[List[str]],\n    figname: str,\n    titles: List[str],\n    suptitle: str = \"\",\n    gridspec_kw: dict = {},\n    figsize: Iterable = (8, 4.5),\n    **kwargs,\n) -&gt; None:\n    \"\"\"Plots multiple Venn Diagrams next to each other\n\n    Args:\n        sizes (List[List[int]]): List of lists with sizes, one per Venn Diagram.\n            Lengths of lists should be either 3 (plot_venn_two) or 7\n            (plot_venn_two).\n        labels (List[List[str]]): List of Lists of str containing names of circles.\n            Lengths of lists should be either 2 or 3.\n        figname (str): Name under which figure is saved. Defaults to '', i.e. it is\n            inferred from labels.\n        titles (List[str]): Titles of subplots. Should have same length like labels\n            and sizes.\n        suptitle (str): Title of entire plot. Defaults to '', i.e. no title.\n        gridspec_kw (dict): Additional keyword args for plt.subplots. Useful to\n            adjust width of plots. E.g.\n                gridspec_kw={'width_ratios': [1, 2]}\n            will make the second Venn Diagram double as wide as first one.\n        **kwargs: Additional keyword arguments for venn3.\n    \"\"\"\n\n    assert len(sizes) == len(labels), \"Length of labels &amp; sizes dont match.\"\n    assert len(sizes) == len(titles), \"Length of titles &amp; sizes dont match.\"\n    assert len(sizes) &gt; 1, \"At least 2 items should be provided.\"\n    assert all(list(map(lambda x: len(x) in [2, 3], labels))), \"Wrong label sizes.\"\n    assert all(list(map(lambda x: len(x) in [3, 7], sizes))), \"Wrong label sizes.\"\n\n    fig, axes = plt.subplots(1, len(sizes), gridspec_kw=gridspec_kw, figsize=figsize)\n    plt.suptitle(suptitle, size=18, fontweight=\"bold\")\n\n    figname = titles[0].lower().replace(\" vs. \", \"_\") if figname == \"\" else figname\n\n    for idx, (size, label, title) in enumerate(zip(sizes, labels, titles)):\n        if len(label) == 2:\n            plot_venn_two(size, label, title=title, ax=axes[idx])\n        elif len(label) == 3:\n            plot_venn_three(size, label, title=title, ax=axes[idx])\n\n    plt.savefig(f\"{figname}.pdf\")\n</code></pre>"},{"location":"api/paperscraper/#postprocessing","title":"postprocessing","text":""},{"location":"api/paperscraper/#paperscraper.postprocessing","title":"<code>paperscraper.postprocessing</code>","text":""},{"location":"api/paperscraper/#paperscraper.postprocessing.aggregate_paper","title":"<code>aggregate_paper(data: List[Dict[str, str]], start_year: int = 2016, bins_per_year: int = 4, filtering: bool = False, filter_keys: List = list(), unwanted_keys: List = list(), return_filtered: bool = False, filter_abstract: bool = True, last_year: int = 2021)</code>","text":"<p>Consumes a list of unstructured keyword results from a .jsonl and aggregates papers into several bins per year.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>List[Dict[str, str]]</code> <p>Content of a .jsonl file, i.e., a list of dictionaries, one per paper.</p> required <code>start_year</code> <code>int</code> <p>First year of interest. Defaults to 2016.</p> <code>2016</code> <code>bins_per_year</code> <code>int</code> <p>Defaults to 4 (quarterly aggregation).</p> <code>4</code> <code>filtering</code> <code>bool</code> <p>Whether or not all papers in .jsonl are perceived as matches or whether an additional sanity checking for the keywords is performed in abstract/title. Defaults to False.</p> <code>False</code> <code>filter_keys</code> <code>list</code> <p>List of str used for filtering. Only applies if filtering is True. Defaults to empty list.</p> <code>list()</code> <code>unwanted_keys</code> <code>list</code> <p>List of str that must not occur in either title or abstract. Only applies if filtering is True.</p> <code>list()</code> <code>return_filtered</code> <code>bool</code> <p>Whether the filtered matches are also returned. Only applies if filtering is True. Defaults to False.</p> <code>False</code> <code>filter_abstract</code> <code>bool</code> <p>Whether the keyword is searched in the abstract or not. Defaults to True.</p> <code>True</code> <code>last_year</code> <code>int</code> <p>Most recent year for the aggregation. Defaults to current year. All newer entries are discarded.</p> <code>2021</code> <p>Returns:</p> Name Type Description <code>bins</code> <code>array</code> <p>Vector of length number of years (2020 - start_year) x bins_per_year.</p> Source code in <code>paperscraper/postprocessing.py</code> <pre><code>def aggregate_paper(\n    data: List[Dict[str, str]],\n    start_year: int = 2016,\n    bins_per_year: int = 4,\n    filtering: bool = False,\n    filter_keys: List = list(),\n    unwanted_keys: List = list(),\n    return_filtered: bool = False,\n    filter_abstract: bool = True,\n    last_year: int = 2021,\n):\n    \"\"\"Consumes a list of unstructured keyword results from a .jsonl and\n    aggregates papers into several bins per year.\n\n    Args:\n        data (List[Dict[str,str]]): Content of a .jsonl file, i.e., a list of\n            dictionaries, one per paper.\n        start_year (int, optional): First year of interest. Defaults to 2016.\n        bins_per_year (int, optional): Defaults to 4 (quarterly aggregation).\n        filtering (bool, optional): Whether or not all papers in .jsonl are\n            perceived as matches or whether an additional sanity checking for\n            the keywords is performed in abstract/title. Defaults to False.\n        filter_keys (list, optional): List of str used for filtering. Only\n            applies if filtering is True. Defaults to empty list.\n        unwanted_keys (list, optional): List of str that must not occur in either\n            title or abstract. Only applies if filtering is True.\n        return_filtered (bool, optional): Whether the filtered matches are also\n            returned. Only applies if filtering is True. Defaults to False.\n        filter_abstract (bool, optional): Whether the keyword is searched in the abstract\n            or not. Defaults to True.\n        last_year (int, optional): Most recent year for the aggregation. Defaults\n            to current year. All newer entries are discarded.\n\n    Returns:\n        bins (np.array): Vector of length number of years (2020 - start_year) x\n            bins_per_year.\n    \"\"\"\n\n    if not isinstance(data, list):\n        raise ValueError(f\"Expected list, received {type(data)}\")\n    if not isinstance(bins_per_year, int):\n        raise ValueError(f\"Expected int, received {type(bins_per_year)}\")\n    if 12 % bins_per_year != 0:\n        raise ValueError(f\"Can't split year into {bins_per_year} bins\")\n\n    num_years = last_year - start_year + 1\n    bins = np.zeros((num_years * bins_per_year))\n\n    if len(data) == 0:\n        return bins if not return_filtered else (bins, [])\n\n    # Remove duplicate entries (keep only the first one)\n    df = pd.DataFrame(data).sort_values(by=\"date\", ascending=True)\n    data = df.drop_duplicates(subset=\"title\", keep=\"first\").to_dict(\"records\")\n\n    dates = [dd[\"date\"] for dd in data]\n\n    filtered = []\n    for paper, date in zip(data, dates):\n        year = int(date.split(\"-\")[0])\n        if year &lt; start_year or year &gt; last_year:\n            continue\n\n        # At least one synonym per keyword needs to be in either title or\n        # abstract.\n        if filtering and filter_keys != list():\n            # Filter out papers which undesired terms\n            unwanted = False\n            for unwanted_key in unwanted_keys:\n                if unwanted_key.lower() in paper[\"title\"].lower():\n                    unwanted = True\n                if (\n                    filter_abstract\n                    and paper[\"abstract\"] is not None\n                    and unwanted_key.lower() in paper[\"abstract\"].lower()\n                ):\n                    unwanted = True\n            if unwanted:\n                continue\n\n            got_keys = []\n            for key_term in filter_keys:\n                got_key = False\n                if not isinstance(key_term, list):\n                    key_term = [key_term]\n                for key in key_term:\n                    if key.lower() in paper[\"title\"].lower():\n                        got_key = True\n                    if (\n                        filter_abstract\n                        and paper[\"abstract\"] is not None\n                        and key.lower() in paper[\"abstract\"].lower()\n                    ):\n                        got_key = True\n                got_keys.append(got_key)\n\n            if len(got_keys) != sum(got_keys):\n                continue\n\n        filtered.append(paper)\n\n        if len(date.split(\"-\")) &lt; 2:\n            logger.warning(\n                f\"Paper without month {date}, randomly assigned month.{paper['title']}\"\n            )\n            month = np.random.choice(12)\n        else:\n            month = int(date.split(\"-\")[1])\n\n        year_bin = year - start_year\n        month_bin = int(np.floor((month - 1) / (12 / bins_per_year)))\n        bins[year_bin * bins_per_year + month_bin] += 1\n\n    if return_filtered:\n        return bins, filtered\n    else:\n        return bins\n</code></pre>"},{"location":"api/paperscraper/#server_dumps","title":"server_dumps","text":""},{"location":"api/paperscraper/#paperscraper.server_dumps","title":"<code>paperscraper.server_dumps</code>","text":"<p>Folder for the metadata dumps from biorxiv, medrxiv and chemrxiv API. No code here but will be populated with your local <code>.jsonl</code> files.</p>"},{"location":"api/paperscraper/#utils","title":"utils","text":""},{"location":"api/paperscraper/#paperscraper.utils","title":"<code>paperscraper.utils</code>","text":""},{"location":"api/paperscraper/#paperscraper.utils.dump_papers","title":"<code>dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None</code>","text":"<p>Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl file with one paper per line.</p> <p>Parameters:</p> Name Type Description Default <code>papers</code> <code>DataFrame</code> <p>A dataframe of paper metadata, one paper per row.</p> required <code>filepath</code> <code>str</code> <p>Path to dump the papers, has to end with <code>.jsonl</code>.</p> required Source code in <code>paperscraper/utils.py</code> <pre><code>def dump_papers(papers: pd.DataFrame, filepath: str) -&gt; None:\n    \"\"\"\n    Receives a pd.DataFrame, one paper per row and dumps it into a .jsonl\n    file with one paper per line.\n\n    Args:\n        papers (pd.DataFrame): A dataframe of paper metadata, one paper per row.\n        filepath (str): Path to dump the papers, has to end with `.jsonl`.\n    \"\"\"\n    if not isinstance(filepath, str):\n        raise TypeError(f\"filepath must be a string, not {type(filepath)}\")\n    if not filepath.endswith(\".jsonl\"):\n        raise ValueError(\"Please provide a filepath with .jsonl extension\")\n\n    if isinstance(papers, List) and all([isinstance(p, Dict) for p in papers]):\n        papers = pd.DataFrame(papers)\n        logger.warning(\n            \"Preferably pass a pd.DataFrame, not a list of dictionaries. \"\n            \"Passing a list is a legacy functionality that might become deprecated.\"\n        )\n\n    if not isinstance(papers, pd.DataFrame):\n        raise TypeError(f\"papers must be a pd.DataFrame, not {type(papers)}\")\n\n    paper_list = list(papers.T.to_dict().values())\n\n    with open(filepath, \"w\") as f:\n        for paper in paper_list:\n            f.write(json.dumps(paper) + \"\\n\")\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.utils.get_filename_from_query","title":"<code>get_filename_from_query(query: List[str]) -&gt; str</code>","text":"<p>Convert a keyword query into filenames to dump the paper.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>list</code> <p>List of string with keywords.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Filename.</p> Source code in <code>paperscraper/utils.py</code> <pre><code>def get_filename_from_query(query: List[str]) -&gt; str:\n    \"\"\"Convert a keyword query into filenames to dump the paper.\n\n    Args:\n        query (list): List of string with keywords.\n\n    Returns:\n        str: Filename.\n    \"\"\"\n    filename = \"_\".join([k if isinstance(k, str) else k[0] for k in query]) + \".jsonl\"\n    filename = filename.replace(\" \", \"\").lower()\n    return filename\n</code></pre>"},{"location":"api/paperscraper/#paperscraper.utils.load_jsonl","title":"<code>load_jsonl(filepath: str) -&gt; List[Dict[str, str]]</code>","text":"<p>Load data from a <code>.jsonl</code> file, i.e., a file with one dictionary per line.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to <code>.jsonl</code> file.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, str]]</code> <p>List[Dict[str, str]]: A list of dictionaries, one per paper.</p> Source code in <code>paperscraper/utils.py</code> <pre><code>def load_jsonl(filepath: str) -&gt; List[Dict[str, str]]:\n    \"\"\"\n    Load data from a `.jsonl` file, i.e., a file with one dictionary per line.\n\n    Args:\n        filepath (str): Path to `.jsonl` file.\n\n    Returns:\n        List[Dict[str, str]]: A list of dictionaries, one per paper.\n    \"\"\"\n\n    with open(filepath, \"r\") as f:\n        data = [json.loads(line) for line in f.readlines()]\n    return data\n</code></pre>"}]}